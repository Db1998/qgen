{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, example_to_train=8000, gen_test=True, gen_test_number=5, gen_train=True, gen_train_number=2, gpu=True, hidden_size=300, load='', load_data='', lr=0.0003, no_eval=True, no_train=False, num_epochs=500, reduced_glove=False, save='', save_data='', seed=42, split_ratio=0.8, tf_ratio=1.0, tf_ratio_decay_rate=0.99, train_data='/home/ra2630/NLU/train-v1.1.json', use_attention=True, use_masked_loss=True, word_tf=True, words_to_take=5000)\n",
      "Original Glove shape: (400003, 300)\n",
      "Reduced Glove shape:  (5000, 300)\n",
      "No. of invalid words/examples: 4858\n",
      "Number of batches =  63\n",
      "No. of batches in training data: 50, with batch_size: 128 \n",
      "Batch: 0 \t Epoch : 0\tNet Loss: 8.5329 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.5329\n",
      "Batch: 1 \t Epoch : 0\tNet Loss: 8.5119 \tAnswer Loss: 0.6520 \tQuestion Loss: 8.5119\n",
      "Batch: 2 \t Epoch : 0\tNet Loss: 8.5004 \tAnswer Loss: 0.6520 \tQuestion Loss: 8.5004\n",
      "Batch: 3 \t Epoch : 0\tNet Loss: 8.4729 \tAnswer Loss: 0.6519 \tQuestion Loss: 8.4729\n",
      "Batch: 4 \t Epoch : 0\tNet Loss: 8.4618 \tAnswer Loss: 0.6523 \tQuestion Loss: 8.4618\n",
      "Batch: 5 \t Epoch : 0\tNet Loss: 8.4526 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.4526\n",
      "Batch: 6 \t Epoch : 0\tNet Loss: 8.4106 \tAnswer Loss: 0.6523 \tQuestion Loss: 8.4106\n",
      "Batch: 7 \t Epoch : 0\tNet Loss: 8.4059 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.4059\n",
      "Batch: 8 \t Epoch : 0\tNet Loss: 8.3815 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.3815\n",
      "Batch: 9 \t Epoch : 0\tNet Loss: 8.3711 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.3711\n",
      "Batch: 10 \t Epoch : 0\tNet Loss: 8.3507 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.3507\n",
      "Batch: 11 \t Epoch : 0\tNet Loss: 8.3231 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.3231\n",
      "Batch: 12 \t Epoch : 0\tNet Loss: 8.2824 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.2824\n",
      "Batch: 13 \t Epoch : 0\tNet Loss: 8.3077 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.3077\n",
      "Batch: 14 \t Epoch : 0\tNet Loss: 8.2875 \tAnswer Loss: 0.6520 \tQuestion Loss: 8.2875\n",
      "Batch: 15 \t Epoch : 0\tNet Loss: 8.2660 \tAnswer Loss: 0.6520 \tQuestion Loss: 8.2660\n",
      "Batch: 16 \t Epoch : 0\tNet Loss: 8.2427 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.2427\n",
      "Batch: 17 \t Epoch : 0\tNet Loss: 8.2409 \tAnswer Loss: 0.6520 \tQuestion Loss: 8.2409\n",
      "Batch: 18 \t Epoch : 0\tNet Loss: 8.2331 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.2331\n",
      "Batch: 19 \t Epoch : 0\tNet Loss: 8.1898 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.1898\n",
      "Batch: 20 \t Epoch : 0\tNet Loss: 8.1759 \tAnswer Loss: 0.6524 \tQuestion Loss: 8.1759\n",
      "Batch: 21 \t Epoch : 0\tNet Loss: 8.1747 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.1747\n",
      "Batch: 22 \t Epoch : 0\tNet Loss: 8.1693 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.1693\n",
      "Batch: 23 \t Epoch : 0\tNet Loss: 8.1430 \tAnswer Loss: 0.6519 \tQuestion Loss: 8.1430\n",
      "Batch: 24 \t Epoch : 0\tNet Loss: 8.1237 \tAnswer Loss: 0.6520 \tQuestion Loss: 8.1237\n",
      "Batch: 25 \t Epoch : 0\tNet Loss: 8.1017 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.1017\n",
      "Batch: 26 \t Epoch : 0\tNet Loss: 8.0737 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.0737\n",
      "Batch: 27 \t Epoch : 0\tNet Loss: 8.0725 \tAnswer Loss: 0.6523 \tQuestion Loss: 8.0725\n",
      "Batch: 28 \t Epoch : 0\tNet Loss: 8.0481 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.0481\n",
      "Batch: 29 \t Epoch : 0\tNet Loss: 8.0470 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.0470\n",
      "Batch: 30 \t Epoch : 0\tNet Loss: 8.0204 \tAnswer Loss: 0.6522 \tQuestion Loss: 8.0204\n",
      "Batch: 31 \t Epoch : 0\tNet Loss: 8.0243 \tAnswer Loss: 0.6521 \tQuestion Loss: 8.0243\n",
      "Batch: 32 \t Epoch : 0\tNet Loss: 7.9699 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.9699\n",
      "Batch: 33 \t Epoch : 0\tNet Loss: 7.9697 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.9697\n",
      "Batch: 34 \t Epoch : 0\tNet Loss: 7.9471 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.9471\n",
      "Batch: 35 \t Epoch : 0\tNet Loss: 7.9644 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.9644\n",
      "Batch: 36 \t Epoch : 0\tNet Loss: 7.9148 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.9148\n",
      "Batch: 37 \t Epoch : 0\tNet Loss: 7.9096 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.9096\n",
      "Batch: 38 \t Epoch : 0\tNet Loss: 7.8945 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.8945\n",
      "Batch: 39 \t Epoch : 0\tNet Loss: 7.8839 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.8839\n",
      "Batch: 40 \t Epoch : 0\tNet Loss: 7.8822 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.8822\n",
      "Batch: 41 \t Epoch : 0\tNet Loss: 7.8498 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.8498\n",
      "Batch: 42 \t Epoch : 0\tNet Loss: 7.8593 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.8593\n",
      "Batch: 43 \t Epoch : 0\tNet Loss: 7.8424 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.8424\n",
      "Batch: 44 \t Epoch : 0\tNet Loss: 7.8346 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.8346\n",
      "Batch: 45 \t Epoch : 0\tNet Loss: 7.8049 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.8049\n",
      "Batch: 46 \t Epoch : 0\tNet Loss: 7.7892 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.7892\n",
      "Batch: 47 \t Epoch : 0\tNet Loss: 7.7479 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.7479\n",
      "Batch: 48 \t Epoch : 0\tNet Loss: 7.7746 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.7746\n",
      "Batch: 49 \t Epoch : 0\tNet Loss: 7.7512 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.7512\n",
      "Average Loss after Epoch 0 : 6.4443\n",
      "Epoch time: 46.49s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 347.6093\tQuestion Loss (full generated): 280.2469\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.7061\tQuestion Loss Masked(full generated): 7.7179\n",
      "Average loss (full gen): 280.2469\n",
      "Eval time: 0.64s\n",
      "T.F. Generated: what are were of was by was protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are used to it of war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war\n",
      "Ground Truth..: what did the <UNK> towns <UNK>                              \n",
      "\n",
      "\n",
      "T.F. Generated: what was ? language used was first perceived late  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are used to it of war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war\n",
      "Ground Truth..: who initially opposed general <UNK> 's landing plan ?                           \n",
      "\n",
      "\n",
      "T.F. Generated: what the the  are ? was were do of was was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are used to it of war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war\n",
      "Ground Truth..: in people , how many <UNK> make up the <UNK> <UNK> ?                        \n",
      "\n",
      "\n",
      "T.F. Generated: what are the of thought people it  the to used the index first was institutions  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are used to it of war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war\n",
      "Ground Truth..: what is the oldest dynasty to be represented with buildings in kathmandu 's <UNK> square ?                    \n",
      "\n",
      "\n",
      "T.F. Generated: what are times was of usa  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are used to it of war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war\n",
      "Ground Truth..: what forced <UNK> throughout tajikistan ?                              \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 347.4058\tQuestion Loss (full generated): 280.1360\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.6790\tQuestion Loss Masked(full generated): 7.7090\n",
      "Average loss (full gen): 280.1360\n",
      "Eval time: 0.63s\n",
      "T.F. Generated: what are had were of airports ? of  were used   protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are used to it of war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war\n",
      "Ground Truth..: what year did the everton football club `` go blue '' ?                        \n",
      "\n",
      "\n",
      "T.F. Generated: what the were a of used of the of ? war policy  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are used to it of war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war war\n",
      "Ground Truth..: was there an increase or decrease in the field of agriculture ?                        \n",
      "\n",
      "\n",
      "No. of batches in training data: 50, with batch_size: 128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 1\tNet Loss: 7.7007 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.7007\n",
      "Batch: 1 \t Epoch : 1\tNet Loss: 7.6917 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.6917\n",
      "Batch: 2 \t Epoch : 1\tNet Loss: 7.6840 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.6840\n",
      "Batch: 3 \t Epoch : 1\tNet Loss: 7.6647 \tAnswer Loss: 0.6519 \tQuestion Loss: 7.6647\n",
      "Batch: 4 \t Epoch : 1\tNet Loss: 7.6583 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.6583\n",
      "Batch: 5 \t Epoch : 1\tNet Loss: 7.6575 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.6575\n",
      "Batch: 6 \t Epoch : 1\tNet Loss: 7.5841 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.5841\n",
      "Batch: 7 \t Epoch : 1\tNet Loss: 7.6238 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.6238\n",
      "Batch: 8 \t Epoch : 1\tNet Loss: 7.5761 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.5761\n",
      "Batch: 9 \t Epoch : 1\tNet Loss: 7.6244 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.6244\n",
      "Batch: 10 \t Epoch : 1\tNet Loss: 7.5952 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.5952\n",
      "Batch: 11 \t Epoch : 1\tNet Loss: 7.5458 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.5458\n",
      "Batch: 12 \t Epoch : 1\tNet Loss: 7.5417 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.5417\n",
      "Batch: 13 \t Epoch : 1\tNet Loss: 7.5921 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.5921\n",
      "Batch: 14 \t Epoch : 1\tNet Loss: 7.5470 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.5470\n",
      "Batch: 15 \t Epoch : 1\tNet Loss: 7.5471 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.5471\n",
      "Batch: 16 \t Epoch : 1\tNet Loss: 7.5112 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.5112\n",
      "Batch: 17 \t Epoch : 1\tNet Loss: 7.5316 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.5316\n",
      "Batch: 18 \t Epoch : 1\tNet Loss: 7.5331 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.5331\n",
      "Batch: 19 \t Epoch : 1\tNet Loss: 7.4874 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.4874\n",
      "Batch: 20 \t Epoch : 1\tNet Loss: 7.4654 \tAnswer Loss: 0.6524 \tQuestion Loss: 7.4654\n",
      "Batch: 21 \t Epoch : 1\tNet Loss: 7.4889 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.4889\n",
      "Batch: 22 \t Epoch : 1\tNet Loss: 7.5099 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.5099\n",
      "Batch: 23 \t Epoch : 1\tNet Loss: 7.4717 \tAnswer Loss: 0.6519 \tQuestion Loss: 7.4717\n",
      "Batch: 24 \t Epoch : 1\tNet Loss: 7.4621 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.4621\n",
      "Batch: 25 \t Epoch : 1\tNet Loss: 7.4527 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.4527\n",
      "Batch: 26 \t Epoch : 1\tNet Loss: 7.4019 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.4019\n",
      "Batch: 27 \t Epoch : 1\tNet Loss: 7.4201 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.4201\n",
      "Batch: 28 \t Epoch : 1\tNet Loss: 7.3953 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3953\n",
      "Batch: 29 \t Epoch : 1\tNet Loss: 7.4159 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.4159\n",
      "Batch: 30 \t Epoch : 1\tNet Loss: 7.3908 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.3908\n",
      "Batch: 31 \t Epoch : 1\tNet Loss: 7.4033 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.4033\n",
      "Batch: 32 \t Epoch : 1\tNet Loss: 7.3277 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3277\n",
      "Batch: 33 \t Epoch : 1\tNet Loss: 7.3564 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.3564\n",
      "Batch: 34 \t Epoch : 1\tNet Loss: 7.3301 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3301\n",
      "Batch: 35 \t Epoch : 1\tNet Loss: 7.3927 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.3927\n",
      "Batch: 36 \t Epoch : 1\tNet Loss: 7.3294 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.3294\n",
      "Batch: 37 \t Epoch : 1\tNet Loss: 7.3380 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.3380\n",
      "Batch: 38 \t Epoch : 1\tNet Loss: 7.3043 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3043\n",
      "Batch: 39 \t Epoch : 1\tNet Loss: 7.3176 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3176\n",
      "Batch: 40 \t Epoch : 1\tNet Loss: 7.3327 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3327\n",
      "Batch: 41 \t Epoch : 1\tNet Loss: 7.2997 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.2997\n",
      "Batch: 42 \t Epoch : 1\tNet Loss: 7.3179 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.3179\n",
      "Batch: 43 \t Epoch : 1\tNet Loss: 7.3069 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3069\n",
      "Batch: 44 \t Epoch : 1\tNet Loss: 7.3027 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.3027\n",
      "Batch: 45 \t Epoch : 1\tNet Loss: 7.2737 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.2737\n",
      "Batch: 46 \t Epoch : 1\tNet Loss: 7.2529 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.2529\n",
      "Batch: 47 \t Epoch : 1\tNet Loss: 7.2303 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.2303\n",
      "Batch: 48 \t Epoch : 1\tNet Loss: 7.2700 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.2700\n",
      "Batch: 49 \t Epoch : 1\tNet Loss: 7.2500 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.2500\n",
      "Average Loss after Epoch 1 : 5.9160\n",
      "Epoch time: 45.65s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 347.6718\tQuestion Loss (full generated): 283.9088\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.3262\tQuestion Loss Masked(full generated): 7.4874\n",
      "Average loss (full gen): 283.9088\n",
      "Eval time: 0.62s\n",
      "T.F. Generated: what many type was was an  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what type of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name\n",
      "Ground Truth..: how did <UNK> <UNK> police ?                              \n",
      "\n",
      "\n",
      "T.F. Generated: what was type was was ? minister the  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what type of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name\n",
      "Ground Truth..: when did <UNK> <UNK> become prime minister ?                            \n",
      "\n",
      "\n",
      "T.F. Generated: what the of country <UNK> name a <UNK> used not type james their  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what type of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name\n",
      "Ground Truth..: in which department of the committee of public safety did napoleon serve ?                       \n",
      "\n",
      "\n",
      "T.F. Generated: what the was the a of ?  the name type   protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what type of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name\n",
      "Ground Truth..: albert <UNK> was also known as `` albert the what '' ?                        \n",
      "\n",
      "\n",
      "T.F. Generated: what does the name term first the the the  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what type of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name\n",
      "Ground Truth..: where was the bbc 's first studio located ?                           \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 347.3925\tQuestion Loss (full generated): 282.8291\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.1942\tQuestion Loss Masked(full generated): 7.3919\n",
      "Average loss (full gen): 282.8291\n",
      "Eval time: 0.77s\n",
      "T.F. Generated: what used new  new the were many be be was was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what type of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name\n",
      "Ground Truth..: are male or female late adolescents more likely to <UNK> <UNK> ?                        \n",
      "\n",
      "\n",
      "T.F. Generated: what the type language of type name of of <UNK> relative of are the be year  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what type of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name of <UNK> was the name\n",
      "Ground Truth..: in what historical period did the different types of germanic languages stop being collectively understood ?                    \n",
      "\n",
      "\n",
      "No. of batches in training data: 50, with batch_size: 128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 2\tNet Loss: 7.1968 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.1968\n",
      "Batch: 1 \t Epoch : 2\tNet Loss: 7.1812 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.1812\n",
      "Batch: 2 \t Epoch : 2\tNet Loss: 7.2032 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.2032\n",
      "Batch: 3 \t Epoch : 2\tNet Loss: 7.1797 \tAnswer Loss: 0.6519 \tQuestion Loss: 7.1797\n",
      "Batch: 4 \t Epoch : 2\tNet Loss: 7.1965 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.1965\n",
      "Batch: 5 \t Epoch : 2\tNet Loss: 7.1831 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.1831\n",
      "Batch: 6 \t Epoch : 2\tNet Loss: 7.1061 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.1061\n",
      "Batch: 7 \t Epoch : 2\tNet Loss: 7.1419 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.1419\n",
      "Batch: 8 \t Epoch : 2\tNet Loss: 7.2260 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.2260\n",
      "Batch: 9 \t Epoch : 2\tNet Loss: 7.1797 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.1797\n",
      "Batch: 10 \t Epoch : 2\tNet Loss: 7.2298 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.2298\n",
      "Batch: 11 \t Epoch : 2\tNet Loss: 7.0865 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.0865\n",
      "Batch: 12 \t Epoch : 2\tNet Loss: 7.0963 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.0963\n",
      "Batch: 13 \t Epoch : 2\tNet Loss: 7.1577 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.1577\n",
      "Batch: 14 \t Epoch : 2\tNet Loss: 7.1152 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.1152\n",
      "Batch: 15 \t Epoch : 2\tNet Loss: 7.1229 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.1229\n",
      "Batch: 16 \t Epoch : 2\tNet Loss: 7.0992 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.0992\n",
      "Batch: 17 \t Epoch : 2\tNet Loss: 7.1108 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.1108\n",
      "Batch: 18 \t Epoch : 2\tNet Loss: 7.1214 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.1214\n",
      "Batch: 19 \t Epoch : 2\tNet Loss: 7.0662 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.0662\n",
      "Batch: 20 \t Epoch : 2\tNet Loss: 7.0478 \tAnswer Loss: 0.6524 \tQuestion Loss: 7.0478\n",
      "Batch: 21 \t Epoch : 2\tNet Loss: 7.0833 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.0833\n",
      "Batch: 22 \t Epoch : 2\tNet Loss: 7.1125 \tAnswer Loss: 0.6522 \tQuestion Loss: 7.1125\n",
      "Batch: 23 \t Epoch : 2\tNet Loss: 7.0770 \tAnswer Loss: 0.6519 \tQuestion Loss: 7.0770\n",
      "Batch: 24 \t Epoch : 2\tNet Loss: 7.1269 \tAnswer Loss: 0.6520 \tQuestion Loss: 7.1269\n",
      "Batch: 25 \t Epoch : 2\tNet Loss: 7.0272 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.0272\n",
      "Batch: 26 \t Epoch : 2\tNet Loss: 6.9926 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.9926\n",
      "Batch: 27 \t Epoch : 2\tNet Loss: 7.0314 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.0314\n",
      "Batch: 28 \t Epoch : 2\tNet Loss: 6.9950 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9950\n",
      "Batch: 29 \t Epoch : 2\tNet Loss: 7.0019 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.0019\n",
      "Batch: 30 \t Epoch : 2\tNet Loss: 6.9937 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.9937\n",
      "Batch: 31 \t Epoch : 2\tNet Loss: 7.0059 \tAnswer Loss: 0.6521 \tQuestion Loss: 7.0059\n",
      "Batch: 32 \t Epoch : 2\tNet Loss: 6.9256 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9256\n",
      "Batch: 33 \t Epoch : 2\tNet Loss: 6.9640 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.9640\n",
      "Batch: 34 \t Epoch : 2\tNet Loss: 6.9810 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9810\n",
      "Batch: 35 \t Epoch : 2\tNet Loss: 7.0047 \tAnswer Loss: 0.6523 \tQuestion Loss: 7.0047\n",
      "Batch: 36 \t Epoch : 2\tNet Loss: 6.9169 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.9169\n",
      "Batch: 37 \t Epoch : 2\tNet Loss: 6.9358 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.9358\n",
      "Batch: 38 \t Epoch : 2\tNet Loss: 6.8988 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.8988\n",
      "Batch: 39 \t Epoch : 2\tNet Loss: 6.9633 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9633\n",
      "Batch: 40 \t Epoch : 2\tNet Loss: 6.9476 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9476\n",
      "Batch: 41 \t Epoch : 2\tNet Loss: 6.9749 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.9749\n",
      "Batch: 42 \t Epoch : 2\tNet Loss: 6.9809 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.9809\n",
      "Batch: 43 \t Epoch : 2\tNet Loss: 6.9600 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9600\n",
      "Batch: 44 \t Epoch : 2\tNet Loss: 6.9521 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9521\n",
      "Batch: 45 \t Epoch : 2\tNet Loss: 6.9004 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.9004\n",
      "Batch: 46 \t Epoch : 2\tNet Loss: 6.8702 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.8702\n",
      "Batch: 47 \t Epoch : 2\tNet Loss: 6.8282 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.8282\n",
      "Batch: 48 \t Epoch : 2\tNet Loss: 6.8883 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.8883\n",
      "Batch: 49 \t Epoch : 2\tNet Loss: 6.9807 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.9807\n",
      "Average Loss after Epoch 2 : 5.5932\n",
      "Epoch time: 46.70s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 341.0983\tQuestion Loss (full generated): 280.5089\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.8905\tQuestion Loss Masked(full generated): 7.4012\n",
      "Average loss (full gen): 280.5089\n",
      "Eval time: 0.62s\n",
      "T.F. Generated: what year the <UNK> be <UNK> <UNK> was the <UNK> ? <UNK> ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what was the cause of the <UNK> in the summer of 2007 ?                       \n",
      "\n",
      "\n",
      "T.F. Generated: what ? be was the year of there ? was  of be  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: due to <UNK> , what species are many <UNK> dogs related to ?                       \n",
      "\n",
      "\n",
      "T.F. Generated: what year ? type was was the ? ? the <UNK> <UNK> ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what song did <UNK> <UNK> first release after being on american idol ?                       \n",
      "\n",
      "\n",
      "T.F. Generated: what year is type <UNK> the ? ? new  the ? ? was <UNK> a was the york take  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what theater did the second australian imperial force come from before they <UNK> the reserve <UNK> in new guinea ?                \n",
      "\n",
      "\n",
      "T.F. Generated: what was the <UNK> was <UNK> 's the of  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: who was the president of france in 1953 ?                           \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 340.4284\tQuestion Loss (full generated): 280.3912\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.8739\tQuestion Loss Masked(full generated): 7.3958\n",
      "Average loss (full gen): 280.3912\n",
      "Eval time: 0.60s\n",
      "T.F. Generated: what year of <UNK> <UNK> a of  <UNK> a a be  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what percentage of the national gdp does the federal district produce ?                        \n",
      "\n",
      "\n",
      "T.F. Generated: what year the <UNK> ? <UNK> was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what is the population of <UNK> ?                             \n",
      "\n",
      "\n",
      "No. of batches in training data: 50, with batch_size: 128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 3\tNet Loss: 6.8535 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.8535\n",
      "Batch: 1 \t Epoch : 3\tNet Loss: 6.8073 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.8073\n",
      "Batch: 2 \t Epoch : 3\tNet Loss: 6.8183 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.8183\n",
      "Batch: 3 \t Epoch : 3\tNet Loss: 6.8036 \tAnswer Loss: 0.6519 \tQuestion Loss: 6.8036\n",
      "Batch: 4 \t Epoch : 3\tNet Loss: 6.8017 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.8017\n",
      "Batch: 5 \t Epoch : 3\tNet Loss: 6.8125 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.8125\n",
      "Batch: 6 \t Epoch : 3\tNet Loss: 6.8427 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.8427\n",
      "Batch: 7 \t Epoch : 3\tNet Loss: 6.7987 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.7987\n",
      "Batch: 8 \t Epoch : 3\tNet Loss: 6.7302 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.7302\n",
      "Batch: 9 \t Epoch : 3\tNet Loss: 6.8142 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.8142\n",
      "Batch: 10 \t Epoch : 3\tNet Loss: 6.7695 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.7695\n",
      "Batch: 11 \t Epoch : 3\tNet Loss: 6.7408 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.7408\n",
      "Batch: 12 \t Epoch : 3\tNet Loss: 6.7241 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.7241\n",
      "Batch: 13 \t Epoch : 3\tNet Loss: 6.7937 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.7937\n",
      "Batch: 14 \t Epoch : 3\tNet Loss: 6.7573 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.7573\n",
      "Batch: 15 \t Epoch : 3\tNet Loss: 6.7363 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.7363\n",
      "Batch: 16 \t Epoch : 3\tNet Loss: 6.7407 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.7407\n",
      "Batch: 17 \t Epoch : 3\tNet Loss: 6.7563 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.7563\n",
      "Batch: 18 \t Epoch : 3\tNet Loss: 6.7633 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.7633\n",
      "Batch: 19 \t Epoch : 3\tNet Loss: 6.7216 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.7216\n",
      "Batch: 20 \t Epoch : 3\tNet Loss: 6.6900 \tAnswer Loss: 0.6524 \tQuestion Loss: 6.6900\n",
      "Batch: 21 \t Epoch : 3\tNet Loss: 6.7529 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.7529\n",
      "Batch: 22 \t Epoch : 3\tNet Loss: 6.7657 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.7657\n",
      "Batch: 23 \t Epoch : 3\tNet Loss: 6.7376 \tAnswer Loss: 0.6519 \tQuestion Loss: 6.7376\n",
      "Batch: 24 \t Epoch : 3\tNet Loss: 6.6930 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.6930\n",
      "Batch: 25 \t Epoch : 3\tNet Loss: 6.6641 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.6641\n",
      "Batch: 26 \t Epoch : 3\tNet Loss: 6.6268 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.6268\n",
      "Batch: 27 \t Epoch : 3\tNet Loss: 6.6615 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.6615\n",
      "Batch: 28 \t Epoch : 3\tNet Loss: 6.6785 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.6785\n",
      "Batch: 29 \t Epoch : 3\tNet Loss: 6.6417 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.6417\n",
      "Batch: 30 \t Epoch : 3\tNet Loss: 6.6350 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.6350\n",
      "Batch: 31 \t Epoch : 3\tNet Loss: 6.6487 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.6487\n",
      "Batch: 32 \t Epoch : 3\tNet Loss: 6.5606 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.5606\n",
      "Batch: 33 \t Epoch : 3\tNet Loss: 6.7405 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.7405\n",
      "Batch: 34 \t Epoch : 3\tNet Loss: 6.5641 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.5641\n",
      "Batch: 35 \t Epoch : 3\tNet Loss: 6.6346 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.6346\n",
      "Batch: 36 \t Epoch : 3\tNet Loss: 6.5579 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.5579\n",
      "Batch: 37 \t Epoch : 3\tNet Loss: 6.5872 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.5872\n",
      "Batch: 38 \t Epoch : 3\tNet Loss: 6.5373 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.5373\n",
      "Batch: 39 \t Epoch : 3\tNet Loss: 6.6407 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.6407\n",
      "Batch: 40 \t Epoch : 3\tNet Loss: 6.6077 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.6077\n",
      "Batch: 41 \t Epoch : 3\tNet Loss: 6.5736 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.5736\n",
      "Batch: 42 \t Epoch : 3\tNet Loss: 6.5804 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.5804\n",
      "Batch: 43 \t Epoch : 3\tNet Loss: 6.5728 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.5728\n",
      "Batch: 44 \t Epoch : 3\tNet Loss: 6.5911 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.5911\n",
      "Batch: 45 \t Epoch : 3\tNet Loss: 6.5379 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.5379\n",
      "Batch: 46 \t Epoch : 3\tNet Loss: 6.5332 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.5332\n",
      "Batch: 47 \t Epoch : 3\tNet Loss: 6.5154 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.5154\n",
      "Batch: 48 \t Epoch : 3\tNet Loss: 6.5411 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.5411\n",
      "Batch: 49 \t Epoch : 3\tNet Loss: 6.5316 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.5316\n",
      "Average Loss after Epoch 3 : 5.3046\n",
      "Epoch time: 45.60s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 334.2666\tQuestion Loss (full generated): 273.5217\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5565\tQuestion Loss Masked(full generated): 7.1630\n",
      "Average loss (full gen): 273.5217\n",
      "Eval time: 0.66s\n",
      "T.F. Generated: what year was ? was ? the ? <UNK> sphere  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what <UNK> turned author established a post on tuvalu ?                          \n",
      "\n",
      "\n",
      "T.F. Generated: what year the <UNK> ? of of ? ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what was southeast asia commonly known as before ?                           \n",
      "\n",
      "\n",
      "T.F. Generated: what year ? the and ?  the york was the <UNK> ?   protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what magazine described portugal as `` a new <UNK> man of europe ? ''                      \n",
      "\n",
      "\n",
      "T.F. Generated: what many of there was be ? <UNK>  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: how often are elections help for parliament ?                            \n",
      "\n",
      "\n",
      "T.F. Generated: what year the <UNK> of <UNK> of 1937 was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what is the estimate of current mali <UNK> ?                           \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 333.1404\tQuestion Loss (full generated): 272.7375\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4977\tQuestion Loss Masked(full generated): 7.1054\n",
      "Average loss (full gen): 272.7375\n",
      "Eval time: 0.65s\n",
      "T.F. Generated: what year  easter is the long be  be is  <UNK> was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what does popper believe is essential to do to theories instead of <UNK> ?                      \n",
      "\n",
      "\n",
      "T.F. Generated: what year of was <UNK> <UNK> <UNK> <UNK> ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters an the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what company <UNK> the votes on american idol ?                           \n",
      "\n",
      "\n",
      "No. of batches in training data: 50, with batch_size: 128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 4\tNet Loss: 6.4865 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.4865\n",
      "Batch: 1 \t Epoch : 4\tNet Loss: 6.4610 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.4610\n",
      "Batch: 2 \t Epoch : 4\tNet Loss: 6.4744 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.4744\n",
      "Batch: 3 \t Epoch : 4\tNet Loss: 6.4762 \tAnswer Loss: 0.6519 \tQuestion Loss: 6.4762\n",
      "Batch: 4 \t Epoch : 4\tNet Loss: 6.5020 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.5020\n",
      "Batch: 5 \t Epoch : 4\tNet Loss: 6.4687 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.4687\n",
      "Batch: 6 \t Epoch : 4\tNet Loss: 6.3860 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.3860\n",
      "Batch: 7 \t Epoch : 4\tNet Loss: 6.4596 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.4596\n",
      "Batch: 8 \t Epoch : 4\tNet Loss: 6.4308 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.4308\n",
      "Batch: 9 \t Epoch : 4\tNet Loss: 6.5147 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.5147\n",
      "Batch: 10 \t Epoch : 4\tNet Loss: 6.4295 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.4295\n",
      "Batch: 11 \t Epoch : 4\tNet Loss: 6.4206 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.4206\n",
      "Batch: 12 \t Epoch : 4\tNet Loss: 6.3860 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.3860\n",
      "Batch: 13 \t Epoch : 4\tNet Loss: 6.4569 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.4569\n",
      "Batch: 14 \t Epoch : 4\tNet Loss: 6.4093 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.4093\n",
      "Batch: 15 \t Epoch : 4\tNet Loss: 6.4652 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.4652\n",
      "Batch: 16 \t Epoch : 4\tNet Loss: 6.3908 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.3908\n",
      "Batch: 17 \t Epoch : 4\tNet Loss: 6.4326 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.4326\n",
      "Batch: 18 \t Epoch : 4\tNet Loss: 6.4325 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.4325\n",
      "Batch: 19 \t Epoch : 4\tNet Loss: 6.4420 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.4420\n",
      "Batch: 20 \t Epoch : 4\tNet Loss: 6.3477 \tAnswer Loss: 0.6524 \tQuestion Loss: 6.3477\n",
      "Batch: 21 \t Epoch : 4\tNet Loss: 6.4038 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.4038\n",
      "Batch: 22 \t Epoch : 4\tNet Loss: 6.4405 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.4405\n",
      "Batch: 23 \t Epoch : 4\tNet Loss: 6.5088 \tAnswer Loss: 0.6519 \tQuestion Loss: 6.5088\n",
      "Batch: 24 \t Epoch : 4\tNet Loss: 6.3619 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.3619\n",
      "Batch: 25 \t Epoch : 4\tNet Loss: 6.3171 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.3171\n",
      "Batch: 26 \t Epoch : 4\tNet Loss: 6.2899 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.2899\n",
      "Batch: 27 \t Epoch : 4\tNet Loss: 6.3322 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.3322\n",
      "Batch: 28 \t Epoch : 4\tNet Loss: 6.3055 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.3055\n",
      "Batch: 29 \t Epoch : 4\tNet Loss: 6.3609 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.3609\n",
      "Batch: 30 \t Epoch : 4\tNet Loss: 6.4008 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.4008\n",
      "Batch: 31 \t Epoch : 4\tNet Loss: 6.3205 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.3205\n",
      "Batch: 32 \t Epoch : 4\tNet Loss: 6.2651 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2651\n",
      "Batch: 33 \t Epoch : 4\tNet Loss: 6.3265 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.3265\n",
      "Batch: 34 \t Epoch : 4\tNet Loss: 6.2329 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2329\n",
      "Batch: 35 \t Epoch : 4\tNet Loss: 6.3209 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.3209\n",
      "Batch: 36 \t Epoch : 4\tNet Loss: 6.2638 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.2638\n",
      "Batch: 37 \t Epoch : 4\tNet Loss: 6.2589 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.2589\n",
      "Batch: 38 \t Epoch : 4\tNet Loss: 6.2020 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2020\n",
      "Batch: 39 \t Epoch : 4\tNet Loss: 6.2506 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2506\n",
      "Batch: 40 \t Epoch : 4\tNet Loss: 6.2587 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2587\n",
      "Batch: 41 \t Epoch : 4\tNet Loss: 6.2548 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.2548\n",
      "Batch: 42 \t Epoch : 4\tNet Loss: 6.2883 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.2883\n",
      "Batch: 43 \t Epoch : 4\tNet Loss: 6.2772 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2772\n",
      "Batch: 44 \t Epoch : 4\tNet Loss: 6.2700 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2700\n",
      "Batch: 45 \t Epoch : 4\tNet Loss: 6.2575 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2575\n",
      "Batch: 46 \t Epoch : 4\tNet Loss: 6.2545 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.2545\n",
      "Batch: 47 \t Epoch : 4\tNet Loss: 6.2045 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.2045\n",
      "Batch: 48 \t Epoch : 4\tNet Loss: 6.2203 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.2203\n",
      "Batch: 49 \t Epoch : 4\tNet Loss: 6.2015 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.2015\n",
      "Average Loss after Epoch 4 : 5.0464\n",
      "Epoch time: 45.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 326.6223\tQuestion Loss (full generated): 268.5657\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3160\tQuestion Loss Masked(full generated): 7.0605\n",
      "Average loss (full gen): 268.5657\n",
      "Eval time: 0.66s\n",
      "T.F. Generated: what long the <UNK> was of been people <UNK> <UNK> of ? year  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters were the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: organisms in the <UNK> group have bodies that are divided into what ?                       \n",
      "\n",
      "\n",
      "T.F. Generated: what year the <UNK> was <UNK> was used  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters were the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what was the <UNK> of <UNK> electric ?                            \n",
      "\n",
      "\n",
      "T.F. Generated: what did the <UNK> was the the  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters were the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: when was the <UNK> temple built ?                             \n",
      "\n",
      "\n",
      "T.F. Generated: what year ? the <UNK> was was ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters were the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: what year was the <UNK> <UNK> published ?                            \n",
      "\n",
      "\n",
      "T.F. Generated: what the was was <UNK> was be ? <UNK> ? <UNK> <UNK> ? ?   protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters were the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: where did <UNK> and <UNK> stay until the end of the world war i ?                     \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 326.0883\tQuestion Loss (full generated): 267.6829\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.1191\tQuestion Loss Masked(full generated): 6.9769\n",
      "Average loss (full gen): 267.6829\n",
      "Eval time: 0.61s\n",
      "T.F. Generated: what was ? film ? ? the ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters were the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: who acquired mercury records ( us ) ?                            \n",
      "\n",
      "\n",
      "T.F. Generated: what was the <UNK> ? first <UNK> the ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what year ?  protesters were the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the\n",
      "Ground Truth..: who won the women 's title in 1937 ?                           \n",
      "\n",
      "\n",
      "No. of batches in training data: 50, with batch_size: 128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 5\tNet Loss: 6.1749 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.1749\n",
      "Batch: 1 \t Epoch : 5\tNet Loss: 6.1459 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.1459\n",
      "Batch: 2 \t Epoch : 5\tNet Loss: 6.2197 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.2197\n",
      "Batch: 3 \t Epoch : 5\tNet Loss: 6.1473 \tAnswer Loss: 0.6519 \tQuestion Loss: 6.1473\n",
      "Batch: 4 \t Epoch : 5\tNet Loss: 6.1557 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.1557\n",
      "Batch: 5 \t Epoch : 5\tNet Loss: 6.1568 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.1568\n",
      "Batch: 6 \t Epoch : 5\tNet Loss: 6.2777 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.2777\n",
      "Batch: 7 \t Epoch : 5\tNet Loss: 6.1997 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.1997\n",
      "Batch: 8 \t Epoch : 5\tNet Loss: 6.0613 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0613\n",
      "Batch: 9 \t Epoch : 5\tNet Loss: 6.1732 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.1732\n",
      "Batch: 10 \t Epoch : 5\tNet Loss: 6.1837 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.1837\n",
      "Batch: 11 \t Epoch : 5\tNet Loss: 6.0546 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0546\n",
      "Batch: 12 \t Epoch : 5\tNet Loss: 6.0680 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0680\n",
      "Batch: 13 \t Epoch : 5\tNet Loss: 6.1541 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.1541\n",
      "Batch: 14 \t Epoch : 5\tNet Loss: 6.0967 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.0967\n",
      "Batch: 15 \t Epoch : 5\tNet Loss: 6.0736 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.0736\n",
      "Batch: 16 \t Epoch : 5\tNet Loss: 6.1755 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.1755\n",
      "Batch: 17 \t Epoch : 5\tNet Loss: 6.1474 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.1474\n",
      "Batch: 18 \t Epoch : 5\tNet Loss: 6.2134 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.2134\n",
      "Batch: 19 \t Epoch : 5\tNet Loss: 6.2175 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2175\n",
      "Batch: 20 \t Epoch : 5\tNet Loss: 6.0456 \tAnswer Loss: 0.6524 \tQuestion Loss: 6.0456\n",
      "Batch: 21 \t Epoch : 5\tNet Loss: 6.1473 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.1473\n",
      "Batch: 22 \t Epoch : 5\tNet Loss: 6.1541 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.1541\n",
      "Batch: 23 \t Epoch : 5\tNet Loss: 6.0983 \tAnswer Loss: 0.6519 \tQuestion Loss: 6.0983\n",
      "Batch: 24 \t Epoch : 5\tNet Loss: 6.0617 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.0617\n",
      "Batch: 25 \t Epoch : 5\tNet Loss: 6.1547 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.1547\n",
      "Batch: 26 \t Epoch : 5\tNet Loss: 6.0105 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.0105\n",
      "Batch: 27 \t Epoch : 5\tNet Loss: 6.0369 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.0369\n",
      "Batch: 28 \t Epoch : 5\tNet Loss: 6.0683 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0683\n",
      "Batch: 29 \t Epoch : 5\tNet Loss: 6.0217 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0217\n",
      "Batch: 30 \t Epoch : 5\tNet Loss: 6.0202 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.0202\n",
      "Batch: 31 \t Epoch : 5\tNet Loss: 6.0989 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0989\n",
      "Batch: 32 \t Epoch : 5\tNet Loss: 6.1424 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.1424\n",
      "Batch: 33 \t Epoch : 5\tNet Loss: 6.0374 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.0374\n",
      "Batch: 34 \t Epoch : 5\tNet Loss: 5.9369 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.9369\n",
      "Batch: 35 \t Epoch : 5\tNet Loss: 6.1115 \tAnswer Loss: 0.6523 \tQuestion Loss: 6.1115\n",
      "Batch: 36 \t Epoch : 5\tNet Loss: 5.9945 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.9945\n",
      "Batch: 37 \t Epoch : 5\tNet Loss: 6.0349 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.0349\n",
      "Batch: 38 \t Epoch : 5\tNet Loss: 5.9143 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.9143\n",
      "Batch: 39 \t Epoch : 5\tNet Loss: 6.1497 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.1497\n",
      "Batch: 40 \t Epoch : 5\tNet Loss: 6.0398 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0398\n",
      "Batch: 41 \t Epoch : 5\tNet Loss: 5.9766 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.9766\n",
      "Batch: 42 \t Epoch : 5\tNet Loss: 6.0457 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.0457\n",
      "Batch: 43 \t Epoch : 5\tNet Loss: 6.2624 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.2624\n",
      "Batch: 44 \t Epoch : 5\tNet Loss: 5.9964 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.9964\n",
      "Batch: 45 \t Epoch : 5\tNet Loss: 5.9736 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.9736\n",
      "Batch: 46 \t Epoch : 5\tNet Loss: 5.9457 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.9457\n",
      "Batch: 47 \t Epoch : 5\tNet Loss: 5.8870 \tAnswer Loss: 0.6523 \tQuestion Loss: 5.8870\n",
      "Batch: 48 \t Epoch : 5\tNet Loss: 5.9482 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.9482\n",
      "Batch: 49 \t Epoch : 5\tNet Loss: 6.1367 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.1367\n",
      "Average Loss after Epoch 5 : 4.8341\n",
      "Epoch time: 45.31s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 321.0934\tQuestion Loss (full generated): 259.1495\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.0083\tQuestion Loss Masked(full generated): 6.5783\n",
      "Average loss (full gen): 259.1495\n",
      "Eval time: 0.60s\n",
      "T.F. Generated: what was <UNK> <UNK> the <UNK> the of ? ? <UNK> of is  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what is the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: <UNK> that are constructed on a single integrated circuit are called what ?                       \n",
      "\n",
      "\n",
      "T.F. Generated: what is ? of is ? ? ? the was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what is the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what magazine called schwarzenegger america 's most famous <UNK> ?                          \n",
      "\n",
      "\n",
      "T.F. Generated: what is  <UNK> of was <UNK> ? ? the <UNK> <UNK> ? was ? <UNK>  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what is the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what kind of mining did the world 's first institute of technology <UNK> students for ?                    \n",
      "\n",
      "\n",
      "T.F. Generated: what was many <UNK> <UNK> to many <UNK> many <UNK> ? the <UNK> city <UNK> associations be to ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what is the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: who felt that the further examination and knowledge of studies in the arena of humanism could further art ?                 \n",
      "\n",
      "\n",
      "T.F. Generated: what ?  <UNK> ? 's 's ? was was the the was <UNK> of ? the  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what is the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: some thought that king edward vii 's <UNK> <UNK> were at <UNK> with whose prior work ?                   \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 317.4835\tQuestion Loss (full generated): 258.6812\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.9157\tQuestion Loss Masked(full generated): 6.5687\n",
      "Average loss (full gen): 258.6812\n",
      "Eval time: 0.61s\n",
      "T.F. Generated: what <UNK> <UNK> was the is the be been the used be the the the the the the  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what is the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: along with <UNK> , what city would have been connected to miami via florida high speed rail ?                  \n",
      "\n",
      "\n",
      "T.F. Generated: what is the <UNK> the  ? the ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what is the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what was the downtown seattle population in 2009 ?                           \n",
      "\n",
      "\n",
      "No. of batches in training data: 50, with batch_size: 128 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 6\tNet Loss: 5.9112 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.9112\n",
      "Batch: 1 \t Epoch : 6\tNet Loss: 5.9406 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.9406\n",
      "Batch: 2 \t Epoch : 6\tNet Loss: 5.9029 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.9029\n",
      "Batch: 3 \t Epoch : 6\tNet Loss: 6.0487 \tAnswer Loss: 0.6519 \tQuestion Loss: 6.0487\n",
      "Batch: 4 \t Epoch : 6\tNet Loss: 5.8764 \tAnswer Loss: 0.6523 \tQuestion Loss: 5.8764\n",
      "Batch: 5 \t Epoch : 6\tNet Loss: 5.9603 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.9603\n",
      "Batch: 6 \t Epoch : 6\tNet Loss: 5.8236 \tAnswer Loss: 0.6523 \tQuestion Loss: 5.8236\n",
      "Batch: 7 \t Epoch : 6\tNet Loss: 5.9234 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.9234\n",
      "Batch: 8 \t Epoch : 6\tNet Loss: 5.8444 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8444\n",
      "Batch: 9 \t Epoch : 6\tNet Loss: 6.0969 \tAnswer Loss: 0.6521 \tQuestion Loss: 6.0969\n",
      "Batch: 10 \t Epoch : 6\tNet Loss: 5.8561 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8561\n",
      "Batch: 11 \t Epoch : 6\tNet Loss: 5.7835 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.7835\n",
      "Batch: 12 \t Epoch : 6\tNet Loss: 5.8296 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8296\n",
      "Batch: 13 \t Epoch : 6\tNet Loss: 5.9279 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.9279\n",
      "Batch: 14 \t Epoch : 6\tNet Loss: 5.8380 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.8380\n",
      "Batch: 15 \t Epoch : 6\tNet Loss: 6.0356 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.0356\n",
      "Batch: 16 \t Epoch : 6\tNet Loss: 6.0344 \tAnswer Loss: 0.6522 \tQuestion Loss: 6.0344\n",
      "Batch: 17 \t Epoch : 6\tNet Loss: 5.9648 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.9648\n",
      "Batch: 18 \t Epoch : 6\tNet Loss: 5.9540 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.9540\n",
      "Batch: 19 \t Epoch : 6\tNet Loss: 5.8174 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8174\n",
      "Batch: 20 \t Epoch : 6\tNet Loss: 5.8654 \tAnswer Loss: 0.6524 \tQuestion Loss: 5.8654\n",
      "Batch: 21 \t Epoch : 6\tNet Loss: 5.9685 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.9685\n",
      "Batch: 22 \t Epoch : 6\tNet Loss: 5.9150 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.9150\n",
      "Batch: 23 \t Epoch : 6\tNet Loss: 5.8595 \tAnswer Loss: 0.6519 \tQuestion Loss: 5.8595\n",
      "Batch: 24 \t Epoch : 6\tNet Loss: 6.0039 \tAnswer Loss: 0.6520 \tQuestion Loss: 6.0039\n",
      "Batch: 25 \t Epoch : 6\tNet Loss: 5.8444 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8444\n",
      "Batch: 26 \t Epoch : 6\tNet Loss: 5.7450 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.7450\n",
      "Batch: 27 \t Epoch : 6\tNet Loss: 5.8731 \tAnswer Loss: 0.6523 \tQuestion Loss: 5.8731\n",
      "Batch: 28 \t Epoch : 6\tNet Loss: 5.7891 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.7891\n",
      "Batch: 29 \t Epoch : 6\tNet Loss: 5.8688 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8688\n",
      "Batch: 30 \t Epoch : 6\tNet Loss: 5.9836 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.9836\n",
      "Batch: 31 \t Epoch : 6\tNet Loss: 5.8616 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8616\n",
      "Batch: 32 \t Epoch : 6\tNet Loss: 5.8220 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.8220\n",
      "Batch: 33 \t Epoch : 6\tNet Loss: 5.8521 \tAnswer Loss: 0.6520 \tQuestion Loss: 5.8521\n",
      "Batch: 34 \t Epoch : 6\tNet Loss: 5.7615 \tAnswer Loss: 0.6521 \tQuestion Loss: 5.7615\n",
      "Batch: 35 \t Epoch : 6\tNet Loss: 5.8012 \tAnswer Loss: 0.6523 \tQuestion Loss: 5.8012\n",
      "Batch: 36 \t Epoch : 6\tNet Loss: 5.7523 \tAnswer Loss: 0.6522 \tQuestion Loss: 5.7523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/NLU/nlu-qgen-project/src/main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;31m# Eval after each epoch from randomly chosen batch of val set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/NLU/nlu-qgen-project/src/main.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(train_data, epoch)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mdoc_ans_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswer_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0manswer_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_encoder_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_ans_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_encoder_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0ma_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         return F.binary_cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 430\u001b[0;31m                                       size_average=self.size_average)\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average)\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%run main.py --no_eval \\\n",
    "--train_data \"/home/ra2630/NLU/train-v1.1.json\" \\\n",
    "--tf_ratio 1.0 \\\n",
    "--tf_ratio_decay_rate 0.99 \\\n",
    "--gpu \\\n",
    "--gen_test \\\n",
    "--gen_test_number 5 \\\n",
    "--gen_train \\\n",
    "--gen_train_number 2 \\\n",
    "--word_tf \\\n",
    "--use_attention \\\n",
    "--use_masked_loss \\\n",
    "--num_epochs 500 \\\n",
    "--batch_size 128 \\\n",
    "--words_to_take 5000 \\\n",
    "--example_to_train 8000 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=32, example_to_train=1000, gen_test=False, gen_test_number=1, gen_train=False, gen_train_number=1, gpu=False, hidden_size=300, load='', load_data='', lr=0.0003, no_eval=True, no_train=True, num_epochs=25, reduced_glove=False, save='', save_data='', seed=42, split_ratio=0.8, tf_ratio=1.0, tf_ratio_decay_rate=1.0, train_data='/home/ra2630/NLU/train-v1.1.json', use_attention=False, use_masked_loss=False, word_tf=False, words_to_take=2000)\n",
      "Original Glove shape: (400003, 300)\n",
      "Reduced Glove shape:  (2000, 300)\n",
      "No. of invalid words/examples: 4858\n",
      "Number of batches =  32\n"
     ]
    }
   ],
   "source": [
    "%run main.py --no_train --no_eval --train_data \"/home/ra2630/NLU/train-v1.1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=8, example_to_train=64, gen_test=True, gen_test_number=2, gen_train=True, gen_train_number=2, gpu=True, hidden_size=300, load='', load_data='', lr=0.0003, no_eval=True, no_train=False, num_epochs=500, reduced_glove=False, save='', save_data='', seed=42, split_ratio=0.8, tf_ratio=1.0, tf_ratio_decay_rate=0.99, train_data='/home/ra2630/NLU/train-v1.1.json', use_attention=True, use_masked_loss=True, word_tf=True, words_to_take=5000)\n"
     ]
    }
   ],
   "source": [
    "%run main.py --no_eval \\\n",
    "--train_data \"/home/ra2630/NLU/train-v1.1.json\" \\\n",
    "--tf_ratio 1.0 \\\n",
    "--tf_ratio_decay_rate 0.99 \\\n",
    "--gpu \\\n",
    "--gen_test \\\n",
    "--gen_test_number 2 \\\n",
    "--gen_train \\\n",
    "--gen_train_number 2 \\\n",
    "--word_tf \\\n",
    "--use_attention \\\n",
    "--use_masked_loss \\\n",
    "--num_epochs 500 \\\n",
    "--batch_size 8 \\\n",
    "--words_to_take 5000 \\\n",
    "--example_to_train 64 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer_labels': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32),\n",
       " 'answer_labels_all': array([[0, 1, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32),\n",
       " 'answer_lengths': array([12, 14, 10,  7, 10, 12, 11, 15,  8, 13, 10, 17, 11, 10, 11, 14, 10,\n",
       "        10, 13, 11, 11, 12, 21, 11, 11, 10, 13, 11, 14,  8, 17,  6, 13, 14,\n",
       "        14,  8,  9, 19,  8,  5, 12, 11, 10, 18,  7, 15,  9,  9, 15, 12, 15,\n",
       "        14, 13, 17,  9, 21, 11, 14, 16, 12, 10, 16, 13, 12, 15, 15, 18, 16,\n",
       "        23, 13,  9, 10, 10, 10,  9,  5, 14,  6, 10,  9, 13, 14, 12, 11,  8,\n",
       "        13, 10,  8, 10, 12, 13, 15, 12, 10, 18, 16, 10,  8, 17, 17, 11,  9,\n",
       "        12, 19, 12, 12, 10, 13, 10, 12, 10, 14, 12, 13, 13,  8,  7, 11, 12,\n",
       "         6, 14, 10, 10,  9, 11, 16, 13, 10], dtype=int32),\n",
       " 'document_lengths': array([123, 225, 150, 111, 114, 141, 198, 126,  34, 105, 395,  45, 117,\n",
       "        242, 185, 161, 104, 157, 114, 138, 130, 251, 131, 112, 139, 139,\n",
       "        170, 108,  91,  98, 140,  99, 133, 169, 211,  60, 146, 106, 167,\n",
       "        155, 134,  56, 122, 204, 168, 147,  92, 117, 204, 138,  96, 152,\n",
       "        242, 138, 189, 180, 223, 111, 180, 154, 110, 128, 137,  84, 340,\n",
       "        183, 129, 138, 129,  73, 137, 190, 190, 178, 122, 131, 102, 138,\n",
       "        107, 101, 156,  96, 124,  44,  99, 179, 111, 100, 173, 129, 133,\n",
       "        148,  94, 107, 131, 233, 144,  99, 114, 124, 154,  99, 113, 184,\n",
       "        100, 117, 181,  85, 150,  56, 141, 135, 154, 171, 249, 125,  73,\n",
       "        102,  80,  59, 112, 118, 102, 153, 102, 208, 110, 222], dtype=int32),\n",
       " 'document_tokens': array([[   3,   49,  513, ...,    2,    2,    2],\n",
       "        [ 207,    0,  554, ...,    2,    2,    2],\n",
       "        [   3,    0,  133, ...,    2,    2,    2],\n",
       "        ...,\n",
       "        [  63,    3, 1360, ...,    2,    2,    2],\n",
       "        [ 143,    3, 2442, ...,    2,    2,    2],\n",
       "        [   9,    3, 2596, ...,    2,    2,    2]]),\n",
       " 'question_input_tokens': array([[   1,   26, 1056, ...,    2,    2,    2],\n",
       "        [   1,   44, 1163, ...,    2,    2,    2],\n",
       "        [   1,   89,  260, ...,    2,    2,    2],\n",
       "        ...,\n",
       "        [   1,   89,   53, ...,    2,    2,    2],\n",
       "        [   1,   26,    0, ...,    2,    2,    2],\n",
       "        [   1,    8,   26, ...,    2,    2,    2]], dtype=int32),\n",
       " 'question_lengths': array([12, 14, 10,  7, 10, 12, 11, 15,  8, 13, 10, 17, 11, 10, 11, 14, 10,\n",
       "        10, 13, 11, 11, 12, 21, 11, 11, 10, 13, 11, 14,  8, 17,  6, 13, 14,\n",
       "        14,  8,  9, 19,  8,  5, 12, 11, 10, 18,  7, 15,  9,  9, 15, 12, 15,\n",
       "        14, 13, 17,  9, 21, 11, 14, 16, 12, 10, 16, 13, 12, 15, 15, 18, 16,\n",
       "        23, 13,  9, 10, 10, 10,  9,  5, 14,  6, 10,  9, 13, 14, 12, 11,  8,\n",
       "        13, 10,  8, 10, 12, 13, 15, 12, 10, 18, 16, 10,  8, 17, 17, 11,  9,\n",
       "        12, 19, 12, 12, 10, 13, 10, 12, 10, 14, 12, 13, 13,  8,  7, 11, 12,\n",
       "         6, 14, 10, 10,  9, 11, 16, 13, 10], dtype=int32),\n",
       " 'question_output_tokens': array([[  26, 1056,  975, ...,    2,    2,    2],\n",
       "        [  44, 1163,    9, ...,    2,    2,    2],\n",
       "        [  89,  260, 1378, ...,    2,    2,    2],\n",
       "        ...,\n",
       "        [  89,   53, 2481, ...,    2,    2,    2],\n",
       "        [  26,    0,   12, ...,    2,    2,    2],\n",
       "        [   8,   26,   92, ...,    2,    2,    2]], dtype=int32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printComp(doc_tokens):\n",
    "    for i in doc_tokens:\n",
    "        print(look_up_token(i), sep = ' ', end = ' ')\n",
    "    print(\"\")\n",
    "def printAnswer():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the resulting battle of <UNK> <UNK> ( <UNK> 1950 ) , the u.s. army <UNK> <UNK> attacks meant to capture the city at the <UNK> <UNK> , <UNK> , and <UNK> . the united states air force ( usaf ) <UNK> <UNK> <UNK> with 40 daily ground support <UNK> that destroyed 32 <UNK> , <UNK> most <UNK> road and rail traffic . <UNK> forces were forced to <UNK> in <UNK> by day and move only at night . to <UNK> <UNK> to the <UNK> , the usaf destroyed <UNK> <UNK> , petroleum <UNK> , and <UNK> , while the u.s. navy air forces attacked transport <UNK> . consequently , the <UNK> <UNK> could not be supplied throughout the south . on 27 august , <UNK> fighter <UNK> aircraft <UNK> attacked facilities in chinese territory and the soviet union called the un security council 's attention to china 's <UNK> about the incident . the us proposed that a commission of india and sweden determine what the us should pay in compensation but the soviets <UNK> the us proposal . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> \n"
     ]
    }
   ],
   "source": [
    "printComp(batches[22]['document_tokens'][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the resulting battle of <UNK> <UNK> ( <UNK> 1950 ) , the u.s. army <UNK> <UNK> attacks meant to capture the city at the <UNK> <UNK> , <UNK> , and <UNK> . the united states air force ( usaf ) <UNK> <UNK> <UNK> with 40 daily ground support <UNK> that destroyed 32 <UNK> , <UNK> most <UNK> road and rail traffic . <UNK> forces were forced to <UNK> in <UNK> by day and move only at night . to <UNK> <UNK> to the <UNK> , the usaf destroyed <UNK> <UNK> , petroleum <UNK> , and <UNK> , while the u.s. navy air forces attacked transport <UNK> . consequently , the <UNK> <UNK> could not be supplied throughout the south . on 27 august , <UNK> fighter <UNK> aircraft <UNK> attacked facilities in chinese territory and the soviet union called the un security council 's attention to china 's <UNK> about the incident . the us proposed that a commission of india and sweden determine what the us should pay in compensation but the soviets <UNK> the us proposal . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> \n",
      "<START> which army was trying to capture <UNK> and the <UNK> <UNK> ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> \n"
     ]
    }
   ],
   "source": [
    "printComp(batches[22]['document_tokens'][120])\n",
    "printComp(batches[22]['question_input_tokens'][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 300)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
