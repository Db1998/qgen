{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run main.py --no_eval \\\n",
    "--train_data \"/home/ra2630/NLU/train-v1.1.json\" \\\n",
    "--tf_ratio 1.0 \\\n",
    "--tf_ratio_decay_rate 0.99 \\\n",
    "--gpu \\\n",
    "--gen_test \\\n",
    "--gen_test_number 5 \\\n",
    "--gen_train \\\n",
    "--gen_train_number 2 \\\n",
    "--word_tf \\\n",
    "--use_attention \\\n",
    "--use_masked_loss \\\n",
    "--num_epochs 500 \\\n",
    "--batch_size 128 \\\n",
    "--words_to_take 5000 \\\n",
    "--example_to_train 8000 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run main.py --no_train --no_eval --train_data \"/home/ra2630/NLU/train-v1.1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=8, example_to_train=64, gen_test=True, gen_test_number=2, gen_train=True, gen_train_number=2, gpu=True, hidden_size=300, load='', load_data='', lr=0.0003, no_eval=True, no_train=False, num_epochs=500, reduced_glove=False, save='', save_data='', seed=42, split_ratio=0.8, tf_ratio=1.0, tf_ratio_decay_rate=0.99, train_data='/home/ra2630/NLU/train-v1.1.json', use_attention=True, use_masked_loss=True, word_tf=True, words_to_take=5000)\n",
      "No. of invalid words/examples: 4858\n",
      "0 8 [4 2 5 1 3 2 3 4]\n",
      "8 16 [1 2 3 3 1 2 2 2]\n",
      "16 24 [ 2  4 18 19  1  2  5  2]\n",
      "24 32 [2 1 1 1 3 1 1 2]\n",
      "32 40 [ 7  1  3  1  1 17  2  2]\n",
      "40 48 [5 1 1 2 3 2 3 8]\n",
      "48 56 [4 2 2 2 3 1 6 3]\n",
      "56 64 [15  3 10  1  1  2  3  2]\n",
      "Number of batches =  8\n",
      "Glove Shape =  (5000, 300)\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 0\tNet Loss: 8.5445 \tAnswer Loss: 0.6534 \tQuestion Loss: 8.5445\n",
      "Batch: 1 \t Epoch : 0\tNet Loss: 8.4651 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.4651\n",
      "Batch: 2 \t Epoch : 0\tNet Loss: 8.5137 \tAnswer Loss: 0.6526 \tQuestion Loss: 8.5137\n",
      "Batch: 3 \t Epoch : 0\tNet Loss: 8.5171 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.5171\n",
      "Batch: 4 \t Epoch : 0\tNet Loss: 8.5099 \tAnswer Loss: 0.6535 \tQuestion Loss: 8.5099\n",
      "Batch: 5 \t Epoch : 0\tNet Loss: 8.4594 \tAnswer Loss: 0.6538 \tQuestion Loss: 8.4594\n",
      "Average Loss after Epoch 0 : 6.3762\n",
      "Epoch time: 2.05s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 181.1104\tQuestion Loss (full generated): 178.2616\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.4519\tQuestion Loss Masked(full generated): 8.4983\n",
      "Average loss (full gen): 178.2616\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: invade elevation was 65 ceo distinguish particles last was connects ceo expected was battles are might might might might might might\n",
      "Full Generated: invade gulf 2000 books chemistry owned flight peninsula territories so laboratory wearing amounts told patents funafuti advice ] tourist normal phone\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: invade connects come expected states argues 65 opening historic reviews smith are might might might might might might might might might\n",
      "Full Generated: invade gulf 2000 books chemistry owned flight peninsula territories so laboratory wearing amounts told patents funafuti advice ] tourist normal phone\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 183.0202\tQuestion Loss (full generated): 177.8895\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.4608\tQuestion Loss Masked(full generated): 8.5090\n",
      "Average loss (full gen): 177.8895\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: invade replaced 2015 generals expected particles involving was are might might might might might might might might might might might might\n",
      "Full Generated: invade gulf 2000 books chemistry owned flight peninsula territories so laboratory wearing amounts told patents funafuti advice ] tourist normal phone\n",
      "Ground Truth..: which branch practices the teachings of <UNK> ?            \n",
      "\n",
      "\n",
      "T.F. Generated: invade denominations water so 65 expected ge involving reviews was perspective 2014 entrance are might might might might might might might\n",
      "Full Generated: invade gulf 2000 books chemistry owned flight peninsula territories so laboratory wearing amounts told patents funafuti advice ] tourist normal phone\n",
      "Ground Truth..: during whose rule was the use of old <UNK> at its peak ?       \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 1\tNet Loss: 8.3882 \tAnswer Loss: 0.6534 \tQuestion Loss: 8.3882\n",
      "Batch: 1 \t Epoch : 1\tNet Loss: 8.3167 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.3167\n",
      "Batch: 2 \t Epoch : 1\tNet Loss: 8.3815 \tAnswer Loss: 0.6526 \tQuestion Loss: 8.3815\n",
      "Batch: 3 \t Epoch : 1\tNet Loss: 8.3684 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.3684\n",
      "Batch: 4 \t Epoch : 1\tNet Loss: 8.3729 \tAnswer Loss: 0.6535 \tQuestion Loss: 8.3729\n",
      "Batch: 5 \t Epoch : 1\tNet Loss: 8.3216 \tAnswer Loss: 0.6538 \tQuestion Loss: 8.3216\n",
      "Average Loss after Epoch 1 : 6.2687\n",
      "Epoch time: 1.41s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 181.4993\tQuestion Loss (full generated): 176.7238\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.3682\tQuestion Loss Masked(full generated): 8.4047\n",
      "Average loss (full gen): 176.7238\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: besides contribute was the ceo are particles last was 2002 ceo connection was other are might might might might might might\n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: besides cable the connection award ottoman argues the global reviews connection parallel moon dominated connection contribute are might might might might\n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 182.8993\tQuestion Loss (full generated): 176.2184\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.2475\tQuestion Loss Masked(full generated): 8.3671\n",
      "Average loss (full gen): 176.2184\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: besides manuscripts water  was  are might might might might might might might might might might might might might might\n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: what are `` <UNK> '' ?              \n",
      "\n",
      "\n",
      "T.F. Generated: besides had practices the dominated pub the heterosexual las connection pub practices cable better are might might might might might might\n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: which model is of federalism is similar to the federalism model in australia ?      \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 2\tNet Loss: 8.2475 \tAnswer Loss: 0.6534 \tQuestion Loss: 8.2475\n",
      "Batch: 1 \t Epoch : 2\tNet Loss: 8.1361 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.1361\n",
      "Batch: 2 \t Epoch : 2\tNet Loss: 8.2540 \tAnswer Loss: 0.6526 \tQuestion Loss: 8.2540\n",
      "Batch: 3 \t Epoch : 2\tNet Loss: 8.2225 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.2225\n",
      "Batch: 4 \t Epoch : 2\tNet Loss: 8.2544 \tAnswer Loss: 0.6535 \tQuestion Loss: 8.2544\n",
      "Batch: 5 \t Epoch : 2\tNet Loss: 8.1818 \tAnswer Loss: 0.6538 \tQuestion Loss: 8.1818\n",
      "Average Loss after Epoch 2 : 6.1620\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 182.2335\tQuestion Loss (full generated): 175.6986\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.2867\tQuestion Loss Masked(full generated): 8.3445\n",
      "Average loss (full gen): 175.6986\n",
      "Eval time: 0.99s\n",
      "T.F. Generated: besides are was levels brain essentially confusion was recovery dominated papers admiral  might might might might might might might might\n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: besides wounded were n't was las 1956 patents 1956 1956 improvements 1956 papers brain was commander dominated was was was \n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 185.2586\tQuestion Loss (full generated): 175.6904\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.1906\tQuestion Loss Masked(full generated): 8.3468\n",
      "Average loss (full gen): 175.6904\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: besides are the the was was laboratory 2002 was cargo favour survived las displayed 1956 relative dominated cover  might might\n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: what is a <UNK> <UNK> way that <UNK> can get access to marketing and word of mouth ?  \n",
      "\n",
      "\n",
      "T.F. Generated: besides wounded were 1956 pronounced analysis battery  might might might might might might might might might might might might might\n",
      "Full Generated: besides elevation design rating m burma known separation reviews advocated drawing sicily already mr. reverse simply contact enlightenment specialized jurisdictions citizen\n",
      "Ground Truth..: how did the japanese win taiwan ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 3\tNet Loss: 8.1139 \tAnswer Loss: 0.6534 \tQuestion Loss: 8.1139\n",
      "Batch: 1 \t Epoch : 3\tNet Loss: 8.0416 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.0416\n",
      "Batch: 2 \t Epoch : 3\tNet Loss: 8.1432 \tAnswer Loss: 0.6526 \tQuestion Loss: 8.1432\n",
      "Batch: 3 \t Epoch : 3\tNet Loss: 8.0795 \tAnswer Loss: 0.6518 \tQuestion Loss: 8.0795\n",
      "Batch: 4 \t Epoch : 3\tNet Loss: 8.1050 \tAnswer Loss: 0.6535 \tQuestion Loss: 8.1050\n",
      "Batch: 5 \t Epoch : 3\tNet Loss: 8.0519 \tAnswer Loss: 0.6538 \tQuestion Loss: 8.0519\n",
      "Average Loss after Epoch 3 : 6.0669\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 182.9569\tQuestion Loss (full generated): 173.4749\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.2074\tQuestion Loss Masked(full generated): 8.2824\n",
      "Average loss (full gen): 173.4749\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what contribute was the branch are past last was 2002 branch 1956 was other  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football linear transported such branch 2015 roles partnership 60 is the 1956 of dominated copper species kennedy latino\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what are were n't was las 1956 patents 1956 1956 improvements 1956 bound brain was commander dominated was was was \n",
      "Full Generated: what are general football linear transported such branch 2015 roles partnership 60 is the 1956 of dominated copper species kennedy latino\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 186.0195\tQuestion Loss (full generated): 172.4505\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.0287\tQuestion Loss Masked(full generated): 8.2063\n",
      "Average loss (full gen): 172.4505\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what are were origins dominated belong 1956 was 2002 los withdraw was of was  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football linear transported such branch 2015 roles partnership 60 is the 1956 of dominated copper species kennedy latino\n",
      "Ground Truth..: what did american settlers call the <UNK> who attacked fort <UNK> in <UNK> ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what are was offices general branch branch was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football linear transported such branch 2015 roles partnership 60 is the 1956 of dominated copper species kennedy latino\n",
      "Ground Truth..: what <UNK> movements are used for <UNK> ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 4\tNet Loss: 7.9946 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.9946\n",
      "Batch: 1 \t Epoch : 4\tNet Loss: 7.9087 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.9087\n",
      "Batch: 2 \t Epoch : 4\tNet Loss: 8.0078 \tAnswer Loss: 0.6526 \tQuestion Loss: 8.0078\n",
      "Batch: 3 \t Epoch : 4\tNet Loss: 7.9518 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.9518\n",
      "Batch: 4 \t Epoch : 4\tNet Loss: 7.9957 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.9957\n",
      "Batch: 5 \t Epoch : 4\tNet Loss: 7.9200 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.9200\n",
      "Average Loss after Epoch 4 : 5.9723\n",
      "Epoch time: 1.30s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 183.6298\tQuestion Loss (full generated): 172.2074\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.1298\tQuestion Loss Masked(full generated): 8.2120\n",
      "Average loss (full gen): 172.2074\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what are was the branch are past to was 2002 branch 1956 was other  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what of are had were was was belong madaris 1956 was juan given  protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 187.1456\tQuestion Loss (full generated): 170.8978\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.8687\tQuestion Loss Masked(full generated): 8.0835\n",
      "Average loss (full gen): 170.8978\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what 2002 kg 2005 moves  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: who built westminster hall ?               \n",
      "\n",
      "\n",
      "T.F. Generated: what 1956 was was intervention entitled revolt the football 2002 are the of  protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: the <UNK> <UNK> ali khan had a title , what was it ?       \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 5\tNet Loss: 7.8556 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.8556\n",
      "Batch: 1 \t Epoch : 5\tNet Loss: 7.7840 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.7840\n",
      "Batch: 2 \t Epoch : 5\tNet Loss: 7.8887 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.8887\n",
      "Batch: 3 \t Epoch : 5\tNet Loss: 7.8036 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.8036\n",
      "Batch: 4 \t Epoch : 5\tNet Loss: 7.8557 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.8557\n",
      "Batch: 5 \t Epoch : 5\tNet Loss: 7.7901 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.7901\n",
      "Average Loss after Epoch 5 : 5.8722\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 184.2448\tQuestion Loss (full generated): 171.0020\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 8.0540\tQuestion Loss Masked(full generated): 8.1502\n",
      "Average loss (full gen): 171.0020\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what other were 1956 discussed federalism was smith  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what the are had were was was belong dominated 1956 was juan a  protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 188.9785\tQuestion Loss (full generated): 169.9738\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.8032\tQuestion Loss Masked(full generated): 8.0263\n",
      "Average loss (full gen): 169.9738\n",
      "Eval time: 0.43s\n",
      "T.F. Generated: what 2002 feature argued  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "T.F. Generated: what are were 1956 pronounced analysis stock  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what are general football commanders easier items pilot brain antibiotic personality responsible 9 were short were short were short were short\n",
      "Ground Truth..: how did the japanese win taiwan ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 6\tNet Loss: 7.7247 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.7247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 6\tNet Loss: 7.5151 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.5151\n",
      "Batch: 2 \t Epoch : 6\tNet Loss: 7.7848 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.7848\n",
      "Batch: 3 \t Epoch : 6\tNet Loss: 7.6764 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.6764\n",
      "Batch: 4 \t Epoch : 6\tNet Loss: 7.7481 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.7481\n",
      "Batch: 5 \t Epoch : 6\tNet Loss: 7.6658 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.6658\n",
      "Average Loss after Epoch 6 : 5.7644\n",
      "Epoch time: 1.34s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 184.9674\tQuestion Loss (full generated): 176.1949\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.9813\tQuestion Loss Masked(full generated): 8.3644\n",
      "Average loss (full gen): 176.1949\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what other were 1956 discussed were was smith  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what  was levels revolt a confusion was recovery dominated papers admiral  protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 189.3881\tQuestion Loss (full generated): 176.6463\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.6027\tQuestion Loss Masked(full generated): 8.3450\n",
      "Average loss (full gen): 176.6463\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  team  atlantic 1.5 branch 1956 defeated  the 2002  protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what famous team defeated arsenal for the league cup in 2007 ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the 1956 frame dominated clubs of 2002  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what was the currency of greece until 2002 ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 7\tNet Loss: 7.6309 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.6309\n",
      "Batch: 1 \t Epoch : 7\tNet Loss: 7.5579 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.5579\n",
      "Batch: 2 \t Epoch : 7\tNet Loss: 7.6756 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.6756\n",
      "Batch: 3 \t Epoch : 7\tNet Loss: 7.5416 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.5416\n",
      "Batch: 4 \t Epoch : 7\tNet Loss: 7.6317 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.6317\n",
      "Batch: 5 \t Epoch : 7\tNet Loss: 7.5433 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.5433\n",
      "Average Loss after Epoch 7 : 5.6976\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 185.4606\tQuestion Loss (full generated): 175.8614\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.9130\tQuestion Loss Masked(full generated): 8.3393\n",
      "Average loss (full gen): 175.8614\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what influenced from 1956 designed 2002 the the 2002 were smith  protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what other were 1956 discussed were was have  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 191.1895\tQuestion Loss (full generated): 176.6268\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.5650\tQuestion Loss Masked(full generated): 8.3504\n",
      "Average loss (full gen): 176.6268\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what  2002 dominated party  are  the   2002  protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: americans with african ancestry have always been classified as what race ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the the was was  2002 was  were had of and 1956  dominated cover  protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what is a <UNK> <UNK> way that <UNK> can get access to marketing and word of mouth ?  \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 8\tNet Loss: 7.4976 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.4976\n",
      "Batch: 1 \t Epoch : 8\tNet Loss: 7.4100 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.4100\n",
      "Batch: 2 \t Epoch : 8\tNet Loss: 7.5839 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.5839\n",
      "Batch: 3 \t Epoch : 8\tNet Loss: 7.4477 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.4477\n",
      "Batch: 4 \t Epoch : 8\tNet Loss: 7.4930 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.4930\n",
      "Batch: 5 \t Epoch : 8\tNet Loss: 7.4519 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.4519\n",
      "Average Loss after Epoch 8 : 5.6105\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 185.7114\tQuestion Loss (full generated): 175.4348\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.8495\tQuestion Loss Masked(full generated): 8.3131\n",
      "Average loss (full gen): 175.4348\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what the  2002 were was was belong australia 1956 was juan a  protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced from 1956 designed 2002 the the 2002 were football  protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 188.2789\tQuestion Loss (full generated): 176.9757\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.3164\tQuestion Loss Masked(full generated): 8.3737\n",
      "Average loss (full gen): 176.9757\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what other are rank revolt present dominated ? cities given  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: where does valencia rank among spain 's largest cities ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced were defeat 1982 the 1956 of chamber 2002  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: who did eisenhower defeat in the 1956 presidential election ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 9\tNet Loss: 7.3803 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.3803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 9\tNet Loss: 7.2997 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.2997\n",
      "Batch: 2 \t Epoch : 9\tNet Loss: 7.4865 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.4865\n",
      "Batch: 3 \t Epoch : 9\tNet Loss: 7.3356 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.3356\n",
      "Batch: 4 \t Epoch : 9\tNet Loss: 7.3875 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.3875\n",
      "Batch: 5 \t Epoch : 9\tNet Loss: 7.3235 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.3235\n",
      "Average Loss after Epoch 9 : 5.5266\n",
      "Epoch time: 1.33s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 185.7239\tQuestion Loss (full generated): 174.9324\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.7913\tQuestion Loss Masked(full generated): 8.2867\n",
      "Average loss (full gen): 174.9324\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  were the      originally all dominated dominated ?  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what other the 1956 award ottoman 2002 the for were 1956 high moon federalism 1956 a  protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 190.3774\tQuestion Loss (full generated): 175.3559\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.2661\tQuestion Loss Masked(full generated): 8.2586\n",
      "Average loss (full gen): 175.3559\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what  used  was   protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what are `` <UNK> '' ?              \n",
      "\n",
      "\n",
      "T.F. Generated: what  community call the football the of the  australia  protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what musical festival was initiated in 1982 in new haven ?         \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 10\tNet Loss: 7.3342 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.3342\n",
      "Batch: 1 \t Epoch : 10\tNet Loss: 7.1967 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.1967\n",
      "Batch: 2 \t Epoch : 10\tNet Loss: 7.3983 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.3983\n",
      "Batch: 3 \t Epoch : 10\tNet Loss: 7.2563 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.2563\n",
      "Batch: 4 \t Epoch : 10\tNet Loss: 7.3400 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.3400\n",
      "Batch: 5 \t Epoch : 10\tNet Loss: 7.2809 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.2809\n",
      "Average Loss after Epoch 10 : 5.4758\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 185.4612\tQuestion Loss (full generated): 174.3309\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.7383\tQuestion Loss Masked(full generated): 8.2593\n",
      "Average loss (full gen): 174.3309\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what influenced from 1956 designed 2002 the the 2002 were football  protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what  were the      originally all the the ?  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 190.2593\tQuestion Loss (full generated): 174.5976\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.1445\tQuestion Loss Masked(full generated): 8.1972\n",
      "Average loss (full gen): 174.5976\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  the 1956 <UNK> <UNK> clubs 2002 the  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: what was the currency of greece until 2002 ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what  copies was of used is the 1956 during 2002  ? 1956 valley access  protesters protesters protesters protesters\n",
      "Full Generated: what  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?  protesters australia ?\n",
      "Ground Truth..: how many <UNK> cities are present in the districts that make up the kathmandu valley ?    \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 11\tNet Loss: 7.2218 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.2218\n",
      "Batch: 1 \t Epoch : 11\tNet Loss: 7.1029 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.1029\n",
      "Batch: 2 \t Epoch : 11\tNet Loss: 7.3139 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.3139\n",
      "Batch: 3 \t Epoch : 11\tNet Loss: 7.1361 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.1361\n",
      "Batch: 4 \t Epoch : 11\tNet Loss: 7.2976 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.2976\n",
      "Batch: 5 \t Epoch : 11\tNet Loss: 7.1370 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.1370\n",
      "Average Loss after Epoch 11 : 5.4012\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 184.9463\tQuestion Loss (full generated): 176.8182\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.6888\tQuestion Loss Masked(full generated): 8.2606\n",
      "Average loss (full gen): 176.8182\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what the was the branch  past to was influenced branch 1956 was other  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ? were was was belong australia 1956 was had a  protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 190.7984\tQuestion Loss (full generated): 179.2306\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.1654\tQuestion Loss Masked(full generated): 8.2977\n",
      "Average loss (full gen): 179.2306\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what influenced eisenhower argued  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by\n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "T.F. Generated: what  the the was was  2002 was   had fire and word  <UNK> regions  protesters protesters\n",
      "Full Generated: what  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by  protesters dominated by\n",
      "Ground Truth..: what is a <UNK> <UNK> way that <UNK> can get access to marketing and word of mouth ?  \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 12\tNet Loss: 7.0950 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.0950\n",
      "Batch: 1 \t Epoch : 12\tNet Loss: 7.2341 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.2341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 \t Epoch : 12\tNet Loss: 7.1847 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.1847\n",
      "Batch: 3 \t Epoch : 12\tNet Loss: 7.0135 \tAnswer Loss: 0.6518 \tQuestion Loss: 7.0135\n",
      "Batch: 4 \t Epoch : 12\tNet Loss: 7.3460 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.3460\n",
      "Batch: 5 \t Epoch : 12\tNet Loss: 7.1044 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.1044\n",
      "Average Loss after Epoch 12 : 5.3722\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 184.0790\tQuestion Loss (full generated): 165.0244\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.6418\tQuestion Loss Masked(full generated): 7.9059\n",
      "Average loss (full gen): 165.0244\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  was many ? a a was branch <UNK> `` of  protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the 1956 of <UNK> was the 1956 of\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what the was the branch  past to was influenced branch 1956 was other  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the 1956 of <UNK> was the 1956 of\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 185.4359\tQuestion Loss (full generated): 164.8684\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.1279\tQuestion Loss Masked(full generated): 7.9240\n",
      "Average loss (full gen): 164.8684\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what  copies was similar <UNK> exist of the resources  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the 1956 of <UNK> was the 1956 of\n",
      "Ground Truth..: how many <UNK> houses of worship exist in plymouth ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what 2002 were 1956 was age fire 1956 was new  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the 1956 of <UNK> was the 1956 of\n",
      "Ground Truth..: when did the <UNK> surrender to the <UNK> army ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 13\tNet Loss: 7.0258 \tAnswer Loss: 0.6534 \tQuestion Loss: 7.0258\n",
      "Batch: 1 \t Epoch : 13\tNet Loss: 6.9933 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.9933\n",
      "Batch: 2 \t Epoch : 13\tNet Loss: 7.0980 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.0980\n",
      "Batch: 3 \t Epoch : 13\tNet Loss: 6.9162 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.9162\n",
      "Batch: 4 \t Epoch : 13\tNet Loss: 7.1237 \tAnswer Loss: 0.6535 \tQuestion Loss: 7.1237\n",
      "Batch: 5 \t Epoch : 13\tNet Loss: 7.0298 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.0298\n",
      "Average Loss after Epoch 13 : 5.2734\n",
      "Epoch time: 1.32s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 182.9940\tQuestion Loss (full generated): 164.1519\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.5953\tQuestion Loss Masked(full generated): 7.8782\n",
      "Average loss (full gen): 164.1519\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  were japanese was fire <UNK> were <UNK> <UNK>  word ? ? was the <UNK> was was was \n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what the was the branch  ? to was influenced branch <UNK> was other  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 183.8769\tQuestion Loss (full generated): 163.6665\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.0460\tQuestion Loss Masked(full generated): 7.8704\n",
      "Average loss (full gen): 163.6665\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  copies dominated ? a the board football ? ?  protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: how many copies worldwide has queen 's 1995 album sold ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> was was long was was  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 of <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what was the <UNK> statute referendum <UNK> <UNK> ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 14\tNet Loss: 6.9695 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.9695\n",
      "Batch: 1 \t Epoch : 14\tNet Loss: 6.8802 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.8802\n",
      "Batch: 2 \t Epoch : 14\tNet Loss: 7.0487 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.0487\n",
      "Batch: 3 \t Epoch : 14\tNet Loss: 6.8312 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.8312\n",
      "Batch: 4 \t Epoch : 14\tNet Loss: 6.9678 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.9678\n",
      "Batch: 5 \t Epoch : 14\tNet Loss: 6.9932 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.9932\n",
      "Average Loss after Epoch 14 : 5.2113\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 181.9323\tQuestion Loss (full generated): 163.2816\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.5495\tQuestion Loss Masked(full generated): 7.8431\n",
      "Average loss (full gen): 163.2816\n",
      "Eval time: 0.42s\n",
      "T.F. Generated: what the  ? were was was belong <UNK> <UNK> was was a  protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  were the      the a the the ?  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 185.0441\tQuestion Loss (full generated): 160.7593\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.8343\tQuestion Loss Masked(full generated): 7.6551\n",
      "Average loss (full gen): 160.7593\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  dominated 2002 the this was <UNK> classified  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what occurs when a wrestler <UNK> the ring ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what  used  was   protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what are `` <UNK> '' ?              \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 15\tNet Loss: 6.8762 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.8762\n",
      "Batch: 1 \t Epoch : 15\tNet Loss: 6.8948 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.8948\n",
      "Batch: 2 \t Epoch : 15\tNet Loss: 7.0998 \tAnswer Loss: 0.6526 \tQuestion Loss: 7.0998\n",
      "Batch: 3 \t Epoch : 15\tNet Loss: 6.7553 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.7553\n",
      "Batch: 4 \t Epoch : 15\tNet Loss: 6.9765 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.9765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 5 \t Epoch : 15\tNet Loss: 7.0019 \tAnswer Loss: 0.6538 \tQuestion Loss: 7.0019\n",
      "Average Loss after Epoch 15 : 5.2006\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 180.7163\tQuestion Loss (full generated): 162.4471\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.5049\tQuestion Loss Masked(full generated): 7.8092\n",
      "Average loss (full gen): 162.4471\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what   the      the a the the ?  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what were  <UNK> american were was   protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 183.7173\tQuestion Loss (full generated): 158.9627\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.7918\tQuestion Loss Masked(full generated): 7.5328\n",
      "Average loss (full gen): 158.9627\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what   <UNK> dominated  <UNK> was influenced fort were was the was  protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what did american settlers call the <UNK> who attacked fort <UNK> in <UNK> ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what    a water was the  protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters protesters\n",
      "Full Generated: what  protesters dominated by whom influenced eisenhower defeat 1982 the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what did these three categories <UNK> with ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 16\tNet Loss: 6.7986 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.7986\n",
      "Batch: 1 \t Epoch : 16\tNet Loss: 6.8174 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.8174\n",
      "Batch: 2 \t Epoch : 16\tNet Loss: 6.8800 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.8800\n",
      "Batch: 3 \t Epoch : 16\tNet Loss: 6.6554 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.6554\n",
      "Batch: 4 \t Epoch : 16\tNet Loss: 6.9680 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.9680\n",
      "Batch: 5 \t Epoch : 16\tNet Loss: 6.8386 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.8386\n",
      "Average Loss after Epoch 16 : 5.1197\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 179.3594\tQuestion Loss (full generated): 160.0618\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.4613\tQuestion Loss Masked(full generated): 7.6692\n",
      "Average loss (full gen): 160.0618\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what were the <UNK> award award a the 1956 were <UNK> high moon <UNK> <UNK> a  houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ?  was was fire <UNK> <UNK> was was a  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 180.8778\tQuestion Loss (full generated): 156.8157\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5886\tQuestion Loss Masked(full generated): 7.4393\n",
      "Average loss (full gen): 156.8157\n",
      "Eval time: 0.43s\n",
      "T.F. Generated: what  <UNK>  ?  today  similar fire  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what species do all living humans today belong to ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what were the ? football located the  houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where were most jazz clubs located ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 17\tNet Loss: 6.8649 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.8649\n",
      "Batch: 1 \t Epoch : 17\tNet Loss: 6.7642 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.7642\n",
      "Batch: 2 \t Epoch : 17\tNet Loss: 6.8451 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.8451\n",
      "Batch: 3 \t Epoch : 17\tNet Loss: 6.7325 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.7325\n",
      "Batch: 4 \t Epoch : 17\tNet Loss: 6.7530 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.7530\n",
      "Batch: 5 \t Epoch : 17\tNet Loss: 6.8301 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.8301\n",
      "Average Loss after Epoch 17 : 5.0987\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 178.0410\tQuestion Loss (full generated): 159.1841\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.4193\tQuestion Loss Masked(full generated): 7.6325\n",
      "Average loss (full gen): 159.1841\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what   japanese was the <UNK> were <UNK> <UNK>  word ? the was the <UNK> was was was \n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what  was many the a a was branch <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 177.1545\tQuestion Loss (full generated): 156.7781\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5432\tQuestion Loss Masked(full generated): 7.4943\n",
      "Average loss (full gen): 156.7781\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what  the <UNK> ? kg the <UNK> ? <UNK> was from  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> was  the  houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what is the <UNK> equal sign ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 18\tNet Loss: 6.6356 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.6356\n",
      "Batch: 1 \t Epoch : 18\tNet Loss: 6.5974 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.5974\n",
      "Batch: 2 \t Epoch : 18\tNet Loss: 6.9659 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.9659\n",
      "Batch: 3 \t Epoch : 18\tNet Loss: 6.4662 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.4662\n",
      "Batch: 4 \t Epoch : 18\tNet Loss: 6.7680 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.7680\n",
      "Batch: 5 \t Epoch : 18\tNet Loss: 6.6838 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.6838\n",
      "Average Loss after Epoch 18 : 5.0146\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 176.5722\tQuestion Loss (full generated): 158.2984\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.3774\tQuestion Loss Masked(full generated): 7.5954\n",
      "Average loss (full gen): 158.2984\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what   the      the a the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what were  <UNK> american were was   houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 177.8598\tQuestion Loss (full generated): 154.1559\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5757\tQuestion Loss Masked(full generated): 7.2778\n",
      "Average loss (full gen): 154.1559\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what influenced believed the was was the the independently <UNK> was <UNK> of age ? the ?  houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "T.F. Generated: what  was are present were  was  houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what <UNK> movements are used for <UNK> ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 19\tNet Loss: 6.5849 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.5849\n",
      "Batch: 1 \t Epoch : 19\tNet Loss: 6.6485 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.6485\n",
      "Batch: 2 \t Epoch : 19\tNet Loss: 6.7334 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.7334\n",
      "Batch: 3 \t Epoch : 19\tNet Loss: 6.4146 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.4146\n",
      "Batch: 4 \t Epoch : 19\tNet Loss: 6.6253 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.6253\n",
      "Batch: 5 \t Epoch : 19\tNet Loss: 6.6226 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.6226\n",
      "Average Loss after Epoch 19 : 4.9537\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 174.8062\tQuestion Loss (full generated): 157.4368\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.3366\tQuestion Loss Masked(full generated): 7.5595\n",
      "Average loss (full gen): 157.4368\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what were the <UNK> award an a the the were <UNK> high moon <UNK> <UNK> a  houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what   the      the a the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 172.4949\tQuestion Loss (full generated): 154.5940\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3933\tQuestion Loss Masked(full generated): 7.3946\n",
      "Average loss (full gen): 154.5940\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what present <UNK> was ? suspended   present suspended trading of <UNK> was stock ?  houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: besides the <UNK> stock exchange , what other exchange suspended trading of <UNK> china stock ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> empire ? the dominated by whom influenced  houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: the seleucid empire was mostly dominated by whom ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 20\tNet Loss: 6.6780 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.6780\n",
      "Batch: 1 \t Epoch : 20\tNet Loss: 6.3985 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.3985\n",
      "Batch: 2 \t Epoch : 20\tNet Loss: 6.5767 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.5767\n",
      "Batch: 3 \t Epoch : 20\tNet Loss: 6.6633 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.6633\n",
      "Batch: 4 \t Epoch : 20\tNet Loss: 6.6218 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.6218\n",
      "Batch: 5 \t Epoch : 20\tNet Loss: 6.6376 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.6376\n",
      "Average Loss after Epoch 20 : 4.9470\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 173.5023\tQuestion Loss (full generated): 156.6432\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.2971\tQuestion Loss Masked(full generated): 7.5268\n",
      "Average loss (full gen): 156.6432\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what were the <UNK> by an a the the were <UNK> high were <UNK> <UNK> a  houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what were  <UNK> american were was   houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 172.8739\tQuestion Loss (full generated): 151.6639\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3855\tQuestion Loss Masked(full generated): 7.1624\n",
      "Average loss (full gen): 151.6639\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what many ago   <UNK> was begin   houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: how long ago did the <UNK> period begin ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what    title this was <UNK> classified  houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what occurs when a wrestler <UNK> the ring ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 21\tNet Loss: 6.3970 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.3970\n",
      "Batch: 1 \t Epoch : 21\tNet Loss: 6.4838 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.4838\n",
      "Batch: 2 \t Epoch : 21\tNet Loss: 6.5236 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.5236\n",
      "Batch: 3 \t Epoch : 21\tNet Loss: 6.2512 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.2512\n",
      "Batch: 4 \t Epoch : 21\tNet Loss: 6.6373 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.6373\n",
      "Batch: 5 \t Epoch : 21\tNet Loss: 6.4048 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.4048\n",
      "Average Loss after Epoch 21 : 4.8372\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 172.3871\tQuestion Loss (full generated): 155.8635\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.2584\tQuestion Loss Masked(full generated): 7.4945\n",
      "Average loss (full gen): 155.8635\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what influenced were <UNK> plymouth a the title 2002 were football  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> by an a the the were <UNK> high were <UNK> <UNK> a  houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 168.9139\tQuestion Loss (full generated): 152.6591\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2440\tQuestion Loss Masked(full generated): 7.3088\n",
      "Average loss (full gen): 152.6591\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  the <UNK> ? kg the <UNK> ? <UNK> was from  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> rank called from were was  houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what was the valencia cathedral called from <UNK> ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 22\tNet Loss: 6.4528 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.4528\n",
      "Batch: 1 \t Epoch : 22\tNet Loss: 6.4633 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.4633\n",
      "Batch: 2 \t Epoch : 22\tNet Loss: 6.5160 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.5160\n",
      "Batch: 3 \t Epoch : 22\tNet Loss: 6.2701 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.2701\n",
      "Batch: 4 \t Epoch : 22\tNet Loss: 6.4022 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.4022\n",
      "Batch: 5 \t Epoch : 22\tNet Loss: 6.3665 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.3665\n",
      "Average Loss after Epoch 22 : 4.8088\n",
      "Epoch time: 1.34s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 171.3696\tQuestion Loss (full generated): 155.0964\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.2199\tQuestion Loss Masked(full generated): 7.4626\n",
      "Average loss (full gen): 155.0964\n",
      "Eval time: 0.42s\n",
      "T.F. Generated: what many  japanese was the <UNK> were <UNK> <UNK>  word ? a was the <UNK> was was was \n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what the was the the  ? to was influenced the <UNK> was <UNK>  houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 171.0483\tQuestion Loss (full generated): 150.8367\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3135\tQuestion Loss Masked(full generated): 7.1753\n",
      "Average loss (full gen): 150.8367\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what  originally the to <UNK> cities to the <UNK> 's board   houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: what sector originally provided the largest contribution to new haven 's economy ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  the ancestry <UNK> widely  the as the  ?  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses found the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK>\n",
      "Ground Truth..: americans with african ancestry have always been classified as what race ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 23\tNet Loss: 6.3028 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.3028\n",
      "Batch: 1 \t Epoch : 23\tNet Loss: 6.4444 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.4444\n",
      "Batch: 2 \t Epoch : 23\tNet Loss: 6.4445 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.4445\n",
      "Batch: 3 \t Epoch : 23\tNet Loss: 6.1983 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.1983\n",
      "Batch: 4 \t Epoch : 23\tNet Loss: 6.3802 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.3802\n",
      "Batch: 5 \t Epoch : 23\tNet Loss: 6.3706 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.3706\n",
      "Average Loss after Epoch 23 : 4.7676\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 170.1714\tQuestion Loss (full generated): 156.9470\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.1822\tQuestion Loss Masked(full generated): 7.5629\n",
      "Average loss (full gen): 156.9470\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what were the <UNK> by an a the the <UNK> <UNK> <UNK> were <UNK> <UNK> a  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced were <UNK> plymouth a the title many <UNK> was  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 168.5885\tQuestion Loss (full generated): 150.4084\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2255\tQuestion Loss Masked(full generated): 7.0906\n",
      "Average loss (full gen): 150.4084\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what    a living was the  houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did these three categories <UNK> with ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> are was  houses houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where were the activists <UNK> ?              \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 24\tNet Loss: 6.2139 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.2139\n",
      "Batch: 1 \t Epoch : 24\tNet Loss: 6.1514 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.1514\n",
      "Batch: 2 \t Epoch : 24\tNet Loss: 6.3994 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.3994\n",
      "Batch: 3 \t Epoch : 24\tNet Loss: 6.1930 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.1930\n",
      "Batch: 4 \t Epoch : 24\tNet Loss: 6.2821 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.2821\n",
      "Batch: 5 \t Epoch : 24\tNet Loss: 6.1988 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.1988\n",
      "Average Loss after Epoch 24 : 4.6798\n",
      "Epoch time: 1.32s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 169.0740\tQuestion Loss (full generated): 156.2878\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.1450\tQuestion Loss Masked(full generated): 7.5365\n",
      "Average loss (full gen): 156.2878\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what many  japanese was the <UNK>  <UNK> <UNK>  word ? the was the <UNK> was was was \n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what  was did the a a was branch <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 166.3359\tQuestion Loss (full generated): 149.7951\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.1001\tQuestion Loss Masked(full generated): 7.0518\n",
      "Average loss (full gen): 149.7951\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what influenced ? the a title artillery award was the ? artillery was  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: who decided to fire a heavy artillery <UNK> with 8 kg <UNK> ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  used  was   houses houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what are `` <UNK> '' ?              \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 25\tNet Loss: 6.1353 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.1353\n",
      "Batch: 1 \t Epoch : 25\tNet Loss: 6.2020 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.2020\n",
      "Batch: 2 \t Epoch : 25\tNet Loss: 6.5099 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.5099\n",
      "Batch: 3 \t Epoch : 25\tNet Loss: 6.2073 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.2073\n",
      "Batch: 4 \t Epoch : 25\tNet Loss: 6.3607 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.3607\n",
      "Batch: 5 \t Epoch : 25\tNet Loss: 6.1032 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.1032\n",
      "Average Loss after Epoch 25 : 4.6898\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 168.0229\tQuestion Loss (full generated): 155.6301\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.1083\tQuestion Loss Masked(full generated): 7.5103\n",
      "Average loss (full gen): 155.6301\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what  was did the a a was branch <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 165.4390\tQuestion Loss (full generated): 148.5763\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.0887\tQuestion Loss Masked(full generated): 7.0032\n",
      "Average loss (full gen): 148.5763\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what were the <UNK> are was  houses houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where were the activists <UNK> ?              \n",
      "\n",
      "\n",
      "T.F. Generated: what   <UNK> dominated  <UNK> was influenced fort were was the was  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did american settlers call the <UNK> who attacked fort <UNK> in <UNK> ?      \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 26\tNet Loss: 6.1473 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.1473\n",
      "Batch: 1 \t Epoch : 26\tNet Loss: 6.5623 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.5623\n",
      "Batch: 2 \t Epoch : 26\tNet Loss: 6.1975 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.1975\n",
      "Batch: 3 \t Epoch : 26\tNet Loss: 6.1658 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.1658\n",
      "Batch: 4 \t Epoch : 26\tNet Loss: 6.4086 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.4086\n",
      "Batch: 5 \t Epoch : 26\tNet Loss: 6.2586 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.2586\n",
      "Average Loss after Epoch 26 : 4.7175\n",
      "Epoch time: 1.33s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 166.7802\tQuestion Loss (full generated): 154.9157\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.0721\tQuestion Loss Masked(full generated): 7.4799\n",
      "Average loss (full gen): 154.9157\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what many  japanese was the <UNK>  <UNK> <UNK>  word ? the was the <UNK> was was was \n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 161.7006\tQuestion Loss (full generated): 147.2036\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.8089\tQuestion Loss Masked(full generated): 6.8560\n",
      "Average loss (full gen): 147.2036\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what many copies by the ? widely accepted madaris records  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: how many regions in singapore have widely accepted madaris ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  do  ? humans today  to the  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what species do all living humans today belong to ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 27\tNet Loss: 6.3183 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.3183\n",
      "Batch: 1 \t Epoch : 27\tNet Loss: 6.2002 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.2002\n",
      "Batch: 2 \t Epoch : 27\tNet Loss: 6.1622 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.1622\n",
      "Batch: 3 \t Epoch : 27\tNet Loss: 5.9795 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.9795\n",
      "Batch: 4 \t Epoch : 27\tNet Loss: 6.0874 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.0874\n",
      "Batch: 5 \t Epoch : 27\tNet Loss: 6.0633 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.0633\n",
      "Average Loss after Epoch 27 : 4.6014\n",
      "Epoch time: 1.34s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 165.7271\tQuestion Loss (full generated): 154.2945\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.0375\tQuestion Loss Masked(full generated): 7.4544\n",
      "Average loss (full gen): 154.2945\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what influenced ? <UNK> plymouth a the title many the was  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 159.0637\tQuestion Loss (full generated): 148.1201\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.8245\tQuestion Loss Masked(full generated): 6.9655\n",
      "Average loss (full gen): 148.1201\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what present <UNK> was exchange suspended the  <UNK> suspended trading of <UNK> was stock exchange  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: besides the <UNK> stock exchange , what other exchange suspended trading of <UNK> china stock ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> ? of the <UNK> ? <UNK> was from  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 28\tNet Loss: 6.0323 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.0323\n",
      "Batch: 1 \t Epoch : 28\tNet Loss: 6.0183 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.0183\n",
      "Batch: 2 \t Epoch : 28\tNet Loss: 6.0857 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.0857\n",
      "Batch: 3 \t Epoch : 28\tNet Loss: 5.7519 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.7519\n",
      "Batch: 4 \t Epoch : 28\tNet Loss: 5.9858 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.9858\n",
      "Batch: 5 \t Epoch : 28\tNet Loss: 6.1379 \tAnswer Loss: 0.6538 \tQuestion Loss: 6.1379\n",
      "Average Loss after Epoch 28 : 4.5015\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 164.8254\tQuestion Loss (full generated): 153.7102\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 7.0035\tQuestion Loss Masked(full generated): 7.4310\n",
      "Average loss (full gen): 153.7102\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what were the <UNK> by an a the the the <UNK> <UNK> were <UNK> <UNK> the  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 161.3899\tQuestion Loss (full generated): 146.7165\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.9091\tQuestion Loss Masked(full generated): 6.8770\n",
      "Average loss (full gen): 146.7165\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what many the ?  speak was  houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: how much english did tito speak ?             \n",
      "\n",
      "\n",
      "T.F. Generated: what  the title was was  a was   to the and word  <UNK> regions  houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what is a <UNK> <UNK> way that <UNK> can get access to marketing and word of mouth ?  \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 29\tNet Loss: 5.9175 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.9175\n",
      "Batch: 1 \t Epoch : 29\tNet Loss: 5.8242 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.8242\n",
      "Batch: 2 \t Epoch : 29\tNet Loss: 6.4009 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.4009\n",
      "Batch: 3 \t Epoch : 29\tNet Loss: 5.9125 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.9125\n",
      "Batch: 4 \t Epoch : 29\tNet Loss: 5.9924 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.9924\n",
      "Batch: 5 \t Epoch : 29\tNet Loss: 5.8943 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.8943\n",
      "Average Loss after Epoch 29 : 4.4927\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 164.0322\tQuestion Loss (full generated): 153.1425\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.9700\tQuestion Loss Masked(full generated): 7.4091\n",
      "Average loss (full gen): 153.1425\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what the was the the  ? to was influenced the <UNK> was <UNK>  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 159.6192\tQuestion Loss (full generated): 145.1324\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.8355\tQuestion Loss Masked(full generated): 6.8362\n",
      "Average loss (full gen): 145.1324\n",
      "Eval time: 0.44s\n",
      "T.F. Generated: what were the    the was community ?  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where is there nothing like this <UNK> speech community ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  was are used the the was  houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what <UNK> movements are used for <UNK> ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 30\tNet Loss: 5.8724 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.8724\n",
      "Batch: 1 \t Epoch : 30\tNet Loss: 5.8396 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.8396\n",
      "Batch: 2 \t Epoch : 30\tNet Loss: 6.0260 \tAnswer Loss: 0.6526 \tQuestion Loss: 6.0260\n",
      "Batch: 3 \t Epoch : 30\tNet Loss: 6.1720 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.1720\n",
      "Batch: 4 \t Epoch : 30\tNet Loss: 6.0153 \tAnswer Loss: 0.6535 \tQuestion Loss: 6.0153\n",
      "Batch: 5 \t Epoch : 30\tNet Loss: 5.8470 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.8470\n",
      "Average Loss after Epoch 30 : 4.4715\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 163.1853\tQuestion Loss (full generated): 152.6188\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.9372\tQuestion Loss Masked(full generated): 7.3891\n",
      "Average loss (full gen): 152.6188\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what the was the the  ? to was influenced the <UNK> was <UNK>  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what  was did the a a was <UNK> <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 158.3629\tQuestion Loss (full generated): 144.3869\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.7734\tQuestion Loss Masked(full generated): 6.8017\n",
      "Average loss (full gen): 144.3869\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what were the <UNK> are was  houses houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where were the activists <UNK> ?              \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced believed the was was the the  <UNK> was <UNK> of age ? the stock  houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 31\tNet Loss: 5.9112 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.9112\n",
      "Batch: 1 \t Epoch : 31\tNet Loss: 6.0627 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.0627\n",
      "Batch: 2 \t Epoch : 31\tNet Loss: 5.9632 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.9632\n",
      "Batch: 3 \t Epoch : 31\tNet Loss: 5.6368 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6368\n",
      "Batch: 4 \t Epoch : 31\tNet Loss: 5.9474 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.9474\n",
      "Batch: 5 \t Epoch : 31\tNet Loss: 5.9037 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.9037\n",
      "Average Loss after Epoch 31 : 4.4281\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 162.2678\tQuestion Loss (full generated): 152.1223\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.9051\tQuestion Loss Masked(full generated): 7.3699\n",
      "Average loss (full gen): 152.1223\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ?  was was was <UNK> <UNK> was was a  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 153.7351\tQuestion Loss (full generated): 144.9967\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.5574\tQuestion Loss Masked(full generated): 6.8103\n",
      "Average loss (full gen): 144.9967\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what present <UNK> was exchange suspended the  <UNK> suspended trading of <UNK> was stock exchange  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: besides the <UNK> stock exchange , what other exchange suspended trading of <UNK> china stock ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced  defeat arsenal the <UNK> presidential election <UNK>  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: who did eisenhower defeat in the 1956 presidential election ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 32\tNet Loss: 5.7353 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.7353\n",
      "Batch: 1 \t Epoch : 32\tNet Loss: 6.0495 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.0495\n",
      "Batch: 2 \t Epoch : 32\tNet Loss: 5.8223 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.8223\n",
      "Batch: 3 \t Epoch : 32\tNet Loss: 5.5816 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.5816\n",
      "Batch: 4 \t Epoch : 32\tNet Loss: 5.9148 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.9148\n",
      "Batch: 5 \t Epoch : 32\tNet Loss: 5.7174 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.7174\n",
      "Average Loss after Epoch 32 : 4.3526\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 161.4973\tQuestion Loss (full generated): 151.6318\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.8742\tQuestion Loss Masked(full generated): 7.3512\n",
      "Average loss (full gen): 151.6318\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what influenced ? <UNK> plymouth a the title a <UNK> of  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> by an a the the <UNK> <UNK> <UNK> were <UNK> <UNK> the  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 155.1455\tQuestion Loss (full generated): 143.5387\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.5915\tQuestion Loss Masked(full generated): 6.7594\n",
      "Average loss (full gen): 143.5387\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what    title this was <UNK> a  houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what occurs when a wrestler <UNK> the ring ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what many ago   <UNK> was begin   houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: how long ago did the <UNK> period begin ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 33\tNet Loss: 5.7910 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.7910\n",
      "Batch: 1 \t Epoch : 33\tNet Loss: 5.6698 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6698\n",
      "Batch: 2 \t Epoch : 33\tNet Loss: 5.7957 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.7957\n",
      "Batch: 3 \t Epoch : 33\tNet Loss: 6.0974 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.0974\n",
      "Batch: 4 \t Epoch : 33\tNet Loss: 5.7256 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.7256\n",
      "Batch: 5 \t Epoch : 33\tNet Loss: 5.9023 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.9023\n",
      "Average Loss after Epoch 33 : 4.3727\n",
      "Epoch time: 1.34s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 160.8059\tQuestion Loss (full generated): 151.1559\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.8438\tQuestion Loss Masked(full generated): 7.3331\n",
      "Average loss (full gen): 151.1559\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ?  was was was <UNK> <UNK> was was a  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 154.1125\tQuestion Loss (full generated): 142.8590\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.5332\tQuestion Loss Masked(full generated): 6.7275\n",
      "Average loss (full gen): 142.8590\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what influenced ? the a title artillery <UNK> was the kg , was  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: who decided to fire a heavy artillery <UNK> with 8 kg <UNK> ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what a <UNK> the <UNK> model the of the <UNK> model <UNK> the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: which model is of federalism is similar to the federalism model in australia ?      \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 34\tNet Loss: 5.8053 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.8053\n",
      "Batch: 1 \t Epoch : 34\tNet Loss: 5.6444 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6444\n",
      "Batch: 2 \t Epoch : 34\tNet Loss: 5.9736 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.9736\n",
      "Batch: 3 \t Epoch : 34\tNet Loss: 5.7690 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.7690\n",
      "Batch: 4 \t Epoch : 34\tNet Loss: 5.7562 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.7562\n",
      "Batch: 5 \t Epoch : 34\tNet Loss: 5.7394 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.7394\n",
      "Average Loss after Epoch 34 : 4.3360\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 159.6854\tQuestion Loss (full generated): 150.6644\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.8141\tQuestion Loss Masked(full generated): 7.3133\n",
      "Average loss (full gen): 150.6644\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what were  <UNK> suspended <UNK> was   houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what many  khan was the <UNK>  <UNK> <UNK>  the ? the was the <UNK> was was was \n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 149.8038\tQuestion Loss (full generated): 142.8988\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.3757\tQuestion Loss Masked(full generated): 6.7061\n",
      "Average loss (full gen): 142.8988\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  the <UNK> ? of the <UNK> ? <UNK> was from  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> cost  <UNK> title gold membership of the was  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was the <UNK> was\n",
      "Ground Truth..: what is the annual cost of a live gold membership in <UNK> ?       \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 35\tNet Loss: 5.5782 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.5782\n",
      "Batch: 1 \t Epoch : 35\tNet Loss: 5.6193 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6193\n",
      "Batch: 2 \t Epoch : 35\tNet Loss: 5.8332 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.8332\n",
      "Batch: 3 \t Epoch : 35\tNet Loss: 5.3763 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3763\n",
      "Batch: 4 \t Epoch : 35\tNet Loss: 5.5704 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5704\n",
      "Batch: 5 \t Epoch : 35\tNet Loss: 5.6422 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.6422\n",
      "Average Loss after Epoch 35 : 4.2024\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 158.6613\tQuestion Loss (full generated): 155.0297\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.7848\tQuestion Loss Masked(full generated): 7.5273\n",
      "Average loss (full gen): 155.0297\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what many  khan ? the <UNK>  <UNK> <UNK>  the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 151.5732\tQuestion Loss (full generated): 146.4270\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.4778\tQuestion Loss Masked(full generated): 6.9778\n",
      "Average loss (full gen): 146.4270\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what influenced believed the ? ? the the  <UNK> ? <UNK> of age ? the ?  houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "T.F. Generated: what  ? are used the the ?  houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what <UNK> movements are used for <UNK> ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 36\tNet Loss: 5.6762 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.6762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 36\tNet Loss: 5.6102 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6102\n",
      "Batch: 2 \t Epoch : 36\tNet Loss: 5.5317 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.5317\n",
      "Batch: 3 \t Epoch : 36\tNet Loss: 5.4958 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.4958\n",
      "Batch: 4 \t Epoch : 36\tNet Loss: 5.9511 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.9511\n",
      "Batch: 5 \t Epoch : 36\tNet Loss: 5.8695 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.8695\n",
      "Average Loss after Epoch 36 : 4.2668\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 157.6910\tQuestion Loss (full generated): 154.3360\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.7562\tQuestion Loss Masked(full generated): 7.5020\n",
      "Average loss (full gen): 154.3360\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> by an a the the <UNK> <UNK> <UNK> were <UNK> <UNK> that  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 149.4704\tQuestion Loss (full generated): 145.1226\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.3652\tQuestion Loss Masked(full generated): 6.8751\n",
      "Average loss (full gen): 145.1226\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what influenced ? the a title artillery <UNK> ? the kg , ?  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who decided to fire a heavy artillery <UNK> with 8 kg <UNK> ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced the <UNK> the <UNK> <UNK> of <UNK> ?  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who was the leader of the revolt of <UNK> ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 37\tNet Loss: 5.6347 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.6347\n",
      "Batch: 1 \t Epoch : 37\tNet Loss: 5.8015 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.8015\n",
      "Batch: 2 \t Epoch : 37\tNet Loss: 5.8870 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.8870\n",
      "Batch: 3 \t Epoch : 37\tNet Loss: 6.0703 \tAnswer Loss: 0.6518 \tQuestion Loss: 6.0703\n",
      "Batch: 4 \t Epoch : 37\tNet Loss: 5.5833 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5833\n",
      "Batch: 5 \t Epoch : 37\tNet Loss: 5.7510 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.7510\n",
      "Average Loss after Epoch 37 : 4.3410\n",
      "Epoch time: 1.34s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 156.8756\tQuestion Loss (full generated): 153.7702\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.7295\tQuestion Loss Masked(full generated): 7.4836\n",
      "Average loss (full gen): 153.7702\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what the ? the the  ? to ? influenced the <UNK> ? <UNK>  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ?  ? ? was <UNK> <UNK> ? was a  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 146.0194\tQuestion Loss (full generated): 147.0971\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.4413\tQuestion Loss Masked(full generated): 7.1518\n",
      "Average loss (full gen): 147.0971\n",
      "Eval time: 0.43s\n",
      "T.F. Generated: what widely fire ? the over <UNK> <UNK> years ?  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: have transfer <UNK> been increasing over the past years ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  the  ? ? to the migrants ? the ?  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what terms did <UNK> <UNK> use to describe migrants to britain ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 38\tNet Loss: 5.8254 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.8254\n",
      "Batch: 1 \t Epoch : 38\tNet Loss: 5.4384 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.4384\n",
      "Batch: 2 \t Epoch : 38\tNet Loss: 5.9925 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.9925\n",
      "Batch: 3 \t Epoch : 38\tNet Loss: 5.2980 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.2980\n",
      "Batch: 4 \t Epoch : 38\tNet Loss: 5.5916 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5916\n",
      "Batch: 5 \t Epoch : 38\tNet Loss: 5.7554 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.7554\n",
      "Average Loss after Epoch 38 : 4.2377\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 156.2662\tQuestion Loss (full generated): 153.3244\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.7048\tQuestion Loss Masked(full generated): 7.4705\n",
      "Average loss (full gen): 153.3244\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what   khan ? the <UNK>  <UNK> <UNK>  the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what the ? the the  ? to ? influenced the <UNK> ? <UNK>  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 148.0989\tQuestion Loss (full generated): 143.7193\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.3331\tQuestion Loss Masked(full generated): 6.8615\n",
      "Average loss (full gen): 143.7193\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what were the    the ? community ?  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where is there nothing like this <UNK> speech community ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> of <UNK> <UNK> <UNK> influenced guam ?  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what was the name of the general who claimed guam ?         \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 39\tNet Loss: 5.7632 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.7632\n",
      "Batch: 1 \t Epoch : 39\tNet Loss: 5.4974 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.4974\n",
      "Batch: 2 \t Epoch : 39\tNet Loss: 5.8554 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.8554\n",
      "Batch: 3 \t Epoch : 39\tNet Loss: 5.3572 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3572\n",
      "Batch: 4 \t Epoch : 39\tNet Loss: 5.5037 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5037\n",
      "Batch: 5 \t Epoch : 39\tNet Loss: 5.4995 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.4995\n",
      "Average Loss after Epoch 39 : 4.1845\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 155.0653\tQuestion Loss (full generated): 152.7406\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.6811\tQuestion Loss Masked(full generated): 7.4543\n",
      "Average loss (full gen): 152.7406\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  ? did the a a ? <UNK> <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ?  ? ? was <UNK> <UNK> ? was a  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 145.5994\tQuestion Loss (full generated): 142.8200\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.2250\tQuestion Loss Masked(full generated): 6.7985\n",
      "Average loss (full gen): 142.8200\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what <UNK> were the to the <UNK> of <UNK> football ?  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: which rules were influential to the codes of association football ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what    title this ? <UNK> ?  houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what occurs when a wrestler <UNK> the ring ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 40\tNet Loss: 5.2991 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.2991\n",
      "Batch: 1 \t Epoch : 40\tNet Loss: 5.3117 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3117\n",
      "Batch: 2 \t Epoch : 40\tNet Loss: 5.3380 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3380\n",
      "Batch: 3 \t Epoch : 40\tNet Loss: 5.3314 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3314\n",
      "Batch: 4 \t Epoch : 40\tNet Loss: 5.4331 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.4331\n",
      "Batch: 5 \t Epoch : 40\tNet Loss: 5.5805 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.5805\n",
      "Average Loss after Epoch 40 : 4.0367\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 154.3635\tQuestion Loss (full generated): 152.3087\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.6567\tQuestion Loss Masked(full generated): 7.4424\n",
      "Average loss (full gen): 152.3087\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  ? did the a a ? <UNK> <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what   khan ? the <UNK>  <UNK> <UNK>  the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 142.3561\tQuestion Loss (full generated): 142.0407\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.9417\tQuestion Loss Masked(full generated): 6.8646\n",
      "Average loss (full gen): 142.0407\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  the <UNK> of <UNK> until 2002 ?  houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what was the currency of greece until 2002 ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced the hall ?  houses houses houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who built westminster hall ?               \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 41\tNet Loss: 5.2826 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.2826\n",
      "Batch: 1 \t Epoch : 41\tNet Loss: 5.4595 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.4595\n",
      "Batch: 2 \t Epoch : 41\tNet Loss: 5.5144 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.5144\n",
      "Batch: 3 \t Epoch : 41\tNet Loss: 5.6895 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6895\n",
      "Batch: 4 \t Epoch : 41\tNet Loss: 5.6249 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.6249\n",
      "Batch: 5 \t Epoch : 41\tNet Loss: 5.6525 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.6525\n",
      "Average Loss after Epoch 41 : 4.1529\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 153.7667\tQuestion Loss (full generated): 151.8922\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.6339\tQuestion Loss Masked(full generated): 7.4303\n",
      "Average loss (full gen): 151.8922\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what were  <UNK> suspended <UNK> ?   houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what  ? did the a a ? <UNK> <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 140.5455\tQuestion Loss (full generated): 141.9844\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.9883\tQuestion Loss Masked(full generated): 6.8555\n",
      "Average loss (full gen): 141.9844\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what present <UNK> ? exchange suspended the  <UNK> suspended trading of <UNK> ? stock exchange  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: besides the <UNK> stock exchange , what other exchange suspended trading of <UNK> china stock ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> rank called from <UNK> ?  houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what was the valencia cathedral called from <UNK> ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 42\tNet Loss: 5.2544 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.2544\n",
      "Batch: 1 \t Epoch : 42\tNet Loss: 5.1397 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1397\n",
      "Batch: 2 \t Epoch : 42\tNet Loss: 5.7743 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.7743\n",
      "Batch: 3 \t Epoch : 42\tNet Loss: 5.4979 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.4979\n",
      "Batch: 4 \t Epoch : 42\tNet Loss: 5.2281 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.2281\n",
      "Batch: 5 \t Epoch : 42\tNet Loss: 5.5934 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.5934\n",
      "Average Loss after Epoch 42 : 4.0610\n",
      "Epoch time: 1.32s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 153.0145\tQuestion Loss (full generated): 151.3278\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.6117\tQuestion Loss Masked(full generated): 7.4097\n",
      "Average loss (full gen): 151.3278\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  ? did the a a ? <UNK> <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> by an a the the <UNK> <UNK> <UNK> were <UNK> <UNK> that  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 143.1293\tQuestion Loss (full generated): 140.7168\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.1438\tQuestion Loss Masked(full generated): 6.7511\n",
      "Average loss (full gen): 140.7168\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what were the    the ? community ?  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where is there nothing like this <UNK> speech community ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> are ?  houses houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where were the activists <UNK> ?              \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 43\tNet Loss: 5.4474 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.4474\n",
      "Batch: 1 \t Epoch : 43\tNet Loss: 5.2331 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.2331\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 \t Epoch : 43\tNet Loss: 5.3160 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3160\n",
      "Batch: 3 \t Epoch : 43\tNet Loss: 5.1852 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1852\n",
      "Batch: 4 \t Epoch : 43\tNet Loss: 5.2891 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.2891\n",
      "Batch: 5 \t Epoch : 43\tNet Loss: 5.6811 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.6811\n",
      "Average Loss after Epoch 43 : 4.0190\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 152.5406\tQuestion Loss (full generated): 150.9292\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5897\tQuestion Loss Masked(full generated): 7.3969\n",
      "Average loss (full gen): 150.9292\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what influenced ? <UNK> ? a the the a <UNK> of  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what   khan ? the <UNK>  <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 139.2807\tQuestion Loss (full generated): 143.1096\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.1442\tQuestion Loss Masked(full generated): 7.0168\n",
      "Average loss (full gen): 143.1096\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what  <UNK> worldwide has a ? board album sold ?  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how many copies worldwide has queen 's 1995 album sold ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what   <UNK> ? to the <UNK> ? a  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: when did the <UNK> surrender to the <UNK> army ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 44\tNet Loss: 5.4940 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.4940\n",
      "Batch: 1 \t Epoch : 44\tNet Loss: 5.1594 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1594\n",
      "Batch: 2 \t Epoch : 44\tNet Loss: 5.2437 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.2437\n",
      "Batch: 3 \t Epoch : 44\tNet Loss: 4.9799 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.9799\n",
      "Batch: 4 \t Epoch : 44\tNet Loss: 5.7697 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.7697\n",
      "Batch: 5 \t Epoch : 44\tNet Loss: 5.6083 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.6083\n",
      "Average Loss after Epoch 44 : 4.0319\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 151.7008\tQuestion Loss (full generated): 150.4836\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5683\tQuestion Loss Masked(full generated): 7.3847\n",
      "Average loss (full gen): 150.4836\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what the ? the the  ? the ? influenced the <UNK> ? <UNK>  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what   khan ? the <UNK>  <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 137.1168\tQuestion Loss (full generated): 139.9525\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.8315\tQuestion Loss Masked(full generated): 6.7791\n",
      "Average loss (full gen): 139.9525\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what influenced  defeat arsenal the <UNK> presidential election <UNK>  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who did eisenhower defeat in the 1956 presidential election ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> ? sign the  houses houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what is the <UNK> equal sign ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 45\tNet Loss: 5.3079 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.3079\n",
      "Batch: 1 \t Epoch : 45\tNet Loss: 5.5709 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.5709\n",
      "Batch: 2 \t Epoch : 45\tNet Loss: 5.3116 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3116\n",
      "Batch: 3 \t Epoch : 45\tNet Loss: 5.0491 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0491\n",
      "Batch: 4 \t Epoch : 45\tNet Loss: 5.9297 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.9297\n",
      "Batch: 5 \t Epoch : 45\tNet Loss: 5.2282 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.2282\n",
      "Average Loss after Epoch 45 : 4.0497\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 150.9667\tQuestion Loss (full generated): 150.0583\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5479\tQuestion Loss Masked(full generated): 7.3728\n",
      "Average loss (full gen): 150.0583\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what   khan ? the <UNK>  <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what were  <UNK> suspended <UNK> ?   houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 140.0223\tQuestion Loss (full generated): 138.6659\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 5.0168\tQuestion Loss Masked(full generated): 6.6719\n",
      "Average loss (full gen): 138.6659\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what   <UNK> call  <UNK> ? influenced fort the ? the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what did american settlers call the <UNK> who attacked fort <UNK> in <UNK> ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced believed the ? ? the the  <UNK> ? <UNK> of age ? the ?  houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 46\tNet Loss: 5.4835 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.4835\n",
      "Batch: 1 \t Epoch : 46\tNet Loss: 5.3404 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3404\n",
      "Batch: 2 \t Epoch : 46\tNet Loss: 5.8678 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.8678\n",
      "Batch: 3 \t Epoch : 46\tNet Loss: 5.5894 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.5894\n",
      "Batch: 4 \t Epoch : 46\tNet Loss: 5.3192 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.3192\n",
      "Batch: 5 \t Epoch : 46\tNet Loss: 5.0982 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0982\n",
      "Average Loss after Epoch 46 : 4.0873\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 149.6313\tQuestion Loss (full generated): 149.2431\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5287\tQuestion Loss Masked(full generated): 7.3412\n",
      "Average loss (full gen): 149.2431\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what   ?      <UNK> <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what were  <UNK> suspended <UNK> ?   houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 137.1219\tQuestion Loss (full generated): 137.8928\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.8995\tQuestion Loss Masked(full generated): 6.6235\n",
      "Average loss (full gen): 137.8928\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what <UNK> were the to the <UNK> of <UNK> football ?  houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: which rules were influential to the codes of association football ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what influenced ? the a the artillery <UNK> ? the kg , ?  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who decided to fire a heavy artillery <UNK> with 8 kg <UNK> ?       \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 47\tNet Loss: 5.2332 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.2332\n",
      "Batch: 1 \t Epoch : 47\tNet Loss: 5.1159 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1159\n",
      "Batch: 2 \t Epoch : 47\tNet Loss: 5.8117 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.8117\n",
      "Batch: 3 \t Epoch : 47\tNet Loss: 5.1449 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1449\n",
      "Batch: 4 \t Epoch : 47\tNet Loss: 5.5793 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5793\n",
      "Batch: 5 \t Epoch : 47\tNet Loss: 5.5310 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.5310\n",
      "Average Loss after Epoch 47 : 4.0520\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 148.2784\tQuestion Loss (full generated): 148.4939\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.5106\tQuestion Loss Masked(full generated): 7.3144\n",
      "Average loss (full gen): 148.4939\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  ? <UNK> the a ? ? <UNK> <UNK> `` of  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> by an a the the <UNK> <UNK> <UNK> were <UNK> <UNK> that  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 132.9139\tQuestion Loss (full generated): 139.6312\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.9638\tQuestion Loss Masked(full generated): 6.8811\n",
      "Average loss (full gen): 139.6312\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what widely fire ? the over <UNK> <UNK> years ?  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: have transfer <UNK> been increasing over the past years ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what the ? <UNK> , the <UNK> library found <UNK> a the the <UNK> 's board a the  <UNK> \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: in <UNK> the grade , the corporate library found that a third of comcast 's board was how old ?\n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 48\tNet Loss: 5.8228 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.8228\n",
      "Batch: 1 \t Epoch : 48\tNet Loss: 5.4182 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.4182\n",
      "Batch: 2 \t Epoch : 48\tNet Loss: 5.8187 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.8187\n",
      "Batch: 3 \t Epoch : 48\tNet Loss: 5.2576 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.2576\n",
      "Batch: 4 \t Epoch : 48\tNet Loss: 5.6574 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.6574\n",
      "Batch: 5 \t Epoch : 48\tNet Loss: 5.5293 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.5293\n",
      "Average Loss after Epoch 48 : 4.1880\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 145.8123\tQuestion Loss (full generated): 147.4858\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4958\tQuestion Loss Masked(full generated): 7.2838\n",
      "Average loss (full gen): 147.4858\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what the  ?  ? ? was <UNK> <UNK> ? a a  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what   khan ? the <UNK>  <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 128.2923\tQuestion Loss (full generated): 135.6543\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.6662\tQuestion Loss Masked(full generated): 6.6068\n",
      "Average loss (full gen): 135.6543\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  the <UNK> priest of the <UNK> ? <UNK> ? from  houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what attacked  defeat arsenal the <UNK> presidential election <UNK>  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who did eisenhower defeat in the 1956 presidential election ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 49\tNet Loss: 5.0549 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.0549\n",
      "Batch: 1 \t Epoch : 49\tNet Loss: 5.6347 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6347\n",
      "Batch: 2 \t Epoch : 49\tNet Loss: 5.3184 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3184\n",
      "Batch: 3 \t Epoch : 49\tNet Loss: 4.8068 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8068\n",
      "Batch: 4 \t Epoch : 49\tNet Loss: 4.9772 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9772\n",
      "Batch: 5 \t Epoch : 49\tNet Loss: 5.3587 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.3587\n",
      "Average Loss after Epoch 49 : 3.8939\n",
      "Epoch time: 1.41s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 144.1447\tQuestion Loss (full generated): 146.6791\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4813\tQuestion Loss Masked(full generated): 7.2574\n",
      "Average loss (full gen): 146.6791\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what were  <UNK> suspended the ?   houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what   ?      the <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 128.8096\tQuestion Loss (full generated): 134.4328\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.7934\tQuestion Loss Masked(full generated): 6.5103\n",
      "Average loss (full gen): 134.4328\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what attacked ? the a the artillery <UNK> ? the kg , ?  houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who decided to fire a heavy artillery <UNK> with 8 kg <UNK> ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what attacked the <UNK> the <UNK> <UNK> of <UNK> ?  houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: who was the leader of the revolt of <UNK> ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 50\tNet Loss: 5.4294 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.4294\n",
      "Batch: 1 \t Epoch : 50\tNet Loss: 5.6824 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 \t Epoch : 50\tNet Loss: 5.6085 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.6085\n",
      "Batch: 3 \t Epoch : 50\tNet Loss: 4.8454 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8454\n",
      "Batch: 4 \t Epoch : 50\tNet Loss: 5.6764 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.6764\n",
      "Batch: 5 \t Epoch : 50\tNet Loss: 5.2734 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.2734\n",
      "Average Loss after Epoch 50 : 4.0644\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 142.4304\tQuestion Loss (full generated): 145.7734\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4670\tQuestion Loss Masked(full generated): 7.2245\n",
      "Average loss (full gen): 145.7734\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what   ?      the <UNK> the the ?  houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> by an a the the the <UNK> <UNK> were <UNK> <UNK> that  houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 127.0388\tQuestion Loss (full generated): 132.4394\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.8340\tQuestion Loss Masked(full generated): 6.4157\n",
      "Average loss (full gen): 132.4394\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  ? are used the the ?  houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what <UNK> movements are used for <UNK> ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what    <UNK>  ? the  houses houses houses houses houses houses houses houses houses houses houses houses\n",
      "Full Generated: what  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?  houses of <UNK> ?\n",
      "Ground Truth..: what did these three categories <UNK> with ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 51\tNet Loss: 5.4961 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.4961\n",
      "Batch: 1 \t Epoch : 51\tNet Loss: 4.9900 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.9900\n",
      "Batch: 2 \t Epoch : 51\tNet Loss: 5.1794 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.1794\n",
      "Batch: 3 \t Epoch : 51\tNet Loss: 5.3848 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3848\n",
      "Batch: 4 \t Epoch : 51\tNet Loss: 5.1524 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.1524\n",
      "Batch: 5 \t Epoch : 51\tNet Loss: 5.4614 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.4614\n",
      "Average Loss after Epoch 51 : 3.9580\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 140.3896\tQuestion Loss (full generated): 147.2924\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4526\tQuestion Loss Masked(full generated): 7.6212\n",
      "Average loss (full gen): 147.2924\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what were the <UNK> by an a the the the <UNK> <UNK> were <UNK> <UNK> that  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what  ? <UNK> the ? ? ? <UNK> <UNK> `` of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 123.0860\tQuestion Loss (full generated): 134.9200\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.7249\tQuestion Loss Masked(full generated): 7.1344\n",
      "Average loss (full gen): 134.9200\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  festival ? the football the the the haven 's  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what musical festival was initiated in 1982 in new haven ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what   ? the claimed ? <UNK> ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what occurs when a wrestler <UNK> the ring ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 52\tNet Loss: 4.8471 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.8471\n",
      "Batch: 1 \t Epoch : 52\tNet Loss: 5.1610 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1610\n",
      "Batch: 2 \t Epoch : 52\tNet Loss: 5.2189 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.2189\n",
      "Batch: 3 \t Epoch : 52\tNet Loss: 5.4643 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.4643\n",
      "Batch: 4 \t Epoch : 52\tNet Loss: 5.5616 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5616\n",
      "Batch: 5 \t Epoch : 52\tNet Loss: 5.9442 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.9442\n",
      "Average Loss after Epoch 52 : 4.0247\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 138.5276\tQuestion Loss (full generated): 146.0793\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4385\tQuestion Loss Masked(full generated): 7.5882\n",
      "Average loss (full gen): 146.0793\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what   khan ? the <UNK>  <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what the ? the the  <UNK> the ? attacked the <UNK> ? <UNK>  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 121.0898\tQuestion Loss (full generated): 131.4117\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.7651\tQuestion Loss Masked(full generated): 6.9062\n",
      "Average loss (full gen): 131.4117\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what    <UNK>  ? the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did these three categories <UNK> with ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what were the <UNK> are ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where were the activists <UNK> ?              \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 53\tNet Loss: 5.1583 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.1583\n",
      "Batch: 1 \t Epoch : 53\tNet Loss: 5.1842 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1842\n",
      "Batch: 2 \t Epoch : 53\tNet Loss: 5.5398 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.5398\n",
      "Batch: 3 \t Epoch : 53\tNet Loss: 4.6297 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6297\n",
      "Batch: 4 \t Epoch : 53\tNet Loss: 5.6250 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.6250\n",
      "Batch: 5 \t Epoch : 53\tNet Loss: 5.0069 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0069\n",
      "Average Loss after Epoch 53 : 3.8930\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 136.9915\tQuestion Loss (full generated): 145.0904\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4247\tQuestion Loss Masked(full generated): 7.5621\n",
      "Average loss (full gen): 145.0904\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what were the <UNK> of an a the the the <UNK> <UNK> were <UNK> <UNK> that  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what were  <UNK> of the ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 115.2019\tQuestion Loss (full generated): 129.8680\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.4956\tQuestion Loss Masked(full generated): 6.7799\n",
      "Average loss (full gen): 129.8680\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what  the <UNK> rank called from the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the valencia cathedral called from <UNK> ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what were  rank among <UNK> 's board cities are  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where does valencia rank among spain 's largest cities ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 54\tNet Loss: 5.2561 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.2561\n",
      "Batch: 1 \t Epoch : 54\tNet Loss: 5.0038 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0038\n",
      "Batch: 2 \t Epoch : 54\tNet Loss: 5.1078 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.1078\n",
      "Batch: 3 \t Epoch : 54\tNet Loss: 5.2185 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.2185\n",
      "Batch: 4 \t Epoch : 54\tNet Loss: 4.8853 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8853\n",
      "Batch: 5 \t Epoch : 54\tNet Loss: 5.1654 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.1654\n",
      "Average Loss after Epoch 54 : 3.8296\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 136.0980\tQuestion Loss (full generated): 144.5842\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.4104\tQuestion Loss Masked(full generated): 7.5519\n",
      "Average loss (full gen): 144.5842\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what the  ?  ? ? was <UNK> <UNK> ? a ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  ? <UNK> the ? ? ? <UNK> <UNK> `` of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 117.3642\tQuestion Loss (full generated): 128.7556\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.7008\tQuestion Loss Masked(full generated): 6.7959\n",
      "Average loss (full gen): 128.7556\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what attacked believed the ? ? the the  <UNK> ? <UNK> of age ? the ?  <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> of <UNK> <UNK> <UNK> attacked guam ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the name of the general who claimed guam ?         \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 55\tNet Loss: 4.9294 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9294\n",
      "Batch: 1 \t Epoch : 55\tNet Loss: 5.1805 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1805\n",
      "Batch: 2 \t Epoch : 55\tNet Loss: 5.3954 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3954\n",
      "Batch: 3 \t Epoch : 55\tNet Loss: 4.6620 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6620\n",
      "Batch: 4 \t Epoch : 55\tNet Loss: 5.0281 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.0281\n",
      "Batch: 5 \t Epoch : 55\tNet Loss: 4.9150 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.9150\n",
      "Average Loss after Epoch 55 : 3.7638\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 135.9824\tQuestion Loss (full generated): 144.7046\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3961\tQuestion Loss Masked(full generated): 7.5654\n",
      "Average loss (full gen): 144.7046\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what were the <UNK> of an a the the <UNK> <UNK> <UNK> were <UNK> <UNK> that  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what   ? ?     the <UNK> the the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 113.5352\tQuestion Loss (full generated): 129.2401\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.4258\tQuestion Loss Masked(full generated): 6.7805\n",
      "Average loss (full gen): 129.2401\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what were  rank among <UNK> 's ? cities are  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where does valencia rank among spain 's largest cities ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> rank called from <UNK> ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the valencia cathedral called from <UNK> ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 56\tNet Loss: 4.9066 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9066\n",
      "Batch: 1 \t Epoch : 56\tNet Loss: 5.0477 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0477\n",
      "Batch: 2 \t Epoch : 56\tNet Loss: 5.1052 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.1052\n",
      "Batch: 3 \t Epoch : 56\tNet Loss: 5.0461 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0461\n",
      "Batch: 4 \t Epoch : 56\tNet Loss: 5.2597 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.2597\n",
      "Batch: 5 \t Epoch : 56\tNet Loss: 4.7846 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.7846\n",
      "Average Loss after Epoch 56 : 3.7687\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 136.1215\tQuestion Loss (full generated): 145.0469\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3824\tQuestion Loss Masked(full generated): 7.5877\n",
      "Average loss (full gen): 145.0469\n",
      "Eval time: 0.43s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> `` of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what attacked ? <UNK> ? <UNK> the the a <UNK> of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 113.5868\tQuestion Loss (full generated): 129.5453\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.3893\tQuestion Loss Masked(full generated): 6.8030\n",
      "Average loss (full gen): 129.5453\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  the <UNK> priest of the <UNK> ? <UNK> ? from  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what were  rank among <UNK> 's ? cities are  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where does valencia rank among spain 's largest cities ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 57\tNet Loss: 5.7563 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.7563\n",
      "Batch: 1 \t Epoch : 57\tNet Loss: 4.8228 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8228\n",
      "Batch: 2 \t Epoch : 57\tNet Loss: 5.0919 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.0919\n",
      "Batch: 3 \t Epoch : 57\tNet Loss: 4.5746 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.5746\n",
      "Batch: 4 \t Epoch : 57\tNet Loss: 5.2264 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.2264\n",
      "Batch: 5 \t Epoch : 57\tNet Loss: 5.0991 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0991\n",
      "Average Loss after Epoch 57 : 3.8214\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 135.5426\tQuestion Loss (full generated): 144.8182\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3697\tQuestion Loss Masked(full generated): 7.5884\n",
      "Average loss (full gen): 144.8182\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what ?  <UNK> of <UNK> ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the <UNK> <UNK> <UNK> were <UNK> <UNK> that  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 114.7212\tQuestion Loss (full generated): 132.6707\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.6401\tQuestion Loss Masked(full generated): 7.0372\n",
      "Average loss (full gen): 132.6707\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what  the <UNK> ? referendum <UNK> ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the <UNK> statute referendum <UNK> <UNK> ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what  the  ? ? to the migrants ? the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what terms did <UNK> <UNK> use to describe migrants to britain ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 58\tNet Loss: 5.3647 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.3647\n",
      "Batch: 1 \t Epoch : 58\tNet Loss: 4.6787 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6787\n",
      "Batch: 2 \t Epoch : 58\tNet Loss: 5.4130 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.4130\n",
      "Batch: 3 \t Epoch : 58\tNet Loss: 5.0779 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0779\n",
      "Batch: 4 \t Epoch : 58\tNet Loss: 5.0425 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.0425\n",
      "Batch: 5 \t Epoch : 58\tNet Loss: 5.1108 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.1108\n",
      "Average Loss after Epoch 58 : 3.8360\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 134.4937\tQuestion Loss (full generated): 144.1899\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3577\tQuestion Loss Masked(full generated): 7.5728\n",
      "Average loss (full gen): 144.1899\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> `` of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ?  <UNK> of <UNK> ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 113.1962\tQuestion Loss (full generated): 131.7917\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.6107\tQuestion Loss Masked(full generated): 7.0105\n",
      "Average loss (full gen): 131.7917\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what attacked the <UNK> an award 1997 the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who gave the bronx an award in 1997 ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what  <UNK> worldwide has ? ? ? album sold ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how many copies worldwide has queen 's 1995 album sold ?         \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 59\tNet Loss: 4.9629 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9629\n",
      "Batch: 1 \t Epoch : 59\tNet Loss: 4.6991 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6991\n",
      "Batch: 2 \t Epoch : 59\tNet Loss: 5.3441 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3441\n",
      "Batch: 3 \t Epoch : 59\tNet Loss: 4.4927 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4927\n",
      "Batch: 4 \t Epoch : 59\tNet Loss: 5.5324 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5324\n",
      "Batch: 5 \t Epoch : 59\tNet Loss: 5.2336 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.2336\n",
      "Average Loss after Epoch 59 : 3.7831\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 133.0912\tQuestion Loss (full generated): 143.3189\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3470\tQuestion Loss Masked(full generated): 7.5494\n",
      "Average loss (full gen): 143.3189\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what   ? ?     <UNK> <UNK> the the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ?  <UNK> of <UNK> ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 108.3685\tQuestion Loss (full generated): 123.2570\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.1581\tQuestion Loss Masked(full generated): 6.4357\n",
      "Average loss (full gen): 123.2570\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what  team defeated arsenal for the <UNK> cup in the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what famous team defeated arsenal for the league cup in 2007 ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? westminster hall ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who built westminster hall ?               \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 60\tNet Loss: 4.9344 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9344\n",
      "Batch: 1 \t Epoch : 60\tNet Loss: 4.6575 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6575\n",
      "Batch: 2 \t Epoch : 60\tNet Loss: 5.0884 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.0884\n",
      "Batch: 3 \t Epoch : 60\tNet Loss: 4.8747 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8747\n",
      "Batch: 4 \t Epoch : 60\tNet Loss: 4.8923 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8923\n",
      "Batch: 5 \t Epoch : 60\tNet Loss: 5.7513 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.7513\n",
      "Average Loss after Epoch 60 : 3.7748\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 131.8469\tQuestion Loss (full generated): 142.6398\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3369\tQuestion Loss Masked(full generated): 7.5349\n",
      "Average loss (full gen): 142.6398\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? <UNK>  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the the a <UNK> of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 109.4234\tQuestion Loss (full generated): 129.7129\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.5573\tQuestion Loss Masked(full generated): 6.9529\n",
      "Average loss (full gen): 129.7129\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what  the <UNK> ? referendum <UNK> ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the <UNK> statute referendum <UNK> <UNK> ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what  the  ? ? to the migrants ? the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what terms did <UNK> <UNK> use to describe migrants to britain ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 61\tNet Loss: 4.6927 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.6927\n",
      "Batch: 1 \t Epoch : 61\tNet Loss: 5.1747 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1747\n",
      "Batch: 2 \t Epoch : 61\tNet Loss: 5.9744 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.9744\n",
      "Batch: 3 \t Epoch : 61\tNet Loss: 5.1125 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1125\n",
      "Batch: 4 \t Epoch : 61\tNet Loss: 5.2308 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.2308\n",
      "Batch: 5 \t Epoch : 61\tNet Loss: 4.9714 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.9714\n",
      "Average Loss after Epoch 61 : 3.8946\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 129.9962\tQuestion Loss (full generated): 141.4537\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3287\tQuestion Loss Masked(full generated): 7.5009\n",
      "Average loss (full gen): 141.4537\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what ? the <UNK> of <UNK> ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ? the ? ? ? <UNK> <UNK> ? a ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.7986\tQuestion Loss (full generated): 126.1948\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.4121\tQuestion Loss Masked(full generated): 6.9022\n",
      "Average loss (full gen): 126.1948\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what <UNK> were the to the <UNK> of <UNK> football ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: which rules were influential to the codes of association football ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what  ago ? the <UNK> ? begin   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how long ago did the <UNK> period begin ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 62\tNet Loss: 5.2239 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.2239\n",
      "Batch: 1 \t Epoch : 62\tNet Loss: 5.3734 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3734\n",
      "Batch: 2 \t Epoch : 62\tNet Loss: 4.9596 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.9596\n",
      "Batch: 3 \t Epoch : 62\tNet Loss: 5.6185 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6185\n",
      "Batch: 4 \t Epoch : 62\tNet Loss: 4.9135 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9135\n",
      "Batch: 5 \t Epoch : 62\tNet Loss: 5.0747 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0747\n",
      "Average Loss after Epoch 62 : 3.8955\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 128.5115\tQuestion Loss (full generated): 140.5620\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3213\tQuestion Loss Masked(full generated): 7.4776\n",
      "Average loss (full gen): 140.5620\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what ? the <UNK> of <UNK> ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what the  ? the ? ? ? <UNK> <UNK> ? by ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 104.7454\tQuestion Loss (full generated): 126.7831\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.5128\tQuestion Loss Masked(full generated): 6.8537\n",
      "Average loss (full gen): 126.7831\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  <UNK> worldwide has ? ? ? album sold ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how many copies worldwide has queen 's 1995 album sold ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what  the the ? ? to the migrants ? the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what terms did <UNK> <UNK> use to describe migrants to britain ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 63\tNet Loss: 5.1919 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.1919\n",
      "Batch: 1 \t Epoch : 63\tNet Loss: 4.7383 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7383\n",
      "Batch: 2 \t Epoch : 63\tNet Loss: 5.2060 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.2060\n",
      "Batch: 3 \t Epoch : 63\tNet Loss: 5.6464 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6464\n",
      "Batch: 4 \t Epoch : 63\tNet Loss: 5.1646 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.1646\n",
      "Batch: 5 \t Epoch : 63\tNet Loss: 5.3318 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.3318\n",
      "Average Loss after Epoch 63 : 3.9099\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 127.0481\tQuestion Loss (full generated): 139.8167\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3148\tQuestion Loss Masked(full generated): 7.4635\n",
      "Average loss (full gen): 139.8167\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  the ? ?     <UNK> the the the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the the a ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 99.5442\tQuestion Loss (full generated): 122.3510\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.1743\tQuestion Loss Masked(full generated): 6.6313\n",
      "Average loss (full gen): 122.3510\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what to <UNK> ? exchange suspended the  <UNK> suspended trading of <UNK> ? stock exchange  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: besides the <UNK> stock exchange , what other exchange suspended trading of <UNK> china stock ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> empire ? the dominated by the the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: the seleucid empire was mostly dominated by whom ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 64\tNet Loss: 4.9603 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9603\n",
      "Batch: 1 \t Epoch : 64\tNet Loss: 4.7724 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7724\n",
      "Batch: 2 \t Epoch : 64\tNet Loss: 5.3210 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3210\n",
      "Batch: 3 \t Epoch : 64\tNet Loss: 4.8598 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8598\n",
      "Batch: 4 \t Epoch : 64\tNet Loss: 5.1579 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.1579\n",
      "Batch: 5 \t Epoch : 64\tNet Loss: 4.8476 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.8476\n",
      "Average Loss after Epoch 64 : 3.7399\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 125.8527\tQuestion Loss (full generated): 139.3077\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3084\tQuestion Loss Masked(full generated): 7.4587\n",
      "Average loss (full gen): 139.3077\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  the khan ? the <UNK> ? <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? <UNK>  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 97.7034\tQuestion Loss (full generated): 121.6084\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.1511\tQuestion Loss Masked(full generated): 6.6247\n",
      "Average loss (full gen): 121.6084\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  the <UNK> cost  <UNK> the gold membership of the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what is the annual cost of a live gold membership in <UNK> ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what ?  rank among <UNK> 's ? cities ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where does valencia rank among spain 's largest cities ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 65\tNet Loss: 4.8777 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.8777\n",
      "Batch: 1 \t Epoch : 65\tNet Loss: 5.1141 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1141\n",
      "Batch: 2 \t Epoch : 65\tNet Loss: 4.7140 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7140\n",
      "Batch: 3 \t Epoch : 65\tNet Loss: 4.7446 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7446\n",
      "Batch: 4 \t Epoch : 65\tNet Loss: 5.2343 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.2343\n",
      "Batch: 5 \t Epoch : 65\tNet Loss: 4.5707 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.5707\n",
      "Average Loss after Epoch 65 : 3.6569\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 125.3360\tQuestion Loss (full generated): 139.1008\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.3006\tQuestion Loss Masked(full generated): 7.4581\n",
      "Average loss (full gen): 139.1008\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the ? <UNK> <UNK> were <UNK> <UNK> that  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "T.F. Generated: what  the ? ?     the the the the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.1614\tQuestion Loss (full generated): 124.5569\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.4507\tQuestion Loss Masked(full generated): 6.8039\n",
      "Average loss (full gen): 124.5569\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  the <UNK> ? referendum <UNK> ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the <UNK> statute referendum <UNK> <UNK> ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> ? to the <UNK> ? <UNK>  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: when did the <UNK> surrender to the <UNK> army ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 66\tNet Loss: 4.6236 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.6236\n",
      "Batch: 1 \t Epoch : 66\tNet Loss: 4.8547 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8547\n",
      "Batch: 2 \t Epoch : 66\tNet Loss: 4.9435 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.9435\n",
      "Batch: 3 \t Epoch : 66\tNet Loss: 4.8826 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8826\n",
      "Batch: 4 \t Epoch : 66\tNet Loss: 4.9426 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9426\n",
      "Batch: 5 \t Epoch : 66\tNet Loss: 5.3687 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.3687\n",
      "Average Loss after Epoch 66 : 3.7020\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 125.1131\tQuestion Loss (full generated): 139.1102\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2937\tQuestion Loss Masked(full generated): 7.4652\n",
      "Average loss (full gen): 139.1102\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the the a ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> `` ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 99.0961\tQuestion Loss (full generated): 123.0809\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.3046\tQuestion Loss Masked(full generated): 6.8660\n",
      "Average loss (full gen): 123.0809\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  <UNK>  ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what are `` <UNK> '' ?              \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of <UNK> <UNK> of <UNK> ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who was the leader of the revolt of <UNK> ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 67\tNet Loss: 4.8381 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.8381\n",
      "Batch: 1 \t Epoch : 67\tNet Loss: 4.8433 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8433\n",
      "Batch: 2 \t Epoch : 67\tNet Loss: 5.1690 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.1690\n",
      "Batch: 3 \t Epoch : 67\tNet Loss: 4.8623 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8623\n",
      "Batch: 4 \t Epoch : 67\tNet Loss: 4.8036 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8036\n",
      "Batch: 5 \t Epoch : 67\tNet Loss: 4.6070 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.6070\n",
      "Average Loss after Epoch 67 : 3.6404\n",
      "Epoch time: 1.41s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 124.4691\tQuestion Loss (full generated): 138.7659\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2871\tQuestion Loss Masked(full generated): 7.4577\n",
      "Average loss (full gen): 138.7659\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what the  ? the ? ? ? <UNK> <UNK> ? by ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  the khan ? the <UNK> ? <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 94.6292\tQuestion Loss (full generated): 116.2343\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.9706\tQuestion Loss Masked(full generated): 6.2600\n",
      "Average loss (full gen): 116.2343\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what <UNK> ? ? khan had the the the the  the the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: the <UNK> <UNK> ali khan had a title , what was it ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  <UNK> ? the ? ? accepted madaris records  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how many regions in singapore have widely accepted madaris ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 68\tNet Loss: 4.5478 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.5478\n",
      "Batch: 1 \t Epoch : 68\tNet Loss: 5.1339 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1339\n",
      "Batch: 2 \t Epoch : 68\tNet Loss: 4.4325 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.4325\n",
      "Batch: 3 \t Epoch : 68\tNet Loss: 5.0639 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0639\n",
      "Batch: 4 \t Epoch : 68\tNet Loss: 4.5313 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5313\n",
      "Batch: 5 \t Epoch : 68\tNet Loss: 5.1028 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.1028\n",
      "Average Loss after Epoch 68 : 3.6015\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 123.9998\tQuestion Loss (full generated): 138.5452\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2803\tQuestion Loss Masked(full generated): 7.4545\n",
      "Average loss (full gen): 138.5452\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the ? ?     <UNK> the the ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 94.4793\tQuestion Loss (full generated): 120.3542\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0519\tQuestion Loss Masked(full generated): 6.6074\n",
      "Average loss (full gen): 120.3542\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what <UNK> empire ? the dominated by the the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: the seleucid empire was mostly dominated by whom ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> priest of the <UNK> ? <UNK> ? from  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 69\tNet Loss: 4.5501 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.5501\n",
      "Batch: 1 \t Epoch : 69\tNet Loss: 4.9668 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.9668\n",
      "Batch: 2 \t Epoch : 69\tNet Loss: 4.9554 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.9554\n",
      "Batch: 3 \t Epoch : 69\tNet Loss: 5.1152 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1152\n",
      "Batch: 4 \t Epoch : 69\tNet Loss: 5.4569 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.4569\n",
      "Batch: 5 \t Epoch : 69\tNet Loss: 4.7454 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.7454\n",
      "Average Loss after Epoch 69 : 3.7237\n",
      "Epoch time: 1.34s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 123.4053\tQuestion Loss (full generated): 138.2119\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2739\tQuestion Loss Masked(full generated): 7.4465\n",
      "Average loss (full gen): 138.2119\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what ? the <UNK> ? ? ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? <UNK>  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 96.6056\tQuestion Loss (full generated): 114.9394\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.2627\tQuestion Loss Masked(full generated): 6.1991\n",
      "Average loss (full gen): 114.9394\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  originally the the <UNK> cities to the haven 's ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what sector originally provided the largest contribution to new haven 's economy ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what ? beyonce worldwide  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 70\tNet Loss: 5.3336 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.3336\n",
      "Batch: 1 \t Epoch : 70\tNet Loss: 5.2892 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.2892\n",
      "Batch: 2 \t Epoch : 70\tNet Loss: 5.0738 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.0738\n",
      "Batch: 3 \t Epoch : 70\tNet Loss: 5.6526 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.6526\n",
      "Batch: 4 \t Epoch : 70\tNet Loss: 5.4032 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.4032\n",
      "Batch: 5 \t Epoch : 70\tNet Loss: 5.3761 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.3761\n",
      "Average Loss after Epoch 70 : 4.0161\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 121.9784\tQuestion Loss (full generated): 137.3980\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2693\tQuestion Loss Masked(full generated): 7.4263\n",
      "Average loss (full gen): 137.3980\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what the  ? the ? ? ? <UNK> <UNK> ? by ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> ? ? ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 90.5459\tQuestion Loss (full generated): 113.6338\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.9054\tQuestion Loss Masked(full generated): 6.1560\n",
      "Average loss (full gen): 113.6338\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what <UNK> ? ? khan had the the the the  the the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: the <UNK> <UNK> ali khan had a title , what was it ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  <UNK> singapore the ? ? accepted madaris records  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how many regions in singapore have widely accepted madaris ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 71\tNet Loss: 4.9753 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9753\n",
      "Batch: 1 \t Epoch : 71\tNet Loss: 4.6980 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6980\n",
      "Batch: 2 \t Epoch : 71\tNet Loss: 5.3194 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.3194\n",
      "Batch: 3 \t Epoch : 71\tNet Loss: 4.5639 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.5639\n",
      "Batch: 4 \t Epoch : 71\tNet Loss: 5.1804 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.1804\n",
      "Batch: 5 \t Epoch : 71\tNet Loss: 5.0461 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0461\n",
      "Average Loss after Epoch 71 : 3.7229\n",
      "Epoch time: 1.41s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 120.2566\tQuestion Loss (full generated): 136.3516\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2640\tQuestion Loss Masked(full generated): 7.3968\n",
      "Average loss (full gen): 136.3516\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> festival ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> ? ? ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 92.2720\tQuestion Loss (full generated): 116.1938\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.2808\tQuestion Loss Masked(full generated): 6.4480\n",
      "Average loss (full gen): 116.1938\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what ? the <UNK> ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where were the activists <UNK> ?              \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the the   <UNK> ? community ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where is there nothing like this <UNK> speech community ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 72\tNet Loss: 4.7954 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.7954\n",
      "Batch: 1 \t Epoch : 72\tNet Loss: 4.3370 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3370\n",
      "Batch: 2 \t Epoch : 72\tNet Loss: 5.2182 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.2182\n",
      "Batch: 3 \t Epoch : 72\tNet Loss: 4.4955 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4955\n",
      "Batch: 4 \t Epoch : 72\tNet Loss: 4.8205 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8205\n",
      "Batch: 5 \t Epoch : 72\tNet Loss: 4.9262 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.9262\n",
      "Average Loss after Epoch 72 : 3.5741\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 117.9081\tQuestion Loss (full generated): 135.0435\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2580\tQuestion Loss Masked(full generated): 7.3645\n",
      "Average loss (full gen): 135.0435\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> festival ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the ? ?   the the <UNK> the ? ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 88.0588\tQuestion Loss (full generated): 110.7095\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.2052\tQuestion Loss Masked(full generated): 6.1004\n",
      "Average loss (full gen): 110.7095\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what  the the ? ? the <UNK> ?   to the and the  <UNK> ?  <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what is a <UNK> <UNK> way that <UNK> can get access to marketing and word of mouth ?  \n",
      "\n",
      "\n",
      "T.F. Generated: what ? rule was the <UNK> to <UNK> <UNK> ? the of ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: during whose rule was the use of old <UNK> at its peak ?       \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 73\tNet Loss: 5.3092 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.3092\n",
      "Batch: 1 \t Epoch : 73\tNet Loss: 5.3661 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3661\n",
      "Batch: 2 \t Epoch : 73\tNet Loss: 5.4879 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.4879\n",
      "Batch: 3 \t Epoch : 73\tNet Loss: 4.6010 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6010\n",
      "Batch: 4 \t Epoch : 73\tNet Loss: 5.1803 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.1803\n",
      "Batch: 5 \t Epoch : 73\tNet Loss: 4.8286 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.8286\n",
      "Average Loss after Epoch 73 : 3.8466\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 115.8769\tQuestion Loss (full generated): 133.9075\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2538\tQuestion Loss Masked(full generated): 7.3364\n",
      "Average loss (full gen): 133.9075\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what  the ? ?   the the the the ? ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 84.8927\tQuestion Loss (full generated): 109.3192\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.1865\tQuestion Loss Masked(full generated): 6.0790\n",
      "Average loss (full gen): 109.3192\n",
      "Eval time: 0.43s\n",
      "T.F. Generated: what  the <UNK> win taiwan stock  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did the japanese win taiwan ?             \n",
      "\n",
      "\n",
      "T.F. Generated: what ? beyonce worldwide  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 74\tNet Loss: 4.7582 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.7582\n",
      "Batch: 1 \t Epoch : 74\tNet Loss: 4.9141 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.9141\n",
      "Batch: 2 \t Epoch : 74\tNet Loss: 4.9517 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.9517\n",
      "Batch: 3 \t Epoch : 74\tNet Loss: 5.3874 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.3874\n",
      "Batch: 4 \t Epoch : 74\tNet Loss: 4.9477 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9477\n",
      "Batch: 5 \t Epoch : 74\tNet Loss: 4.9628 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.9628\n",
      "Average Loss after Epoch 74 : 3.7402\n",
      "Epoch time: 1.50s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 113.7366\tQuestion Loss (full generated): 132.8791\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2503\tQuestion Loss Masked(full generated): 7.3183\n",
      "Average loss (full gen): 132.8791\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> festival ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 81.5680\tQuestion Loss (full generated): 107.9973\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.1695\tQuestion Loss Masked(full generated): 6.0690\n",
      "Average loss (full gen): 107.9973\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what many the ? the speak ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how much english did tito speak ?             \n",
      "\n",
      "\n",
      "T.F. Generated: what  the the ? ? the <UNK> ?   to the and the  <UNK> ?  <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what is a <UNK> <UNK> way that <UNK> can get access to marketing and word of mouth ?  \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 75\tNet Loss: 4.6737 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.6737\n",
      "Batch: 1 \t Epoch : 75\tNet Loss: 4.4551 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4551\n",
      "Batch: 2 \t Epoch : 75\tNet Loss: 4.9194 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.9194\n",
      "Batch: 3 \t Epoch : 75\tNet Loss: 4.4086 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4086\n",
      "Batch: 4 \t Epoch : 75\tNet Loss: 4.7338 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.7338\n",
      "Batch: 5 \t Epoch : 75\tNet Loss: 4.8522 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.8522\n",
      "Average Loss after Epoch 75 : 3.5053\n",
      "Epoch time: 1.46s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 112.0568\tQuestion Loss (full generated): 132.1221\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2458\tQuestion Loss Masked(full generated): 7.3078\n",
      "Average loss (full gen): 132.1221\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? <UNK> <UNK> festival ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 76.5508\tQuestion Loss (full generated): 112.2218\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.9119\tQuestion Loss Masked(full generated): 6.4720\n",
      "Average loss (full gen): 112.2218\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  the <UNK> priest of the <UNK> ? <UNK> ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the defeat arsenal the <UNK> presidential election ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who did eisenhower defeat in the 1956 presidential election ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 76\tNet Loss: 4.5436 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.5436\n",
      "Batch: 1 \t Epoch : 76\tNet Loss: 4.8315 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8315\n",
      "Batch: 2 \t Epoch : 76\tNet Loss: 5.0152 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.0152\n",
      "Batch: 3 \t Epoch : 76\tNet Loss: 5.0552 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0552\n",
      "Batch: 4 \t Epoch : 76\tNet Loss: 4.9577 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9577\n",
      "Batch: 5 \t Epoch : 76\tNet Loss: 5.4131 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.4131\n",
      "Average Loss after Epoch 76 : 3.7270\n",
      "Epoch time: 1.46s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 110.6407\tQuestion Loss (full generated): 131.5491\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2412\tQuestion Loss Masked(full generated): 7.3035\n",
      "Average loss (full gen): 131.5491\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> ? ? ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 74.3960\tQuestion Loss (full generated): 111.5853\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8919\tQuestion Loss Masked(full generated): 6.4839\n",
      "Average loss (full gen): 111.5853\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what  the <UNK> ? sign the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what is the <UNK> equal sign ?             \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> cost the <UNK> the gold membership of the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what is the annual cost of a live gold membership in <UNK> ?       \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 77\tNet Loss: 4.2350 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.2350\n",
      "Batch: 1 \t Epoch : 77\tNet Loss: 4.2957 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2957\n",
      "Batch: 2 \t Epoch : 77\tNet Loss: 5.1839 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.1839\n",
      "Batch: 3 \t Epoch : 77\tNet Loss: 4.5573 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.5573\n",
      "Batch: 4 \t Epoch : 77\tNet Loss: 5.0397 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.0397\n",
      "Batch: 5 \t Epoch : 77\tNet Loss: 4.6360 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.6360\n",
      "Average Loss after Epoch 77 : 3.4934\n",
      "Epoch time: 1.47s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 109.9138\tQuestion Loss (full generated): 131.2036\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2368\tQuestion Loss Masked(full generated): 7.2979\n",
      "Average loss (full gen): 131.2036\n",
      "Eval time: 0.43s\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the the <UNK> ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the ? <UNK> <UNK> were <UNK> <UNK> that  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 75.5117\tQuestion Loss (full generated): 105.9410\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.1148\tQuestion Loss Masked(full generated): 6.0792\n",
      "Average loss (full gen): 105.9410\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what  originally the the <UNK> cities to the haven 's ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what sector originally provided the largest contribution to new haven 's economy ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what ? beyonce worldwide  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 78\tNet Loss: 4.7820 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.7820\n",
      "Batch: 1 \t Epoch : 78\tNet Loss: 4.2942 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2942\n",
      "Batch: 2 \t Epoch : 78\tNet Loss: 5.6289 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.6289\n",
      "Batch: 3 \t Epoch : 78\tNet Loss: 4.2510 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2510\n",
      "Batch: 4 \t Epoch : 78\tNet Loss: 4.9283 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9283\n",
      "Batch: 5 \t Epoch : 78\tNet Loss: 5.2232 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.2232\n",
      "Average Loss after Epoch 78 : 3.6384\n",
      "Epoch time: 1.34s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 109.5246\tQuestion Loss (full generated): 130.8692\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2325\tQuestion Loss Masked(full generated): 7.2846\n",
      "Average loss (full gen): 130.8692\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what ? the <UNK> ? ? ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 74.8293\tQuestion Loss (full generated): 105.7391\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0976\tQuestion Loss Masked(full generated): 6.0808\n",
      "Average loss (full gen): 105.7391\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what <UNK> practices of <UNK> of <UNK> ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: which branch practices the teachings of <UNK> ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what ? beyonce worldwide  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 79\tNet Loss: 4.9590 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9590\n",
      "Batch: 1 \t Epoch : 79\tNet Loss: 4.9679 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.9679\n",
      "Batch: 2 \t Epoch : 79\tNet Loss: 4.8871 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.8871\n",
      "Batch: 3 \t Epoch : 79\tNet Loss: 4.5776 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.5776\n",
      "Batch: 4 \t Epoch : 79\tNet Loss: 4.3289 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.3289\n",
      "Batch: 5 \t Epoch : 79\tNet Loss: 4.7686 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.7686\n",
      "Average Loss after Epoch 79 : 3.5611\n",
      "Epoch time: 1.33s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 109.2696\tQuestion Loss (full generated): 130.6374\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2290\tQuestion Loss Masked(full generated): 7.2748\n",
      "Average loss (full gen): 130.6374\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  the ? ?   the the the the ? ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the the <UNK> ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 70.3397\tQuestion Loss (full generated): 104.5263\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.7259\tQuestion Loss Masked(full generated): 5.9855\n",
      "Average loss (full gen): 104.5263\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what many <UNK> singapore the ? ? accepted madaris records  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how many regions in singapore have widely accepted madaris ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> of <UNK> until ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what was the currency of greece until 2002 ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 80\tNet Loss: 4.2507 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.2507\n",
      "Batch: 1 \t Epoch : 80\tNet Loss: 4.7610 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7610\n",
      "Batch: 2 \t Epoch : 80\tNet Loss: 4.5484 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.5484\n",
      "Batch: 3 \t Epoch : 80\tNet Loss: 4.3661 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3661\n",
      "Batch: 4 \t Epoch : 80\tNet Loss: 5.5937 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.5937\n",
      "Batch: 5 \t Epoch : 80\tNet Loss: 4.9499 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.9499\n",
      "Average Loss after Epoch 80 : 3.5587\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 108.7032\tQuestion Loss (full generated): 130.2386\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2250\tQuestion Loss Masked(full generated): 7.2613\n",
      "Average loss (full gen): 130.2386\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the the <UNK> ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 70.9698\tQuestion Loss (full generated): 110.4247\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8065\tQuestion Loss Masked(full generated): 6.4750\n",
      "Average loss (full gen): 110.4247\n",
      "Eval time: 0.43s\n",
      "T.F. Generated: what ? the defeat arsenal the <UNK> presidential election ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who did eisenhower defeat in the 1956 presidential election ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> cost the <UNK> the gold membership of the ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what is the annual cost of a live gold membership in <UNK> ?       \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 81\tNet Loss: 6.6134 \tAnswer Loss: 0.6534 \tQuestion Loss: 6.6134\n",
      "Batch: 1 \t Epoch : 81\tNet Loss: 4.1315 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.1315\n",
      "Batch: 2 \t Epoch : 81\tNet Loss: 4.8979 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.8979\n",
      "Batch: 3 \t Epoch : 81\tNet Loss: 5.1922 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1922\n",
      "Batch: 4 \t Epoch : 81\tNet Loss: 5.0588 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.0588\n",
      "Batch: 5 \t Epoch : 81\tNet Loss: 5.0838 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0838\n",
      "Average Loss after Epoch 81 : 3.8722\n",
      "Epoch time: 1.45s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 107.8062\tQuestion Loss (full generated): 129.6437\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2229\tQuestion Loss Masked(full generated): 7.2416\n",
      "Average loss (full gen): 129.6437\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> ? ? ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 71.8730\tQuestion Loss (full generated): 104.3375\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0456\tQuestion Loss Masked(full generated): 6.0406\n",
      "Average loss (full gen): 104.3375\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? beyonce worldwide  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "T.F. Generated: what  the ancestry to ?  the as ?  ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: americans with african ancestry have always been classified as what race ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 82\tNet Loss: 4.5916 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.5916\n",
      "Batch: 1 \t Epoch : 82\tNet Loss: 4.4337 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4337\n",
      "Batch: 2 \t Epoch : 82\tNet Loss: 4.7829 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7829\n",
      "Batch: 3 \t Epoch : 82\tNet Loss: 4.2766 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2766\n",
      "Batch: 4 \t Epoch : 82\tNet Loss: 4.8016 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8016\n",
      "Batch: 5 \t Epoch : 82\tNet Loss: 4.6530 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.6530\n",
      "Average Loss after Epoch 82 : 3.4424\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.7296\tQuestion Loss (full generated): 129.1586\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2207\tQuestion Loss Masked(full generated): 7.2347\n",
      "Average loss (full gen): 129.1586\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? ? <UNK> festival ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 70.0306\tQuestion Loss (full generated): 110.4629\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0047\tQuestion Loss Masked(full generated): 6.6136\n",
      "Average loss (full gen): 110.4629\n",
      "Eval time: 0.42s\n",
      "T.F. Generated: what  do ? the arsenal ? <UNK> ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what occurs when a wrestler <UNK> the ring ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what  <UNK>  ?   <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what are `` <UNK> '' ?              \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 83\tNet Loss: 5.0823 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.0823\n",
      "Batch: 1 \t Epoch : 83\tNet Loss: 4.4501 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4501\n",
      "Batch: 2 \t Epoch : 83\tNet Loss: 4.7834 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7834\n",
      "Batch: 3 \t Epoch : 83\tNet Loss: 4.2538 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2538\n",
      "Batch: 4 \t Epoch : 83\tNet Loss: 4.7220 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.7220\n",
      "Batch: 5 \t Epoch : 83\tNet Loss: 4.5834 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.5834\n",
      "Average Loss after Epoch 83 : 3.4844\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 105.9048\tQuestion Loss (full generated): 128.8406\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2175\tQuestion Loss Masked(full generated): 7.2334\n",
      "Average loss (full gen): 128.8406\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what the ? the the  ? <UNK> ? ? the <UNK> ? of  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ?  ? the ? ? ? <UNK> <UNK> ? by ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 69.8851\tQuestion Loss (full generated): 106.2900\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0933\tQuestion Loss Masked(full generated): 6.2209\n",
      "Average loss (full gen): 106.2900\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  the <UNK> <UNK> spain ? the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did these three categories <UNK> with ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what ? believed the ? ? the the  <UNK> ? <UNK> of age ? the ?  <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 84\tNet Loss: 4.2773 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.2773\n",
      "Batch: 1 \t Epoch : 84\tNet Loss: 4.2420 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2420\n",
      "Batch: 2 \t Epoch : 84\tNet Loss: 4.6887 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.6887\n",
      "Batch: 3 \t Epoch : 84\tNet Loss: 4.2568 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2568\n",
      "Batch: 4 \t Epoch : 84\tNet Loss: 4.8613 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8613\n",
      "Batch: 5 \t Epoch : 84\tNet Loss: 4.8966 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.8966\n",
      "Average Loss after Epoch 84 : 3.4029\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 105.6069\tQuestion Loss (full generated): 117.6403\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2137\tQuestion Loss Masked(full generated): 7.0436\n",
      "Average loss (full gen): 117.6403\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what  the ? ?   the the the the ? ? ?       \n",
      "Full Generated: what                    \n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of ? ? britain             \n",
      "Full Generated: what                    \n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 65.8812\tQuestion Loss (full generated): 93.0241\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.7286\tQuestion Loss Masked(full generated): 6.0387\n",
      "Average loss (full gen): 93.0241\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what  the <UNK> cost the <UNK> the gold membership of ? ?        \n",
      "Full Generated: what                    \n",
      "Ground Truth..: what is the annual cost of a live gold membership in <UNK> ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> ? sign the              \n",
      "Full Generated: what                    \n",
      "Ground Truth..: what is the <UNK> equal sign ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 85\tNet Loss: 5.6285 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.6285\n",
      "Batch: 1 \t Epoch : 85\tNet Loss: 4.5146 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.5146\n",
      "Batch: 2 \t Epoch : 85\tNet Loss: 4.6010 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.6010\n",
      "Batch: 3 \t Epoch : 85\tNet Loss: 4.4504 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4504\n",
      "Batch: 4 \t Epoch : 85\tNet Loss: 4.5403 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5403\n",
      "Batch: 5 \t Epoch : 85\tNet Loss: 5.4682 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.4682\n",
      "Average Loss after Epoch 85 : 3.6504\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.2383\tQuestion Loss (full generated): 118.1673\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2098\tQuestion Loss Masked(full generated): 7.0325\n",
      "Average loss (full gen): 118.1673\n",
      "Eval time: 0.45s\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the the <UNK> ? of          \n",
      "Full Generated: what                    \n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what ?  ? the ? ? ? <UNK> <UNK> ? by ?        \n",
      "Full Generated: what                    \n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 66.6316\tQuestion Loss (full generated): 93.7796\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.7092\tQuestion Loss Masked(full generated): 6.0197\n",
      "Average loss (full gen): 93.7796\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ?  rank among <UNK> 's ? cities ?           \n",
      "Full Generated: what                    \n",
      "Ground Truth..: where does valencia rank among spain 's largest cities ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> ? sign the              \n",
      "Full Generated: what                    \n",
      "Ground Truth..: what is the <UNK> equal sign ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 86\tNet Loss: 4.4518 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.4518\n",
      "Batch: 1 \t Epoch : 86\tNet Loss: 5.7578 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.7578\n",
      "Batch: 2 \t Epoch : 86\tNet Loss: 4.6725 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.6725\n",
      "Batch: 3 \t Epoch : 86\tNet Loss: 4.1774 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.1774\n",
      "Batch: 4 \t Epoch : 86\tNet Loss: 4.5767 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5767\n",
      "Batch: 5 \t Epoch : 86\tNet Loss: 4.6650 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.6650\n",
      "Average Loss after Epoch 86 : 3.5377\n",
      "Epoch time: 1.44s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.9683\tQuestion Loss (full generated): 128.9034\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2080\tQuestion Loss Masked(full generated): 7.2110\n",
      "Average loss (full gen): 128.9034\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what  ? <UNK> the the ? ? ? <UNK> festival ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the ? <UNK> <UNK> were <UNK> <UNK> that  <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 71.1661\tQuestion Loss (full generated): 106.0099\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0496\tQuestion Loss Masked(full generated): 6.1484\n",
      "Average loss (full gen): 106.0099\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what  the <UNK> <UNK> spain ? the  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did these three categories <UNK> with ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the the   <UNK> ? community ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where is there nothing like this <UNK> speech community ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 87\tNet Loss: 4.6657 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.6657\n",
      "Batch: 1 \t Epoch : 87\tNet Loss: 5.0970 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.0970\n",
      "Batch: 2 \t Epoch : 87\tNet Loss: 5.2606 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.2606\n",
      "Batch: 3 \t Epoch : 87\tNet Loss: 4.7381 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7381\n",
      "Batch: 4 \t Epoch : 87\tNet Loss: 4.8876 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8876\n",
      "Batch: 5 \t Epoch : 87\tNet Loss: 4.5468 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.5468\n",
      "Average Loss after Epoch 87 : 3.6495\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 107.1780\tQuestion Loss (full generated): 128.7738\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2072\tQuestion Loss Masked(full generated): 7.1963\n",
      "Average loss (full gen): 128.7738\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? the <UNK> of ? ? britain  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what  the ? ?  the the the the the ? ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 71.3750\tQuestion Loss (full generated): 105.9005\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0390\tQuestion Loss Masked(full generated): 6.1283\n",
      "Average loss (full gen): 105.9005\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? believed the ? ? the the  <UNK> ? <UNK> of age ? the ?  <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "T.F. Generated: what  the <UNK> call  <UNK> ? ? fort ? ? ? ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: what did american settlers call the <UNK> who attacked fort <UNK> in <UNK> ?      \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 88\tNet Loss: 5.4495 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.4495\n",
      "Batch: 1 \t Epoch : 88\tNet Loss: 4.9744 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.9744\n",
      "Batch: 2 \t Epoch : 88\tNet Loss: 4.9951 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.9951\n",
      "Batch: 3 \t Epoch : 88\tNet Loss: 4.2793 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2793\n",
      "Batch: 4 \t Epoch : 88\tNet Loss: 4.7269 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.7269\n",
      "Batch: 5 \t Epoch : 88\tNet Loss: 5.0892 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0892\n",
      "Average Loss after Epoch 88 : 3.6893\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.9457\tQuestion Loss (full generated): 128.4510\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2059\tQuestion Loss Masked(full generated): 7.1784\n",
      "Average loss (full gen): 128.4510\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? the <UNK> of ? ? britain  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what ?  ? the ? ? ? ? <UNK> ? by ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 70.9261\tQuestion Loss (full generated): 105.4683\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0275\tQuestion Loss Masked(full generated): 6.0980\n",
      "Average loss (full gen): 105.4683\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? believed the ? ? the the  <UNK> ? <UNK> of age ? the ?  <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the the   <UNK> ? community ?  <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
      "Full Generated: what  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK> ?  <UNK>\n",
      "Ground Truth..: where is there nothing like this <UNK> speech community ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 89\tNet Loss: 5.8912 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.8912\n",
      "Batch: 1 \t Epoch : 89\tNet Loss: 4.2407 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2407\n",
      "Batch: 2 \t Epoch : 89\tNet Loss: 5.1406 \tAnswer Loss: 0.6526 \tQuestion Loss: 5.1406\n",
      "Batch: 3 \t Epoch : 89\tNet Loss: 4.3805 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3805\n",
      "Batch: 4 \t Epoch : 89\tNet Loss: 4.5484 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5484\n",
      "Batch: 5 \t Epoch : 89\tNet Loss: 4.6418 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.6418\n",
      "Average Loss after Epoch 89 : 3.6054\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.8456\tQuestion Loss (full generated): 106.8410\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2049\tQuestion Loss Masked(full generated): 6.6038\n",
      "Average loss (full gen): 106.8410\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?  ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the ? <UNK> <UNK> were <UNK> <UNK> that  ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 69.4832\tQuestion Loss (full generated): 77.1234\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.9118\tQuestion Loss Masked(full generated): 5.3162\n",
      "Average loss (full gen): 77.1234\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what was festival ? the in ? in ? haven 's  ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what musical festival was initiated in 1982 in new haven ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what was <UNK>  ?   ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what are `` <UNK> '' ?              \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 90\tNet Loss: 4.8072 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.8072\n",
      "Batch: 1 \t Epoch : 90\tNet Loss: 4.1602 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.1602\n",
      "Batch: 2 \t Epoch : 90\tNet Loss: 4.6509 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.6509\n",
      "Batch: 3 \t Epoch : 90\tNet Loss: 4.0908 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.0908\n",
      "Batch: 4 \t Epoch : 90\tNet Loss: 4.3361 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.3361\n",
      "Batch: 5 \t Epoch : 90\tNet Loss: 4.3965 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.3965\n",
      "Average Loss after Epoch 90 : 3.3052\n",
      "Epoch time: 1.36s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.7617\tQuestion Loss (full generated): 106.7851\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2031\tQuestion Loss Masked(full generated): 6.6017\n",
      "Average loss (full gen): 106.7851\n",
      "Eval time: 0.42s\n",
      "T.F. Generated: what the ? the the  ? <UNK> ? ? the <UNK> ? of  ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the the ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 70.4310\tQuestion Loss (full generated): 72.0583\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 4.0025\tQuestion Loss Masked(full generated): 4.8946\n",
      "Average loss (full gen): 72.0583\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what ? the <UNK> ? ?  ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: where were the activists <UNK> ?              \n",
      "\n",
      "\n",
      "T.F. Generated: what was the <UNK> of <UNK> <UNK> <UNK> ? guam ?  ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what was the name of the general who claimed guam ?         \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 91\tNet Loss: 4.7660 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.7660\n",
      "Batch: 1 \t Epoch : 91\tNet Loss: 4.8215 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8215\n",
      "Batch: 2 \t Epoch : 91\tNet Loss: 4.7179 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7179\n",
      "Batch: 3 \t Epoch : 91\tNet Loss: 4.6317 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6317\n",
      "Batch: 4 \t Epoch : 91\tNet Loss: 4.8305 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.8305\n",
      "Batch: 5 \t Epoch : 91\tNet Loss: 5.0306 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.0306\n",
      "Average Loss after Epoch 91 : 3.5998\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 106.1229\tQuestion Loss (full generated): 106.3945\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2019\tQuestion Loss Masked(full generated): 6.5957\n",
      "Average loss (full gen): 106.3945\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?  ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? was ? the ? ? ? ? <UNK> ? by ?  ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 68.1510\tQuestion Loss (full generated): 74.5004\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8985\tQuestion Loss Masked(full generated): 5.1783\n",
      "Average loss (full gen): 74.5004\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what with the ancestry always ? the the as ? was ?  ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: americans with african ancestry have always been classified as what race ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> practices teachings <UNK> of <UNK> ?  ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: which branch practices the teachings of <UNK> ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 92\tNet Loss: 4.5790 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.5790\n",
      "Batch: 1 \t Epoch : 92\tNet Loss: 4.3797 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3797\n",
      "Batch: 2 \t Epoch : 92\tNet Loss: 4.4890 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.4890\n",
      "Batch: 3 \t Epoch : 92\tNet Loss: 4.6986 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6986\n",
      "Batch: 4 \t Epoch : 92\tNet Loss: 4.5589 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5589\n",
      "Batch: 5 \t Epoch : 92\tNet Loss: 4.5644 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.5644\n",
      "Average Loss after Epoch 92 : 3.4087\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 105.0412\tQuestion Loss (full generated): 105.7882\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2032\tQuestion Loss Masked(full generated): 6.5918\n",
      "Average loss (full gen): 105.7882\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what ? the <UNK> of ? ? britain  ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what ? was ? the ? ? ? ? <UNK> ? by ?  ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 69.3147\tQuestion Loss (full generated): 78.3453\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.9827\tQuestion Loss Masked(full generated): 5.2338\n",
      "Average loss (full gen): 78.3453\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what ? the <UNK> ? to the <UNK> ? <UNK>  ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: when did the <UNK> surrender to the <UNK> army ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what ? ? <UNK> , ? <UNK> library found the <UNK> <UNK> the <UNK> 's ? the the many <UNK> \n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: in <UNK> the grade , the corporate library found that a third of comcast 's board was how old ?\n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 93\tNet Loss: 4.3775 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.3775\n",
      "Batch: 1 \t Epoch : 93\tNet Loss: 4.2144 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2144\n",
      "Batch: 2 \t Epoch : 93\tNet Loss: 4.7184 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7184\n",
      "Batch: 3 \t Epoch : 93\tNet Loss: 4.9068 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.9068\n",
      "Batch: 4 \t Epoch : 93\tNet Loss: 4.5047 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5047\n",
      "Batch: 5 \t Epoch : 93\tNet Loss: 4.9669 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.9669\n",
      "Average Loss after Epoch 93 : 3.4611\n",
      "Epoch time: 1.35s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 104.5258\tQuestion Loss (full generated): 105.5233\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2052\tQuestion Loss Masked(full generated): 6.5922\n",
      "Average loss (full gen): 105.5233\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the ? ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the ? <UNK> <UNK> were <UNK> <UNK> that  ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 63.1454\tQuestion Loss (full generated): 75.7597\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.6119\tQuestion Loss Masked(full generated): 5.2351\n",
      "Average loss (full gen): 75.7597\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what was the <UNK> rank called ? ? ?  ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what was the valencia cathedral called from <UNK> ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the defeat arsenal ? <UNK> presidential election ?  ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: who did eisenhower defeat in the 1956 presidential election ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 94\tNet Loss: 4.5803 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.5803\n",
      "Batch: 1 \t Epoch : 94\tNet Loss: 4.6399 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6399\n",
      "Batch: 2 \t Epoch : 94\tNet Loss: 4.5531 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.5531\n",
      "Batch: 3 \t Epoch : 94\tNet Loss: 4.7648 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7648\n",
      "Batch: 4 \t Epoch : 94\tNet Loss: 4.6718 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.6718\n",
      "Batch: 5 \t Epoch : 94\tNet Loss: 4.5322 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.5322\n",
      "Average Loss after Epoch 94 : 3.4678\n",
      "Epoch time: 1.41s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 103.8081\tQuestion Loss (full generated): 105.1502\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2067\tQuestion Loss Masked(full generated): 6.5915\n",
      "Average loss (full gen): 105.1502\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? was ? the ? ? ? ? <UNK> ? by ?  ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what was the ? ?  the the the <UNK> the ? ? ?  ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 65.6359\tQuestion Loss (full generated): 69.4335\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.9723\tQuestion Loss Masked(full generated): 4.8576\n",
      "Average loss (full gen): 69.4335\n",
      "Eval time: 0.36s\n",
      "T.F. Generated: what ? the <UNK> called <UNK> maria maria ?  ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: where is the cathedral of santa maria <UNK> ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what was the <UNK> <UNK> spain ? the  ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what did these three categories <UNK> with ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 95\tNet Loss: 4.5601 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.5601\n",
      "Batch: 1 \t Epoch : 95\tNet Loss: 4.3746 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3746\n",
      "Batch: 2 \t Epoch : 95\tNet Loss: 4.8269 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.8269\n",
      "Batch: 3 \t Epoch : 95\tNet Loss: 4.1726 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.1726\n",
      "Batch: 4 \t Epoch : 95\tNet Loss: 4.9518 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9518\n",
      "Batch: 5 \t Epoch : 95\tNet Loss: 4.6126 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.6126\n",
      "Average Loss after Epoch 95 : 3.4373\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 102.4254\tQuestion Loss (full generated): 104.3903\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2073\tQuestion Loss Masked(full generated): 6.5882\n",
      "Average loss (full gen): 104.3903\n",
      "Eval time: 0.42s\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?  ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what the ? the the  ? <UNK> ? ? the <UNK> ? of  ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 62.0041\tQuestion Loss (full generated): 71.5725\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8556\tQuestion Loss Masked(full generated): 5.1741\n",
      "Average loss (full gen): 71.5725\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what many the ? the speak ?  ? ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: how much english did tito speak ?             \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> practices teachings <UNK> of <UNK> ?  ? ? ? ? ? ? ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: which branch practices the teachings of <UNK> ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 96\tNet Loss: 4.4097 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.4097\n",
      "Batch: 1 \t Epoch : 96\tNet Loss: 4.2676 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2676\n",
      "Batch: 2 \t Epoch : 96\tNet Loss: 4.5934 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.5934\n",
      "Batch: 3 \t Epoch : 96\tNet Loss: 3.9744 \tAnswer Loss: 0.6518 \tQuestion Loss: 3.9744\n",
      "Batch: 4 \t Epoch : 96\tNet Loss: 4.5843 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5843\n",
      "Batch: 5 \t Epoch : 96\tNet Loss: 4.8941 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.8941\n",
      "Average Loss after Epoch 96 : 3.3404\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 101.3851\tQuestion Loss (full generated): 103.8411\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2075\tQuestion Loss Masked(full generated): 6.5869\n",
      "Average loss (full gen): 103.8411\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what the ? the the  ? <UNK> ? ? the <UNK> ? of  ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the ? <UNK> <UNK> were <UNK> <UNK> that  ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 61.8061\tQuestion Loss (full generated): 67.2899\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.9525\tQuestion Loss Masked(full generated): 4.8308\n",
      "Average loss (full gen): 67.2899\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what ? believed the ? ? the the  <UNK> ? <UNK> of age ? the ?  ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: who strongly believed <UNK> <UNK> wanted to restore the <UNK> of its imperial age to tibet ?   \n",
      "\n",
      "\n",
      "T.F. Generated: what was the <UNK> call  <UNK> ? ? fort ? ? ? ?  ? ? ? ? ? ?\n",
      "Full Generated: what was the <UNK> ?  ?  ?  ?  ?  ?  ?  ?  ?\n",
      "Ground Truth..: what did american settlers call the <UNK> who attacked fort <UNK> in <UNK> ?      \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 97\tNet Loss: 4.4488 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.4488\n",
      "Batch: 1 \t Epoch : 97\tNet Loss: 4.7354 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7354\n",
      "Batch: 2 \t Epoch : 97\tNet Loss: 4.6368 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.6368\n",
      "Batch: 3 \t Epoch : 97\tNet Loss: 4.5261 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.5261\n",
      "Batch: 4 \t Epoch : 97\tNet Loss: 4.6348 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.6348\n",
      "Batch: 5 \t Epoch : 97\tNet Loss: 4.2325 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.2325\n",
      "Average Loss after Epoch 97 : 3.4018\n",
      "Epoch time: 1.49s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.5537\tQuestion Loss (full generated): 106.2276\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2086\tQuestion Loss Masked(full generated): 6.5999\n",
      "Average loss (full gen): 106.2276\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the <UNK> <UNK> ? of          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> exchange ? ? britain             \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 59.3340\tQuestion Loss (full generated): 77.3560\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8455\tQuestion Loss Masked(full generated): 5.4299\n",
      "Average loss (full gen): 77.3560\n",
      "Eval time: 0.45s\n",
      "T.F. Generated: what <UNK> were the to the <UNK> of <UNK> football ?          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: which rules were influential to the codes of association football ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of <UNK> <UNK> of <UNK> ?           \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who was the leader of the revolt of <UNK> ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 98\tNet Loss: 4.9610 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.9610\n",
      "Batch: 1 \t Epoch : 98\tNet Loss: 4.8089 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8089\n",
      "Batch: 2 \t Epoch : 98\tNet Loss: 4.5478 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.5478\n",
      "Batch: 3 \t Epoch : 98\tNet Loss: 4.7224 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7224\n",
      "Batch: 4 \t Epoch : 98\tNet Loss: 4.4894 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.4894\n",
      "Batch: 5 \t Epoch : 98\tNet Loss: 4.6000 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.6000\n",
      "Average Loss after Epoch 98 : 3.5162\n",
      "Epoch time: 1.46s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.1904\tQuestion Loss (full generated): 106.0253\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2097\tQuestion Loss Masked(full generated): 6.6121\n",
      "Average loss (full gen): 106.0253\n",
      "Eval time: 0.42s\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the <UNK> <UNK> ? of          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 58.6711\tQuestion Loss (full generated): 76.5425\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8366\tQuestion Loss Masked(full generated): 5.4077\n",
      "Average loss (full gen): 76.5425\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what many ? ? the <UNK> ? begin             \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: how long ago did the <UNK> period begin ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> <UNK> the <UNK> model the <UNK> the <UNK> model <UNK> ? ?       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: which model is of federalism is similar to the federalism model in australia ?      \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 99\tNet Loss: 4.7658 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.7658\n",
      "Batch: 1 \t Epoch : 99\tNet Loss: 4.3297 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3297\n",
      "Batch: 2 \t Epoch : 99\tNet Loss: 4.2703 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.2703\n",
      "Batch: 3 \t Epoch : 99\tNet Loss: 4.4473 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.4473\n",
      "Batch: 4 \t Epoch : 99\tNet Loss: 5.1091 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.1091\n",
      "Batch: 5 \t Epoch : 99\tNet Loss: 4.4928 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.4928\n",
      "Average Loss after Epoch 99 : 3.4269\n",
      "Epoch time: 1.46s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.0412\tQuestion Loss (full generated): 105.8719\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2104\tQuestion Loss Masked(full generated): 6.6125\n",
      "Average loss (full gen): 105.8719\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the <UNK> <UNK> ? of          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what was the ? ?  the the the <UNK> the ? ? ?       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 55.8132\tQuestion Loss (full generated): 74.3953\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.5401\tQuestion Loss Masked(full generated): 5.1215\n",
      "Average loss (full gen): 74.3953\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what was the <UNK> rank called <UNK> ? ?            \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what was the valencia cathedral called from <UNK> ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what was the <UNK> priest of ? <UNK> ? <UNK> ? <UNK>         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 100\tNet Loss: 4.3702 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.3702\n",
      "Batch: 1 \t Epoch : 100\tNet Loss: 4.5942 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.5942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 \t Epoch : 100\tNet Loss: 4.4706 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.4706\n",
      "Batch: 3 \t Epoch : 100\tNet Loss: 4.6442 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6442\n",
      "Batch: 4 \t Epoch : 100\tNet Loss: 4.7789 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.7789\n",
      "Batch: 5 \t Epoch : 100\tNet Loss: 5.1051 \tAnswer Loss: 0.6538 \tQuestion Loss: 5.1051\n",
      "Average Loss after Epoch 100 : 3.4954\n",
      "Epoch time: 1.44s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.2282\tQuestion Loss (full generated): 106.0296\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2118\tQuestion Loss Masked(full generated): 6.6119\n",
      "Average loss (full gen): 106.0296\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the <UNK> <UNK> ? of          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of an <UNK> the the ? <UNK> <UNK> that <UNK> <UNK> that     \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: where was the residential neighborhood that was removed from the final boundaries of the district ?    \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 58.5155\tQuestion Loss (full generated): 75.8589\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8219\tQuestion Loss Masked(full generated): 5.3466\n",
      "Average loss (full gen): 75.8589\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what was do ? <UNK> arsenal ? <UNK> ?            \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what occurs when a wrestler <UNK> the ring ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> of <UNK> <UNK> of <UNK> ?           \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who was the leader of the revolt of <UNK> ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 101\tNet Loss: 5.1576 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.1576\n",
      "Batch: 1 \t Epoch : 101\tNet Loss: 4.6259 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6259\n",
      "Batch: 2 \t Epoch : 101\tNet Loss: 4.7350 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7350\n",
      "Batch: 3 \t Epoch : 101\tNet Loss: 4.3272 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3272\n",
      "Batch: 4 \t Epoch : 101\tNet Loss: 5.1029 \tAnswer Loss: 0.6535 \tQuestion Loss: 5.1029\n",
      "Batch: 5 \t Epoch : 101\tNet Loss: 4.2934 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.2934\n",
      "Average Loss after Epoch 101 : 3.5302\n",
      "Epoch time: 1.30s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.5573\tQuestion Loss (full generated): 106.3209\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2141\tQuestion Loss Masked(full generated): 6.6116\n",
      "Average loss (full gen): 106.3209\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> exchange ? ? britain             \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 58.9199\tQuestion Loss (full generated): 76.2015\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8190\tQuestion Loss Masked(full generated): 5.3383\n",
      "Average loss (full gen): 76.2015\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what was festival ? the in ? in ? haven 's          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what musical festival was initiated in 1982 in new haven ?         \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> were the to the <UNK> of <UNK> football ?          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: which rules were influential to the codes of association football ?         \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 102\tNet Loss: 4.4206 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.4206\n",
      "Batch: 1 \t Epoch : 102\tNet Loss: 4.7994 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7994\n",
      "Batch: 2 \t Epoch : 102\tNet Loss: 4.4150 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.4150\n",
      "Batch: 3 \t Epoch : 102\tNet Loss: 4.6653 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.6653\n",
      "Batch: 4 \t Epoch : 102\tNet Loss: 4.5643 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5643\n",
      "Batch: 5 \t Epoch : 102\tNet Loss: 4.9734 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.9734\n",
      "Average Loss after Epoch 102 : 3.4798\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.7650\tQuestion Loss (full generated): 106.6227\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2161\tQuestion Loss Masked(full generated): 6.6201\n",
      "Average loss (full gen): 106.6227\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? of       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? was ? the ? ? ? ? <UNK> ? by ?        \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: in what year did <UNK> <UNK> take over the <UNK> controlled city ?       \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 58.5881\tQuestion Loss (full generated): 72.6364\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.7903\tQuestion Loss Masked(full generated): 5.0479\n",
      "Average loss (full gen): 72.6364\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what ? beyonce do                 \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "T.F. Generated: what was the <UNK> ? ? the <UNK> ?  access to the and ? of <UNK> ?   \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what is a <UNK> <UNK> way that <UNK> can get access to marketing and word of mouth ?  \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 103\tNet Loss: 5.1142 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.1142\n",
      "Batch: 1 \t Epoch : 103\tNet Loss: 4.8429 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8429\n",
      "Batch: 2 \t Epoch : 103\tNet Loss: 4.7080 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7080\n",
      "Batch: 3 \t Epoch : 103\tNet Loss: 4.1375 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.1375\n",
      "Batch: 4 \t Epoch : 103\tNet Loss: 4.4153 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.4153\n",
      "Batch: 5 \t Epoch : 103\tNet Loss: 4.0564 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.0564\n",
      "Average Loss after Epoch 103 : 3.4093\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.6390\tQuestion Loss (full generated): 106.6222\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2171\tQuestion Loss Masked(full generated): 6.6298\n",
      "Average loss (full gen): 106.6222\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what was the ? ?  <UNK> the the <UNK> the ? ? ?       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what ? ? <UNK> ? <UNK> the <UNK> <UNK> ? of          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who attended the coronation that was a break from tradition ?         \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 59.8613\tQuestion Loss (full generated): 70.1236\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8969\tQuestion Loss Masked(full generated): 4.7990\n",
      "Average loss (full gen): 70.1236\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what was the <UNK> call  <UNK> ? ? fort ? ? ? ?       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what did american settlers call the <UNK> who attacked fort <UNK> in <UNK> ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what was ? are <UNK> the ? ?             \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> movements are used for <UNK> ?            \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 104\tNet Loss: 4.8175 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.8175\n",
      "Batch: 1 \t Epoch : 104\tNet Loss: 3.8678 \tAnswer Loss: 0.6518 \tQuestion Loss: 3.8678\n",
      "Batch: 2 \t Epoch : 104\tNet Loss: 4.4964 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.4964\n",
      "Batch: 3 \t Epoch : 104\tNet Loss: 4.8394 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.8394\n",
      "Batch: 4 \t Epoch : 104\tNet Loss: 4.5479 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 5 \t Epoch : 104\tNet Loss: 4.2932 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.2932\n",
      "Average Loss after Epoch 104 : 3.3578\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.8818\tQuestion Loss (full generated): 106.9731\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2170\tQuestion Loss Masked(full generated): 6.6371\n",
      "Average loss (full gen): 106.9731\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what the ? the the  <UNK> <UNK> ? ? the <UNK> ? of       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: while <UNK> was used my most german <UNK> who used the <UNK> system ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 59.0231\tQuestion Loss (full generated): 75.8387\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.7913\tQuestion Loss Masked(full generated): 5.2696\n",
      "Average loss (full gen): 75.8387\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what many ? ? the <UNK> ? begin             \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: how long ago did the <UNK> period begin ?           \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> were the to the <UNK> of <UNK> football ?          \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: which rules were influential to the codes of association football ?         \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 105\tNet Loss: 4.7088 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.7088\n",
      "Batch: 1 \t Epoch : 105\tNet Loss: 4.1214 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.1214\n",
      "Batch: 2 \t Epoch : 105\tNet Loss: 4.2789 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.2789\n",
      "Batch: 3 \t Epoch : 105\tNet Loss: 5.1165 \tAnswer Loss: 0.6518 \tQuestion Loss: 5.1165\n",
      "Batch: 4 \t Epoch : 105\tNet Loss: 4.4400 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.4400\n",
      "Batch: 5 \t Epoch : 105\tNet Loss: 4.0357 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.0357\n",
      "Average Loss after Epoch 105 : 3.3377\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 100.8271\tQuestion Loss (full generated): 107.0294\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2178\tQuestion Loss Masked(full generated): 6.6455\n",
      "Average loss (full gen): 107.0294\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what was the ? ?  <UNK> the the <UNK> the ? ? ?       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 61.9238\tQuestion Loss (full generated): 78.7771\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.8599\tQuestion Loss Masked(full generated): 5.2218\n",
      "Average loss (full gen): 78.7771\n",
      "Eval time: 0.39s\n",
      "T.F. Generated: what ? ? ? the over ? <UNK> years ?           \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: have transfer <UNK> been increasing over the past years ?          \n",
      "\n",
      "\n",
      "T.F. Generated: what many <UNK> ? of <UNK> exist plymouth ? ?           \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: how many <UNK> houses of worship exist in plymouth ?          \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 106\tNet Loss: 5.3318 \tAnswer Loss: 0.6534 \tQuestion Loss: 5.3318\n",
      "Batch: 1 \t Epoch : 106\tNet Loss: 4.3877 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.3877\n",
      "Batch: 2 \t Epoch : 106\tNet Loss: 4.4998 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.4998\n",
      "Batch: 3 \t Epoch : 106\tNet Loss: 4.2195 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2195\n",
      "Batch: 4 \t Epoch : 106\tNet Loss: 4.5354 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5354\n",
      "Batch: 5 \t Epoch : 106\tNet Loss: 4.5371 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.5371\n",
      "Average Loss after Epoch 106 : 3.4389\n",
      "Epoch time: 1.38s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 101.2680\tQuestion Loss (full generated): 107.5967\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2193\tQuestion Loss Masked(full generated): 6.6558\n",
      "Average loss (full gen): 107.5967\n",
      "Eval time: 0.35s\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what ? the <UNK> exchange ? ? britain             \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 58.6986\tQuestion Loss (full generated): 72.9134\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.7393\tQuestion Loss Masked(full generated): 5.0124\n",
      "Average loss (full gen): 72.9134\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what ? beyonce do                 \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: who influenced beyonce ?                \n",
      "\n",
      "\n",
      "T.F. Generated: what many the <UNK> win taiwan ?              \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: how did the japanese win taiwan ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 107\tNet Loss: 4.2219 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.2219\n",
      "Batch: 1 \t Epoch : 107\tNet Loss: 4.1395 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.1395\n",
      "Batch: 2 \t Epoch : 107\tNet Loss: 4.7444 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.7444\n",
      "Batch: 3 \t Epoch : 107\tNet Loss: 4.7833 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.7833\n",
      "Batch: 4 \t Epoch : 107\tNet Loss: 4.9442 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.9442\n",
      "Batch: 5 \t Epoch : 107\tNet Loss: 4.5777 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.5777\n",
      "Average Loss after Epoch 107 : 3.4264\n",
      "Epoch time: 1.39s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 101.3751\tQuestion Loss (full generated): 107.9115\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2201\tQuestion Loss Masked(full generated): 6.6709\n",
      "Average loss (full gen): 107.9115\n",
      "Eval time: 0.40s\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the ? ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "T.F. Generated: what was ? <UNK> the the ? ? ? <UNK> festival ?         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 58.7206\tQuestion Loss (full generated): 72.9963\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.7284\tQuestion Loss Masked(full generated): 5.0071\n",
      "Average loss (full gen): 72.9963\n",
      "Eval time: 0.38s\n",
      "T.F. Generated: what was originally the the <UNK> cities to the haven 's ? ?        \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what sector originally provided the largest contribution to new haven 's economy ?       \n",
      "\n",
      "\n",
      "T.F. Generated: what many the ? the speak ?              \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: how much english did tito speak ?             \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 108\tNet Loss: 4.3783 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.3783\n",
      "Batch: 1 \t Epoch : 108\tNet Loss: 3.8581 \tAnswer Loss: 0.6518 \tQuestion Loss: 3.8581\n",
      "Batch: 2 \t Epoch : 108\tNet Loss: 4.5295 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.5295\n",
      "Batch: 3 \t Epoch : 108\tNet Loss: 4.2895 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2895\n",
      "Batch: 4 \t Epoch : 108\tNet Loss: 4.5099 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5099\n",
      "Batch: 5 \t Epoch : 108\tNet Loss: 4.4359 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.4359\n",
      "Average Loss after Epoch 108 : 3.2502\n",
      "Epoch time: 1.40s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 101.2421\tQuestion Loss (full generated): 107.9459\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2203\tQuestion Loss Masked(full generated): 6.6826\n",
      "Average loss (full gen): 107.9459\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what ? the <UNK> exchange ? ? britain             \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: where did the traders from <UNK> settle ?            \n",
      "\n",
      "\n",
      "T.F. Generated: what many the khan ? the <UNK> ? <UNK> <UNK> the ? ? the ? the <UNK> ? ? ? \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: how did nasser <UNK> to the news the the us and uk had <UNK> construction of <UNK> <UNK> <UNK> ?\n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\tQuestion Loss (teacher forcing): 56.1500\tQuestion Loss (full generated): 74.2184\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 3.4407\tQuestion Loss Masked(full generated): 4.9784\n",
      "Average loss (full gen): 74.2184\n",
      "Eval time: 0.41s\n",
      "T.F. Generated: what was the <UNK> priest of ? <UNK> ? <UNK> ? <UNK>         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what was the high priest in the college of <UNK> called ?        \n",
      "\n",
      "\n",
      "T.F. Generated: what <UNK> empire ? the dominated by whom the            \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: the seleucid empire was mostly dominated by whom ?           \n",
      "\n",
      "\n",
      "No. of batches in training data: 6, with batch_size: 8 \n",
      "Batch: 0 \t Epoch : 109\tNet Loss: 4.3663 \tAnswer Loss: 0.6534 \tQuestion Loss: 4.3663\n",
      "Batch: 1 \t Epoch : 109\tNet Loss: 4.2728 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2728\n",
      "Batch: 2 \t Epoch : 109\tNet Loss: 4.8964 \tAnswer Loss: 0.6526 \tQuestion Loss: 4.8964\n",
      "Batch: 3 \t Epoch : 109\tNet Loss: 4.2522 \tAnswer Loss: 0.6518 \tQuestion Loss: 4.2522\n",
      "Batch: 4 \t Epoch : 109\tNet Loss: 4.5976 \tAnswer Loss: 0.6535 \tQuestion Loss: 4.5976\n",
      "Batch: 5 \t Epoch : 109\tNet Loss: 4.8353 \tAnswer Loss: 0.6538 \tQuestion Loss: 4.8353\n",
      "Average Loss after Epoch 109 : 3.4026\n",
      "Epoch time: 1.37s\n",
      "Test Data:\n",
      "Evaluating:\n",
      "Batch: 0\tQuestion Loss (teacher forcing): 101.2493\tQuestion Loss (full generated): 107.9880\n",
      "Batch: 0\tQuestion Loss Masked(teacher forcing): 6.2207\tQuestion Loss Masked(full generated): 6.6854\n",
      "Average loss (full gen): 107.9880\n",
      "Eval time: 0.37s\n",
      "T.F. Generated: what was the ? ? ? <UNK> ? the <UNK> the ? ? ?       \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what did people once believe could not be found further south than australia ?      \n",
      "\n",
      "\n",
      "T.F. Generated: what was ? <UNK> the ? ? ? ? <UNK> festival ?         \n",
      "Full Generated: what was the <UNK> ?                \n",
      "Ground Truth..: what <UNK> character had his own <UNK> series of comic books ?        \n",
      "\n",
      "\n",
      "Train Data:\n",
      "Evaluating:\n"
     ]
    }
   ],
   "source": [
    "%run main.py --no_eval \\\n",
    "--train_data \"/home/ra2630/NLU/train-v1.1.json\" \\\n",
    "--tf_ratio 1.0 \\\n",
    "--tf_ratio_decay_rate 0.99 \\\n",
    "--gpu \\\n",
    "--gen_test \\\n",
    "--gen_test_number 2 \\\n",
    "--gen_train \\\n",
    "--gen_train_number 2 \\\n",
    "--word_tf \\\n",
    "--use_attention \\\n",
    "--use_masked_loss \\\n",
    "--num_epochs 500 \\\n",
    "--batch_size 8 \\\n",
    "--words_to_take 5000 \\\n",
    "--example_to_train 64 \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printComp(doc_tokens):\n",
    "    for i in doc_tokens:\n",
    "        print(look_up_token(i), sep = ' ', end = ' ')\n",
    "    print(\"\")\n",
    "def printAnswer():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printComp(batches[22]['document_tokens'][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printComp(batches[22]['document_tokens'][120])\n",
    "printComp(batches[22]['question_input_tokens'][120])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
