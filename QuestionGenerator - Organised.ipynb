{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "!CUDA_VISIBLE_DEVICES=2\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "from embedding import *\n",
    "import nltk\n",
    "import itertools\n",
    "import random\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "nltkStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordsToTake = 80000\n",
    "words = findKMostFrequentWords(400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "PAD_TOKEN = _add_word_reduced(PAD_WORD)\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "    if(len(reduced_glove) == wordsToTake):\n",
    "        break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i], end = ' ', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples_to_take_train = 72000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reduced_glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(PAD_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "        if(j<max_len):\n",
    "            X_indices[i,j] = look_up_word_reduced(END_WORD)\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    start = 0\n",
    "    while start < len(inputs[0]):\n",
    "        end = min(len(inputs[0]), start + batch_size)\n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_mask': [],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[],\n",
    "                 }\n",
    "        \n",
    "        for index,inp in enumerate(inputs):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            \n",
    "            if index == 0:\n",
    "                output['document_tokens'].append(inp[start:end,0:maxD])\n",
    "            elif index==1:\n",
    "                output['document_lengths'].append(inp[start:end])\n",
    "            elif index==2:\n",
    "                output['answer_labels'].append(inp[start:end,0:maxD])\n",
    "            elif index==3:\n",
    "                output['answer_mask'].append(inp[start:end,0:maxA,0:maxD])\n",
    "            elif index==4:\n",
    "                output['answer_lengths'].append(inp[start:end])\n",
    "            elif index==5:\n",
    "                output['question_input_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==6:\n",
    "                output['question_output_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==7:\n",
    "                output['question_lengths'].append(inp[start:end])\n",
    "            \n",
    "        output[\"document_tokens\"] = np.array(output[\"document_tokens\"])\n",
    "        output[\"document_lengths\"] = np.array(output[\"document_lengths\"])\n",
    "        output[\"answer_labels\"] = np.array(output[\"answer_labels\"])\n",
    "        output[\"answer_mask\"] = np.array(output[\"answer_mask\"])\n",
    "        output[\"answer_lengths\"] = np.array(output[\"answer_lengths\"])\n",
    "        output[\"question_input_tokens\"] = np.array(output[\"question_input_tokens\"])\n",
    "        output[\"question_output_tokens\"] = np.array(output[\"question_output_tokens\"])\n",
    "        output[\"question_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        outputs.append(output)\n",
    "        start = start + batch_size\n",
    "            \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths]\n",
    "                    ,batch_size)\n",
    "\n",
    "for b in batch_input:\n",
    "    for k, v in b.items():\n",
    "        b[k] = v.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of features:\",len(batch_input[0]))\n",
    "print(\"No of batches:\",len(batch_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_ratio = 0.8\n",
    "split = int(len(batch_input) * split_ratio)\n",
    "batch_input_train = batch_input[0:split]\n",
    "batch_input_test = batch_input[split:]\n",
    "print(\"Number of train batches = \", len(batch_input_train))\n",
    "print(\"Number of test batches = \", len(batch_input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HelperFunctions:\n",
    "    def __init__(self):\n",
    "        None\n",
    "    def printDoc(self, batch, batch_num, example_num):\n",
    "        for i in batch[batch_num]['document_tokens'][example_num]:\n",
    "            print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "        print(\" \")\n",
    "\n",
    "    def printQues(self, batch, batch_num, example_num):\n",
    "        for i in batch[batch_num]['question_output_tokens'][example_num]:\n",
    "            print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "        print(\" \")\n",
    "        \n",
    "    def printGroundTruthFromBatch(self, batch, batchNum, exampleNum, printGroundTruthQuestion = True):\n",
    "        print(\"Comprehension : \")\n",
    "        self.printDoc(batch, batchNum ,exampleNum)\n",
    "        print(\"*****************************************************************************************************\")\n",
    "        if printGroundTruthQuestion:\n",
    "            print(\"Ground Truth Question : \")\n",
    "            self.printQues(batch, batchNum, exampleNum)\n",
    "            print(\"*****************************************************************************************************\")\n",
    "        print(\"Ground Truth Answer: \") #TODO FIX THIS !!!!\n",
    "        print(X_train_ans_shuffled[batch_size*batchNum + exampleNum])\n",
    "        print(\"*****************************************************************************************************\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FileUtils:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "    def clearFile(self):\n",
    "        open(self.filepath, 'w').close()\n",
    "    def appendString(self, string):\n",
    "        with open(self.filepath, \"a\") as myfile:\n",
    "            myfile.write(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class InferenceUtils:\n",
    "    def __init__(self, batch_size, qgen):\n",
    "        self.batch_size = batch_size\n",
    "        self.qgen = qgen\n",
    "        \n",
    "        \n",
    "    def createTokens(self, passage, batch_size):\n",
    "        passage = word_tokenize(passage.lower())\n",
    "        passage_len = len(passage)\n",
    "        p = np.array(sentences_to_indices_glove([passage], passage_len))\n",
    "        p_len = np.array(passage_len).reshape((1,))\n",
    "        p_batch = np.repeat(p,repeats=batch_size, axis = 0)\n",
    "        p_len_batch = np.repeat(p_len,repeats=batch_size, axis = 0)\n",
    "        return p_batch, p_len_batch\n",
    "    \n",
    "    def makeInferenceOnText2(self, passage, batch_size, use_beam = False):\n",
    "    \n",
    "        p_batch, p_len_batch = self.createTokens(passage, batch_size)\n",
    "        answers = session.run(qgen.answer_tags, {\n",
    "            qgen.d_tokens: p_batch,\n",
    "            qgen.d_lengths: p_len_batch,\n",
    "        })\n",
    "        answers = np.argmax(answers[0], 1)\n",
    "        print(\"Answers = \", answers)\n",
    "        answers = np.nonzero(answers)[0]\n",
    "        \n",
    "        outputs = []\n",
    "        for i in np.split(answers, np.where(np.diff(answers) != 1)[0]+1):\n",
    "            print(i)\n",
    "            left,right = i[0],i[-1]+1\n",
    "            answer = \" \".join(word_tokenize(passage.lower())[j] for j in range(left,right))\n",
    "            self.makeInferenceOnText(passage, answer, session, batch_size, use_beam)\n",
    "            \n",
    "    \n",
    "    def makeInferenceOnText(self, passage, answer, batch_size, use_beam = False):\n",
    "        passage = word_tokenize(passage.lower())\n",
    "        passage_len = len(passage)\n",
    "\n",
    "        answer = word_tokenize(answer.lower())\n",
    "        answer_len = len(answer)    \n",
    "\n",
    "        print(\"Passage Length = \", passage_len, \", Answer Length = \",answer_len)\n",
    "\n",
    "        left,right = find_sub_list(answer,passage)\n",
    "        if(left==-1):\n",
    "            print(\"Couldn't find answer in the passage !!\")\n",
    "            return\n",
    "        p = np.array(sentences_to_indices_glove([passage], passage_len))\n",
    "\n",
    "        p_len = np.array(passage_len).reshape((1,))\n",
    "        a_len = np.array(answer_len).reshape((1,))\n",
    "\n",
    "        ans_labels = np.zeros((1,passage_len))\n",
    "        ans_labels[0][left:right] = 1\n",
    "\n",
    "        enc_mask = np.zeros((1,answer_len, passage_len))\n",
    "        for i in range(left,right):\n",
    "            enc_mask[0,i-left,i] = 1\n",
    "\n",
    "        p_batch = np.repeat(p,repeats=batch_size, axis = 0)\n",
    "        p_len_batch = np.repeat(p_len,repeats=batch_size, axis = 0)\n",
    "        a_labels_batch = np.repeat(ans_labels,repeats=batch_size, axis = 0)\n",
    "        enc_mask_batch = np.repeat(enc_mask,repeats=batch_size, axis = 0)\n",
    "        a_len_batch = np.repeat(a_len,repeats=batch_size, axis = 0)\n",
    "        \n",
    "        output = {'document_tokens':p_batch,\n",
    "                    'document_lengths':p_len_batch,\n",
    "                    'answer_labels':a_labels_batch,\n",
    "                    'answer_mask': enc_mask_batch,\n",
    "                    'answer_lengths': a_len_batch\n",
    "                 }\n",
    "        if use_beam:\n",
    "            qgen.inferFromBatchBeam([output], 0, 1, 5, shuffle = False, printGroundTruthQuestion = False)\n",
    "        else: \n",
    "            qgen.inferFromBatch([output], 0, 1, shuffle = False, printGroundTruthQuestion = False)\n",
    "            \n",
    "    # Takes as input -> batch of labels and outputs in matrix form, gives mean bleu score for the batch\n",
    "    def bleu_score_batch(labels, outputs):\n",
    "        batch_bleu_score = 0\n",
    "        #np.where(hypothesis =='<EOS>')[0][0]\n",
    "        for i in range(labels.shape[0]):\n",
    "            gt = ' '.join(labels[i]).replace('<END>','')\n",
    "            out = ' '.join(outputs[i][1:]).replace('<END>','')\n",
    "            batch_bleu_score += nltk.translate.bleu_score.sentence_bleu(gt, out)\n",
    "        return(batch_bleu_score/labels.shape[0])\n",
    "    \n",
    "    # Takes as input 2 sentences and outputs the bleu score\n",
    "    def blue_score(label, output):\n",
    "        return(nltk.translate.bleu_score.sentence_bleu(label,output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QGen:\n",
    "    def __init__(\n",
    "        self, learning_rate, cell_size , n_layers, reduced_glove, batch_size, \n",
    "            attention_type = 'Luong', cell_type = 'GRU', sess=tf.Session(), \n",
    "            helperFunctions = HelperFunctions(), fileUtils = FileUtils('/data/ra2630/tfLog64k'),\n",
    "            grad_clip=1.0, beam_width=10, force_teaching_ratio=0.5):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cell_size = cell_size\n",
    "        self.cell_type = cell_type\n",
    "        self.attention_type = attention_type\n",
    "        self.n_layers = n_layers\n",
    "        self.grad_clip = grad_clip\n",
    "        self.glove_embedding = reduced_glove\n",
    "        self.batch_size = batch_size\n",
    "        self.beam_width = beam_width\n",
    "        self.force_teaching_ratio = force_teaching_ratio\n",
    "        self.sess = sess\n",
    "        self.helperFunctions = helperFunctions\n",
    "        self.fileUtils = fileUtils\n",
    "        self.build_graph()\n",
    "        \n",
    "    def saveSession(self, filepath):\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(self.sess, filepath)\n",
    "        \n",
    "    def loadFromSession(self, filepath):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.Session(config=config)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(self.sess, filepath)\n",
    "\n",
    "\n",
    "    def build_graph(self):\n",
    "        self.add_embedding_layer()\n",
    "        self.add_document_encoder_layer()\n",
    "        self.add_answer_encoder_layer()\n",
    "        with tf.variable_scope('decode'):\n",
    "            self.add_decoder_for_training()\n",
    "        with tf.variable_scope('decode', reuse=True):\n",
    "            self.add_beam_decoder_for_inference()\n",
    "        self.add_decoder_for_inference()\n",
    "            \n",
    "        \n",
    "        self.add_backward_path()\n",
    "\n",
    "\n",
    "    def add_embedding_layer(self): \n",
    "        self.embedding = tf.get_variable(\"embedding\", initializer=self.glove_embedding)\n",
    "        self. embedding = tf.cast(self.embedding, dtype=tf.float32)\n",
    "        self.embedding_dimensions = reduced_glove.shape[1]\n",
    "        self.vocabulary_size = reduced_glove.shape[0]\n",
    "        \n",
    "    def add_document_encoder_layer(self):\n",
    "        self.d_tokens = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        self.d_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "        \n",
    "        document_emb = tf.nn.embedding_lookup(self.embedding, self.d_tokens)\n",
    "        document_emb = tf.cast(document_emb, dtype=tf.float64)\n",
    "        \n",
    "        if self.cell_type == 'LSTM':\n",
    "            self.document_forward_cell = tf.contrib.rnn.LSTMCell(self.cell_size)\n",
    "            self.document_backward_cell = tf.contrib.rnn.LSTMCell(self.cell_size)\n",
    "        elif self.cell_type == 'GRU':\n",
    "            self.document_forward_cell = tf.contrib.rnn.GRUCell(self.cell_size)\n",
    "            self.document_backward_cell = tf.contrib.rnn.GRUCell(self.cell_size)\n",
    "        \n",
    "        self.document_encoder_outputs, self.document_encoder_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "            self.document_forward_cell, \n",
    "            self.document_backward_cell, \n",
    "            document_emb, \n",
    "            self.d_lengths, \n",
    "            dtype=tf.float64\n",
    "        )\n",
    "        \n",
    "        self.document_encoder_outputs = tf.concat(self.document_encoder_outputs, 2)\n",
    "        self.document_encoder_outputs = tf.cast(self.document_encoder_outputs,tf.float32)\n",
    "        self.answer_tags = tf.layers.dense(inputs = self.document_encoder_outputs, units=2)\n",
    "\n",
    "\n",
    "        self.a_labels = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        self.answer_mask = tf.sequence_mask(self.d_lengths, dtype=tf.float32)\n",
    "        self.answer_loss = seq2seq.sequence_loss(\n",
    "            logits=self.answer_tags, targets=self.a_labels, weights=self.answer_mask)\n",
    "        \n",
    "        self.answer_loss = tf.Print(self.answer_loss, [self.answer_loss], message=\"This is answer_loss: \")\n",
    "        \n",
    "\n",
    "\n",
    "    def add_answer_encoder_layer(self):\n",
    "        self.answer_encoder_input_mask = tf.placeholder(\n",
    "        tf.float32, shape=[None, None, None])\n",
    "        \n",
    "        self.answer_encoder_inputs = tf.matmul(self.answer_encoder_input_mask, self.document_encoder_outputs)\n",
    "        self.answer_encoder_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "        if self.cell_type == 'GRU':\n",
    "            self.answer_encoder_cell = tf.contrib.rnn.GRUCell(self.document_forward_cell.state_size + self.document_backward_cell.state_size)\n",
    "            self.question_decoder_cell = tf.contrib.rnn.GRUCell(self.answer_encoder_cell.state_size)\n",
    "        \n",
    "        elif self.cell_type == 'LSTM':\n",
    "            self.answer_encoder_cell = tf.contrib.rnn.LSTMCell(self.document_forward_cell.state_size[0] + self.document_backward_cell.state_size[0])\n",
    "            self.question_decoder_cell = tf.contrib.rnn.LSTMCell(self.answer_encoder_cell.state_size[0])\n",
    "            \n",
    "        _, self.answer_encoder_state = tf.nn.dynamic_rnn(\n",
    "            self.answer_encoder_cell, self.answer_encoder_inputs, self.answer_encoder_lengths, dtype=tf.float32)\n",
    "\n",
    "\n",
    "\n",
    "    def add_attention_for_training(self):\n",
    "        if self.attention_type == 'Luong':\n",
    "            self.attention_function = tf.contrib.seq2seq.LuongAttention\n",
    "        else:\n",
    "            self.attention_function = tf.contrib.seq2seq.BahdanauAttention\n",
    "            \n",
    "        if self.cell_type == 'GRU':\n",
    "            attention_mechanism = self.attention_function(\n",
    "                num_units = self.answer_encoder_cell.state_size, \n",
    "                memory = self.document_encoder_outputs,\n",
    "                memory_sequence_length = self.d_lengths)\n",
    "\n",
    "            self.question_decoder_cell_attention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = self.question_decoder_cell, \n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = self.question_decoder_cell.state_size)\n",
    "\n",
    "        elif self.cell_type == 'LSTM':\n",
    "            attention_mechanism = self.attention_function(\n",
    "                num_units = self.answer_encoder_cell.state_size[0], \n",
    "                memory = self.document_encoder_outputs,\n",
    "                memory_sequence_length = self.d_lengths)\n",
    "\n",
    "            self.question_decoder_cell_attention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = self.question_decoder_cell, \n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = self.question_decoder_cell.state_size[0])\n",
    "        \n",
    "        self.question_decoder_cell_attention_without_beam = self.question_decoder_cell_attention\n",
    "\n",
    "\n",
    "    def add_decoder_for_training(self):\n",
    "        self.add_attention_for_training()\n",
    "        self.question_decoder_inputs = tf.placeholder(tf.int32, shape=[None, None])\n",
    "        self.question_decoder_lengths = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "        question_decoder_embedding = tf.nn.embedding_lookup(self.embedding, self.question_decoder_inputs)\n",
    "        question_decoder_embedding = tf.cast(question_decoder_embedding,tf.float32)\n",
    "\n",
    "        helper = seq2seq.TrainingHelper(question_decoder_embedding , self.question_decoder_lengths)\n",
    "\n",
    "        self.projection = Dense(self.vocabulary_size, use_bias=False)\n",
    "        \n",
    "        decoder = seq2seq.BasicDecoder(\n",
    "            self.question_decoder_cell_attention, \n",
    "            helper, \n",
    "            self.question_decoder_cell_attention.zero_state(self.batch_size, dtype=tf.float32).clone(\n",
    "              cell_state=self.answer_encoder_state),\n",
    "            output_layer = self.projection\n",
    "        )\n",
    "        \n",
    "        decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder)\n",
    "        self.training_question_decoder_outputs = decoder_outputs.rnn_output\n",
    "        \n",
    "    def add_decoder_for_inference(self):\n",
    "        helper = seq2seq.GreedyEmbeddingHelper(\n",
    "            self.embedding, \n",
    "            tf.fill([self.batch_size], START_TOKEN), \n",
    "            END_TOKEN\n",
    "        )\n",
    "        decoder = seq2seq.BasicDecoder(\n",
    "            self.question_decoder_cell_attention_without_beam, \n",
    "            helper, \n",
    "            self.question_decoder_cell_attention_without_beam.zero_state(self.batch_size, dtype=tf.float32).clone(\n",
    "                  cell_state=self.answer_encoder_state), output_layer=self.projection)\n",
    "        \n",
    "        decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "        self.decoder_outputs = decoder_outputs.rnn_output\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def add_beam_attention_for_inference(self):\n",
    "        self.document_encoder_output_tiled = tf.contrib.seq2seq.tile_batch(self.document_encoder_outputs, self.beam_width)\n",
    "        self.answer_encoder_state_tiled = tf.contrib.seq2seq.tile_batch(self.answer_encoder_state, self.beam_width)\n",
    "        self.document_lengths_tiled = tf.contrib.seq2seq.tile_batch(self.d_lengths, self.beam_width)\n",
    "\n",
    "        if self.cell_type == 'GRU':\n",
    "            attention_mechanism = self.attention_function(\n",
    "                num_units = self.answer_encoder_cell.state_size, \n",
    "                memory = self.document_encoder_output_tiled,\n",
    "                memory_sequence_length = self.document_lengths_tiled)\n",
    "\n",
    "            self.question_decoder_cell_attention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = self.question_decoder_cell, \n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = self.question_decoder_cell.state_size)\n",
    "            \n",
    "\n",
    "        elif self.cell_type == 'LSTM':\n",
    "            attention_mechanism = self.attention_function(\n",
    "                num_units = self.answer_encoder_cell.state_size[0], \n",
    "                memory = self.document_encoder_output_tiled,\n",
    "                memory_sequence_length = self.document_lengths_tiled)\n",
    "\n",
    "            self.question_decoder_cell_attention = tf.contrib.seq2seq.AttentionWrapper(\n",
    "                cell = self.question_decoder_cell, \n",
    "                attention_mechanism = attention_mechanism,\n",
    "                attention_layer_size = self.question_decoder_cell.state_size[0])\n",
    "\n",
    "\n",
    "\n",
    "    def add_beam_decoder_for_inference(self):\n",
    "        self.add_beam_attention_for_inference()\n",
    "        \n",
    "        decoder = tf.contrib.seq2seq.BeamSearchDecoder(\n",
    "            cell = self.question_decoder_cell_attention,\n",
    "            embedding = self.embedding,\n",
    "            start_tokens = tf.fill([self.batch_size], START_TOKEN),\n",
    "            end_token = END_TOKEN,\n",
    "            initial_state = self.question_decoder_cell_attention.zero_state(self.batch_size * self.beam_width,tf.float32).clone(\n",
    "                cell_state = self.answer_encoder_state_tiled),\n",
    "            beam_width = self.beam_width,\n",
    "            output_layer = self.projection,\n",
    "            length_penalty_weight = 0.0)\n",
    "        \n",
    "        \n",
    "        predicting_decoder_output, _, _ = tf.contrib.seq2seq.dynamic_decode(\n",
    "            decoder = decoder,\n",
    "            maximum_iterations = max_question_len\n",
    "        )\n",
    "        self.predicting_question_ids = predicting_decoder_output.predicted_ids[:, :, :]\n",
    "\n",
    "\n",
    "    def add_backward_path(self):\n",
    "        self.decoder_labels = tf.placeholder(tf.int64, shape=[None, None])\n",
    "        question_mask = tf.sequence_mask(self.question_decoder_lengths ,dtype=tf.float32)\n",
    "        self.question_loss = seq2seq.sequence_loss(\n",
    "            logits = self.training_question_decoder_outputs, \n",
    "            targets = self.decoder_labels,\n",
    "            weights = question_mask)\n",
    "        self.question_loss = tf.Print(self.question_loss, [self.question_loss], message=\"This is question_loss: \")\n",
    "        \n",
    "        self.net_loss = self.question_loss + self.answer_loss\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        gvs = optimizer.compute_gradients(self.net_loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -self.grad_clip, self.grad_clip), var) for grad, var in gvs]\n",
    "        self.train_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "\n",
    "    def fit(self, batch_input_train, batch_input_test,save_after_every_epoch = 5, n_epoch=60, generateTrain = False, generateTest = False, clearFile = False):\n",
    "\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        if clearFile:\n",
    "            self.fileUtils.clearFile()\n",
    "        \n",
    "        for epoch in range(1, n_epoch + 1):\n",
    "            batch_loss = 0\n",
    "            for batchNum in range(len(batch_input_train)):\n",
    "                t = session.run([self.train_op, self.net_loss, self.question_loss, self.answer_loss], {\n",
    "                    self.d_tokens: batch_input_train[batchNum]['document_tokens'],\n",
    "                    self.d_lengths: batch_input_train[batchNum]['document_lengths'],\n",
    "                    self.a_labels: batch_input_train[batchNum]['answer_labels'],\n",
    "                    self.answer_encoder_input_mask: batch_input_train[batchNum]['answer_mask'],\n",
    "                    self.answer_encoder_lengths: batch_input_train[batchNum]['answer_lengths'],\n",
    "                    self.question_decoder_inputs: batch_input_train[batchNum]['question_input_tokens'],\n",
    "                    self.decoder_labels: batch_input_train[batchNum]['question_output_tokens'],\n",
    "                    self.question_decoder_lengths: batch_input_train[batchNum]['question_lengths']\n",
    "                })\n",
    "\n",
    "                print(\"Batch: {0}/{1}, Loss: {2}\".format(batchNum, len(batch_input_train), t[1]), end='\\r')\n",
    "                #self.fileUtils.appendString(\"Batch: {0}/{1}, Loss: {2}\".format(batchNum, len(batch_input_train), t[1]))\n",
    "                batch_loss += t[1]\n",
    "                sys.stdout.flush()\n",
    "            batch_loss /= len(batch_input_train)\n",
    "            print(\"Batch - Loss (Train): {0}\".format(batch_loss))\n",
    "            #self.fileUtils.appendString(\"Batch - Loss (Train): {0} Epoch - {1}\\n\".format(batch_loss, epoch))(\"Batch - Loss: {0} Epoch - {1}\\n\".format(batch_loss, epoch))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            batch_loss = 0\n",
    "            for batchNum in range(len(batch_input_test)):\n",
    "                t = session.run([self.net_loss, self.question_loss, self.answer_loss], {\n",
    "                    self.d_tokens: batch_input_test[batchNum]['document_tokens'],\n",
    "                    self.d_lengths: batch_input_test[batchNum]['document_lengths'],\n",
    "                    self.a_labels: batch_input_test[batchNum]['answer_labels'],\n",
    "                    self.answer_encoder_input_mask: batch_input_test[batchNum]['answer_mask'],\n",
    "                    self.answer_encoder_lengths: batch_input_test[batchNum]['answer_lengths'],\n",
    "                    self.question_decoder_inputs: batch_input_test[batchNum]['question_input_tokens'],\n",
    "                    self.decoder_labels: batch_input_test[batchNum]['question_output_tokens'],\n",
    "                    self.question_decoder_lengths: batch_input_test[batchNum]['question_lengths']\n",
    "                })\n",
    "                batch_loss += t[1]\n",
    "            batch_loss /= len(batch_input_test)\n",
    "            print(\"Batch - Loss (Test): {0}\".format(batch_loss))\n",
    "            #self.fileUtils.appendString(\"Batch - Loss (Test): {0} Epoch - {1}\\n\".format(batch_loss, epoch))\n",
    "            sys.stdout.flush()\n",
    "            if generateTrain:\n",
    "                print(\"Train Samples Generated : \")\n",
    "                self.inferFromBatch(batch_input_train, random.randint(0,len(batch_input_train)-1), 5)\n",
    "                self.inferFromBatchBeam(batch_input_train, random.randint(0,len(batch_input_train)-1), 5, 5)\n",
    "            if generateTest:\n",
    "                self.inferFromBatch(batch_input_test, random.randint(0,len(batch_input_test)-1), 5)\n",
    "                self.inferFromBatchBeam(batch_input_test, random.randint(0,len(batch_input_test)-1), 5, 5)\n",
    "                \n",
    "            if save_after_every_epoch % epoch == 0:\n",
    "                self.saveSession(\"/data/ra2630/tf64k\")\n",
    "        \n",
    "    \n",
    "\n",
    "    def inferFromBatch(self, batch, batchNum, numExamples, shuffle = True, printGroundTruthQuestion = True):\n",
    "        if shuffle:\n",
    "            p = random.sample(range(0, self.batch_size), self.batch_size)\n",
    "        else:\n",
    "            p = range(0,self.batch_size)\n",
    "        j = 0\n",
    "        questions = session.run(self.decoder_outputs, {\n",
    "            self.d_tokens: batch[batchNum]['document_tokens'][p],\n",
    "            self.d_lengths: batch[batchNum]['document_lengths'][p],\n",
    "            self.a_labels: batch[batchNum]['answer_labels'][p],\n",
    "            self.answer_encoder_input_mask: batch[batchNum]['answer_mask'][p],\n",
    "            self.answer_encoder_lengths: batch[batchNum]['answer_lengths'][p],\n",
    "        })\n",
    "        p = p[:numExamples]\n",
    "\n",
    "        qs = np.argmax(questions, 2)\n",
    "        for i in p:\n",
    "            question = itertools.takewhile(lambda t: t != END_TOKEN, qs[j])\n",
    "            print(\"----------------------------------------------------------------------------------------------------\")\n",
    "            self.helperFunctions.printGroundTruthFromBatch(batch, batchNum, i, printGroundTruthQuestion)\n",
    "            print(\"Generated Question :\")\n",
    "            print(\" \".join(look_up_token_reduced(token) for token in question))\n",
    "            print(\"*****************************************************************************************************\")    \n",
    "            print(\"----------------------------------------------------------------------------------------------------\")\n",
    "            j=j+1\n",
    "    \n",
    "    def inferFromBatchBeam(self, batch, batchNum, numExamples, numQuestions, shuffle = True, printGroundTruthQuestion = True):\n",
    "        \n",
    "        if shuffle:\n",
    "            p = random.sample(range(0, self.batch_size), self.batch_size)\n",
    "        else:\n",
    "            p = range(0,self.batch_size)\n",
    "        j = 0\n",
    "        questions = session.run(self.predicting_question_ids, {\n",
    "            self.d_tokens: batch[batchNum]['document_tokens'][p],\n",
    "            self.d_lengths: batch[batchNum]['document_lengths'][p],\n",
    "            self.a_labels: batch[batchNum]['answer_labels'][p],\n",
    "            self.answer_encoder_input_mask: batch[batchNum]['answer_mask'][p],\n",
    "            self.answer_encoder_lengths: batch[batchNum]['answer_lengths'][p],\n",
    "        })\n",
    "        p = p[:numExamples]\n",
    "\n",
    "        for i in p:\n",
    "            question = itertools.takewhile(lambda t: t != END_TOKEN, questions[j,:,0])\n",
    "            print(\"----------------------------------------------------------------------------------------------------\")\n",
    "            self.helperFunctions.printGroundTruthFromBatch(batch, batchNum, i, printGroundTruthQuestion)\n",
    "            for k in range(numQuestions):\n",
    "                print(\"Generated Question Number : \", (k+1))\n",
    "                print(\" \".join(look_up_token_reduced(token) for token in question))\n",
    "            print(\"*****************************************************************************************************\")    \n",
    "            print(\"----------------------------------------------------------------------------------------------------\")\n",
    "            j=j+1\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "#config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "qgen = QGen(learning_rate = 3e-3,\n",
    "            cell_size = 500, \n",
    "            n_layers = 1, \n",
    "            reduced_glove = reduced_glove,\n",
    "            batch_size = batch_size,\n",
    "            attention_type = 'Bahadanau', \n",
    "            cell_type = 'GRU', \n",
    "            sess=session, \n",
    "            grad_clip=1.0, \n",
    "            beam_width=10, \n",
    "            force_teaching_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sys.stdout = open('/data/ra2630/tfLog64k', 'w', 1)\n",
    "qgen.fit(\n",
    "    batch_input_train, \n",
    "    batch_input_test, \n",
    "    n_epoch=30, \n",
    "    save_after_every_epoch = 5, \n",
    "    generateTrain = True, \n",
    "    generateTest = True,\n",
    "    clearFile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qgen.loadFromSession(\"/data/ra2630/qgenTemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qgen.inferFromBatch(batch_input_train, 2, 1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qgen.inferFromBatchBeam(batch_input_train, 2, 1, 5, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inferenceUtils = InferenceUtils(batch_size=batch_size, qgen=qgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inferenceUtils.makeInferenceOnText(\n",
    "    passage = \"LG Electronics has four business units: Home Entertainment, Mobile Communications, Home Appliances & Air Solutions, and Vehicle Components.\", \n",
    "    answer = \"Mobile Communications\",\n",
    "    batch_size = batch_size, \n",
    "    use_beam = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
