{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_with_answer_marked_all = []\n",
    "X_train_ans_all = []\n",
    "X_train_comp_answer_label_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            invalid=invalid+1\n",
    "            continue\n",
    "        a_lab[start:end] = 1\n",
    "        \n",
    "              \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            start,end = find_sub_list(answer,passage)\n",
    "            if start == -1:\n",
    "                invalid = invalid+1\n",
    "                continue\n",
    "            marked_comp = np.zeros(len(passage))\n",
    "            marked_comp[start:end] = 1\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_comp_with_answer_marked_all.append(marked_comp)\n",
    "            X_train_ans_all.append(answer)\n",
    "            X_train_comp_answer_label_all.append(a_lab)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X_train_comp_all,X_train_ans_all, Y_train_ques_all,X_train_comp_with_answer_marked_all, X_train_comp_answer_label_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_ans_all_shuffled, Y_train_ques_all_shuffled, X_train_comp_with_answer_marked_all_shuffled, X_train_comp_answer_label_shuffled = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all_shuffled[0:400] for item in sublist] + [item for sublist in Y_train_ques_all_shuffled[0:400] for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 4000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "4858\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 320\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_all_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_with_answer_marked = X_train_comp_with_answer_marked_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_answer_label = X_train_comp_answer_label_shuffled[0:examples_to_take_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.]), 344)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comp_with_answer_marked[0], max_document_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.full((examples_to_take_train, max_document_len), END_TOKEN,dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "answer_labels_all = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(examples_to_take_train):\n",
    "    answer_labels_all[i,0:len(X_train_comp_answer_label[i])] = X_train_comp_answer_label[i]\n",
    "    answer_labels[i, 0:len(X_train_comp_with_answer_marked[i])] = X_train_comp_with_answer_marked[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    answer_lengths[i] = len(X_train_ans[i])\n",
    "    \n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 300)\n",
      "8700\n",
      "565\n",
      "8506\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = context_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "344"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    start = 0\n",
    "    while start < len(inputs[0]):\n",
    "        end = min(len(inputs[0]), start + batch_size)\n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels_all':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[]}\n",
    "        \n",
    "        for index,inp in enumerate(inputs):\n",
    "            #maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max_question_len\n",
    "            \n",
    "            if index == 0:\n",
    "                output['document_tokens'].append(inp[start:end,0:maxD])\n",
    "            elif index==1:\n",
    "                output['document_lengths'].append(inp[start:end])\n",
    "            elif index==2:\n",
    "                output['answer_labels_all'].append(inp[start:end,0:maxD])\n",
    "            elif index==3:\n",
    "                output['answer_labels'].append(inp[start:end,0:maxD])\n",
    "            elif index==4:\n",
    "                output['answer_lengths'].append(inp[start:end])\n",
    "            elif index==5:\n",
    "                output['question_input_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==6:\n",
    "                output['question_output_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==7:\n",
    "                output['question_lengths'].append(inp[start:end])\n",
    "        \n",
    "        output[\"document_tokens\"] = np.array(output[\"document_tokens\"])\n",
    "        output[\"document_lengths\"] = np.array(output[\"document_lengths\"])\n",
    "        output[\"answer_labels_all\"] = np.array(output[\"answer_labels_all\"])\n",
    "        output[\"answer_labels\"] = np.array(output[\"answer_labels\"])\n",
    "        output[\"answer_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        output[\"question_input_tokens\"] = np.array(output[\"question_input_tokens\"])\n",
    "        output[\"question_output_tokens\"] = np.array(output[\"question_output_tokens\"])\n",
    "        output[\"question_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        outputs.append(output)\n",
    "        start = start + batch_size\n",
    "            \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches =  10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels_all,answer_labels,answer_lengths,question_input_tokens,question_output_tokens,question_lengths]\n",
    "                    ,batch_size)\n",
    "for b in batch_input:\n",
    "    for k, v in b.items():\n",
    "        b[k] = v.squeeze()\n",
    "number_of_batches = len(batch_input)\n",
    "print(\"Number of batches = \", number_of_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input_test = batch_input[7:]\n",
    "batch_input = batch_input[0:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(AnswerEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True, bidirectional=True) #Input_size = Hidden_Size\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        self.hiddenState = hidden\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(2, batch_size, self.hidden_size)) #2 for BiDirectional\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    \n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, max_document_len, dropout_p=0.1):\n",
    "        \n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_document_len = max_document_len\n",
    "\n",
    "        self.attn_reduce = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.max_document_len + self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.attn_reduce = self.attn_reduce.cuda()\n",
    "            self.attn_combine = self.attn_combine.cuda()\n",
    "            self.dropout = self.dropout.cuda()\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.dropout(hidden)\n",
    "        concat = torch.cat((input.unsqueeze(0), hidden.unsqueeze(0)), 1)\n",
    "        concat_reduced = self.attn_reduce(concat)\n",
    "        attn_weights = F.softmax(concat_reduced, dim=1)\n",
    "        attn_applied = torch.mm(encoder_outputs, attn_weights.squeeze(0).unsqueeze(1))\n",
    "        output = torch.cat((hidden.unsqueeze(0), attn_applied.squeeze(1).unsqueeze(0)), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        self.hidden_state = hidden\n",
    "        return output, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters =  30\n"
     ]
    }
   ],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "fcLayer = FCLayer(max_document_len+1, 1)\n",
    "answerEncoder = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder1 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionEncoder2 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention = AttnDecoderRNN(hidden_size= hidden_size,max_document_len = max_document_len)\n",
    "\n",
    "answerEncoder.train()\n",
    "questionEncoder1.train()\n",
    "questionEncoder2.train()\n",
    "questionDecoder.train()\n",
    "questionGenerator.train()\n",
    "attention.train()\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [answerEncoder, questionEncoder1, questionEncoder2, questionDecoder, questionGenerator, attention, fcLayer]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 0.01)\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "#criterion2 = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 1\tNet Loss: 239.0333 \tAnswer Loss: 0.2605 \tQuestion Loss: 238.7728 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 1\tNet Loss: 143.0057 \tAnswer Loss: 0.1259 \tQuestion Loss: 142.8798 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 1\tNet Loss: 141.9254 \tAnswer Loss: 0.1393 \tQuestion Loss: 141.7861 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 1\tNet Loss: 149.3813 \tAnswer Loss: 0.1663 \tQuestion Loss: 149.2150 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 1\tNet Loss: 115.0185 \tAnswer Loss: 0.1973 \tQuestion Loss: 114.8212 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 1\tNet Loss: 112.2680 \tAnswer Loss: 0.1371 \tQuestion Loss: 112.1308 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 1\tNet Loss: 95.5194 \tAnswer Loss: 0.1225 \tQuestion Loss: 95.3970 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 1 : 142.3074 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 2\tNet Loss: 101.5938 \tAnswer Loss: 0.1219 \tQuestion Loss: 101.4718 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 2\tNet Loss: 100.0057 \tAnswer Loss: 0.1157 \tQuestion Loss: 99.8900 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 2\tNet Loss: 92.1769 \tAnswer Loss: 0.1338 \tQuestion Loss: 92.0431 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 2\tNet Loss: 87.7096 \tAnswer Loss: 0.1285 \tQuestion Loss: 87.5811 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 2\tNet Loss: 85.4375 \tAnswer Loss: 0.1802 \tQuestion Loss: 85.2574 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 2\tNet Loss: 92.7748 \tAnswer Loss: 0.1305 \tQuestion Loss: 92.6443 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 2\tNet Loss: 83.7969 \tAnswer Loss: 0.1224 \tQuestion Loss: 83.6745 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 2 : 91.9279 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 3\tNet Loss: 97.3037 \tAnswer Loss: 0.1221 \tQuestion Loss: 97.1816 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 3\tNet Loss: 83.8009 \tAnswer Loss: 0.1095 \tQuestion Loss: 83.6914 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 3\tNet Loss: 78.1605 \tAnswer Loss: 0.1414 \tQuestion Loss: 78.0191 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 3\tNet Loss: 76.8836 \tAnswer Loss: 0.1278 \tQuestion Loss: 76.7558 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 3\tNet Loss: 74.8125 \tAnswer Loss: 0.1864 \tQuestion Loss: 74.6261 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 3\tNet Loss: 77.4484 \tAnswer Loss: 0.1278 \tQuestion Loss: 77.3206 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 3\tNet Loss: 73.1551 \tAnswer Loss: 0.1277 \tQuestion Loss: 73.0275 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 3 : 80.2235 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 4\tNet Loss: 75.8796 \tAnswer Loss: 0.1297 \tQuestion Loss: 75.7498 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 4\tNet Loss: 70.3799 \tAnswer Loss: 0.1080 \tQuestion Loss: 70.2718 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 4\tNet Loss: 69.5011 \tAnswer Loss: 0.1402 \tQuestion Loss: 69.3609 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 4\tNet Loss: 68.6868 \tAnswer Loss: 0.1326 \tQuestion Loss: 68.5542 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 4\tNet Loss: 63.3856 \tAnswer Loss: 0.1946 \tQuestion Loss: 63.1911 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 4\tNet Loss: 69.7077 \tAnswer Loss: 0.1283 \tQuestion Loss: 69.5795 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 4\tNet Loss: 62.8751 \tAnswer Loss: 0.1170 \tQuestion Loss: 62.7581 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 4 : 68.6308 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 5\tNet Loss: 68.2811 \tAnswer Loss: 0.1304 \tQuestion Loss: 68.1507 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 5\tNet Loss: 60.8978 \tAnswer Loss: 0.1166 \tQuestion Loss: 60.7812 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 5\tNet Loss: 68.9816 \tAnswer Loss: 0.1260 \tQuestion Loss: 68.8556 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 5\tNet Loss: 59.9804 \tAnswer Loss: 0.1223 \tQuestion Loss: 59.8581 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 5\tNet Loss: 56.7790 \tAnswer Loss: 0.1847 \tQuestion Loss: 56.5942 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 5\tNet Loss: 60.2916 \tAnswer Loss: 0.1284 \tQuestion Loss: 60.1632 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 5\tNet Loss: 59.7152 \tAnswer Loss: 0.1155 \tQuestion Loss: 59.5996 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 5 : 62.1324 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 6\tNet Loss: 65.1882 \tAnswer Loss: 0.1200 \tQuestion Loss: 65.0682 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 6\tNet Loss: 66.0629 \tAnswer Loss: 0.1116 \tQuestion Loss: 65.9513 \t Time Taken: 5 seconds\n",
      "Batch: 2 \t Epoch : 6\tNet Loss: 57.0734 \tAnswer Loss: 0.1227 \tQuestion Loss: 56.9507 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 6\tNet Loss: 56.6441 \tAnswer Loss: 0.1193 \tQuestion Loss: 56.5248 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 6\tNet Loss: 58.4323 \tAnswer Loss: 0.1728 \tQuestion Loss: 58.2595 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 6\tNet Loss: 54.4298 \tAnswer Loss: 0.1245 \tQuestion Loss: 54.3054 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 6\tNet Loss: 46.8236 \tAnswer Loss: 0.1142 \tQuestion Loss: 46.7094 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 6 : 57.8078 Time Taken: 31 seconds\n",
      "Batch: 0 \t Epoch : 7\tNet Loss: 57.2788 \tAnswer Loss: 0.1176 \tQuestion Loss: 57.1613 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 7\tNet Loss: 50.8245 \tAnswer Loss: 0.1071 \tQuestion Loss: 50.7174 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 7\tNet Loss: 50.2056 \tAnswer Loss: 0.1210 \tQuestion Loss: 50.0846 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 7\tNet Loss: 43.7475 \tAnswer Loss: 0.1168 \tQuestion Loss: 43.6307 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 7\tNet Loss: 42.6123 \tAnswer Loss: 0.1642 \tQuestion Loss: 42.4481 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 7\tNet Loss: 49.4841 \tAnswer Loss: 0.1200 \tQuestion Loss: 49.3641 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 7\tNet Loss: 41.1403 \tAnswer Loss: 0.1145 \tQuestion Loss: 41.0258 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 7 : 47.8990 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 8\tNet Loss: 47.7588 \tAnswer Loss: 0.1182 \tQuestion Loss: 47.6406 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 8\tNet Loss: 48.3099 \tAnswer Loss: 0.1040 \tQuestion Loss: 48.2059 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 8\tNet Loss: 50.4515 \tAnswer Loss: 0.1186 \tQuestion Loss: 50.3329 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 8\tNet Loss: 42.1623 \tAnswer Loss: 0.1169 \tQuestion Loss: 42.0454 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 8\tNet Loss: 42.9730 \tAnswer Loss: 0.1580 \tQuestion Loss: 42.8150 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 8\tNet Loss: 46.8029 \tAnswer Loss: 0.1160 \tQuestion Loss: 46.6868 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 8\tNet Loss: 44.9227 \tAnswer Loss: 0.1117 \tQuestion Loss: 44.8110 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 8 : 46.1973 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 9\tNet Loss: 58.1703 \tAnswer Loss: 0.1168 \tQuestion Loss: 58.0534 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 9\tNet Loss: 42.1596 \tAnswer Loss: 0.1032 \tQuestion Loss: 42.0565 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 9\tNet Loss: 42.7365 \tAnswer Loss: 0.1123 \tQuestion Loss: 42.6243 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 9\tNet Loss: 54.1523 \tAnswer Loss: 0.1128 \tQuestion Loss: 54.0395 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 9\tNet Loss: 51.8933 \tAnswer Loss: 0.1542 \tQuestion Loss: 51.7391 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 9\tNet Loss: 51.1229 \tAnswer Loss: 0.1174 \tQuestion Loss: 51.0054 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 9\tNet Loss: 46.7739 \tAnswer Loss: 0.1050 \tQuestion Loss: 46.6689 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 9 : 49.5727 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 10\tNet Loss: 53.8902 \tAnswer Loss: 0.1083 \tQuestion Loss: 53.7819 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 10\tNet Loss: 56.9052 \tAnswer Loss: 0.1059 \tQuestion Loss: 56.7993 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 10\tNet Loss: 43.9817 \tAnswer Loss: 0.1146 \tQuestion Loss: 43.8671 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 10\tNet Loss: 39.8107 \tAnswer Loss: 0.1141 \tQuestion Loss: 39.6966 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 10\tNet Loss: 46.8655 \tAnswer Loss: 0.1478 \tQuestion Loss: 46.7177 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 10\tNet Loss: 54.3098 \tAnswer Loss: 0.1244 \tQuestion Loss: 54.1853 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 10\tNet Loss: 44.8490 \tAnswer Loss: 0.1164 \tQuestion Loss: 44.7326 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 10 : 48.6589 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 11\tNet Loss: 46.0527 \tAnswer Loss: 0.1092 \tQuestion Loss: 45.9435 \t Time Taken: 4 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 11\tNet Loss: 43.2659 \tAnswer Loss: 0.1043 \tQuestion Loss: 43.1616 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 11\tNet Loss: 43.1205 \tAnswer Loss: 0.1147 \tQuestion Loss: 43.0058 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 11\tNet Loss: 40.4991 \tAnswer Loss: 0.1170 \tQuestion Loss: 40.3821 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 11\tNet Loss: 39.3364 \tAnswer Loss: 0.1460 \tQuestion Loss: 39.1904 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 11\tNet Loss: 44.0989 \tAnswer Loss: 0.1174 \tQuestion Loss: 43.9814 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 11\tNet Loss: 40.2468 \tAnswer Loss: 0.1111 \tQuestion Loss: 40.1357 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 11 : 42.3743 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 12\tNet Loss: 47.2956 \tAnswer Loss: 0.1075 \tQuestion Loss: 47.1881 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 12\tNet Loss: 40.2735 \tAnswer Loss: 0.1004 \tQuestion Loss: 40.1731 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 12\tNet Loss: 36.0073 \tAnswer Loss: 0.1086 \tQuestion Loss: 35.8988 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 12\tNet Loss: 36.4412 \tAnswer Loss: 0.1111 \tQuestion Loss: 36.3300 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 12\tNet Loss: 35.8179 \tAnswer Loss: 0.1444 \tQuestion Loss: 35.6734 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 12\tNet Loss: 40.3863 \tAnswer Loss: 0.1107 \tQuestion Loss: 40.2755 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 12\tNet Loss: 33.7986 \tAnswer Loss: 0.1056 \tQuestion Loss: 33.6930 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 12 : 38.5743 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 13\tNet Loss: 41.1490 \tAnswer Loss: 0.1053 \tQuestion Loss: 41.0437 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 13\tNet Loss: 36.7241 \tAnswer Loss: 0.0978 \tQuestion Loss: 36.6263 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 13\tNet Loss: 31.8342 \tAnswer Loss: 0.1041 \tQuestion Loss: 31.7301 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 13\tNet Loss: 33.6853 \tAnswer Loss: 0.1070 \tQuestion Loss: 33.5783 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 13\tNet Loss: 33.4990 \tAnswer Loss: 0.1413 \tQuestion Loss: 33.3577 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 13\tNet Loss: 35.6911 \tAnswer Loss: 0.1048 \tQuestion Loss: 35.5863 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 13\tNet Loss: 30.8242 \tAnswer Loss: 0.1017 \tQuestion Loss: 30.7225 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 13 : 34.7724 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 14\tNet Loss: 38.1908 \tAnswer Loss: 0.1033 \tQuestion Loss: 38.0874 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 14\tNet Loss: 36.7504 \tAnswer Loss: 0.0953 \tQuestion Loss: 36.6551 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 14\tNet Loss: 34.0559 \tAnswer Loss: 0.0999 \tQuestion Loss: 33.9560 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 14\tNet Loss: 30.8123 \tAnswer Loss: 0.1042 \tQuestion Loss: 30.7081 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 14\tNet Loss: 30.1431 \tAnswer Loss: 0.1379 \tQuestion Loss: 30.0052 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 14\tNet Loss: 40.8278 \tAnswer Loss: 0.0985 \tQuestion Loss: 40.7293 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 14\tNet Loss: 33.0119 \tAnswer Loss: 0.0939 \tQuestion Loss: 32.9179 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 14 : 34.8274 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 15\tNet Loss: 35.6878 \tAnswer Loss: 0.0993 \tQuestion Loss: 35.5885 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 15\tNet Loss: 41.2705 \tAnswer Loss: 0.0946 \tQuestion Loss: 41.1759 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 15\tNet Loss: 32.7735 \tAnswer Loss: 0.0986 \tQuestion Loss: 32.6749 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 15\tNet Loss: 34.3581 \tAnswer Loss: 0.0984 \tQuestion Loss: 34.2597 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 15\tNet Loss: 42.5452 \tAnswer Loss: 0.1382 \tQuestion Loss: 42.4070 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 15\tNet Loss: 42.2939 \tAnswer Loss: 0.0931 \tQuestion Loss: 42.2008 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 15\tNet Loss: 30.3606 \tAnswer Loss: 0.0898 \tQuestion Loss: 30.2708 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 15 : 37.0414 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 16\tNet Loss: 44.6011 \tAnswer Loss: 0.0979 \tQuestion Loss: 44.5032 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 16\tNet Loss: 37.7056 \tAnswer Loss: 0.0945 \tQuestion Loss: 37.6110 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 16\tNet Loss: 37.9245 \tAnswer Loss: 0.0988 \tQuestion Loss: 37.8256 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 16\tNet Loss: 40.7764 \tAnswer Loss: 0.0968 \tQuestion Loss: 40.6796 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 16\tNet Loss: 37.3778 \tAnswer Loss: 0.1275 \tQuestion Loss: 37.2503 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 16\tNet Loss: 41.9769 \tAnswer Loss: 0.1058 \tQuestion Loss: 41.8711 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 16\tNet Loss: 32.2277 \tAnswer Loss: 0.0968 \tQuestion Loss: 32.1309 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 16 : 38.9414 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 17\tNet Loss: 45.7119 \tAnswer Loss: 0.0941 \tQuestion Loss: 45.6179 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 17\tNet Loss: 46.0199 \tAnswer Loss: 0.0974 \tQuestion Loss: 45.9225 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 17\tNet Loss: 35.3313 \tAnswer Loss: 0.1160 \tQuestion Loss: 35.2154 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 17\tNet Loss: 31.0717 \tAnswer Loss: 0.1104 \tQuestion Loss: 30.9613 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 17\tNet Loss: 34.6443 \tAnswer Loss: 0.1315 \tQuestion Loss: 34.5128 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 17\tNet Loss: 45.6408 \tAnswer Loss: 0.0897 \tQuestion Loss: 45.5511 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 17\tNet Loss: 38.8812 \tAnswer Loss: 0.1329 \tQuestion Loss: 38.7483 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 17 : 39.6144 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 18\tNet Loss: 38.8937 \tAnswer Loss: 0.1294 \tQuestion Loss: 38.7643 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 18\tNet Loss: 34.8800 \tAnswer Loss: 0.0858 \tQuestion Loss: 34.7942 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 18\tNet Loss: 37.4376 \tAnswer Loss: 0.1120 \tQuestion Loss: 37.3256 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 18\tNet Loss: 37.1417 \tAnswer Loss: 0.1280 \tQuestion Loss: 37.0137 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 18\tNet Loss: 36.6870 \tAnswer Loss: 0.1840 \tQuestion Loss: 36.5030 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 18\tNet Loss: 41.8145 \tAnswer Loss: 0.1192 \tQuestion Loss: 41.6953 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 18\tNet Loss: 35.6251 \tAnswer Loss: 0.0913 \tQuestion Loss: 35.5338 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 18 : 37.4971 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 19\tNet Loss: 37.7710 \tAnswer Loss: 0.1154 \tQuestion Loss: 37.6556 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 19\tNet Loss: 36.7044 \tAnswer Loss: 0.1300 \tQuestion Loss: 36.5744 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 19\tNet Loss: 39.6510 \tAnswer Loss: 0.0979 \tQuestion Loss: 39.5531 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 19\tNet Loss: 32.1257 \tAnswer Loss: 0.0910 \tQuestion Loss: 32.0347 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 19\tNet Loss: 33.4032 \tAnswer Loss: 0.1260 \tQuestion Loss: 33.2772 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 19\tNet Loss: 43.5608 \tAnswer Loss: 0.1220 \tQuestion Loss: 43.4388 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 19\tNet Loss: 36.8430 \tAnswer Loss: 0.1105 \tQuestion Loss: 36.7325 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 19 : 37.1513 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 20\tNet Loss: 36.4485 \tAnswer Loss: 0.1068 \tQuestion Loss: 36.3417 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 20\tNet Loss: 37.2941 \tAnswer Loss: 0.0834 \tQuestion Loss: 37.2107 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 20\tNet Loss: 34.1046 \tAnswer Loss: 0.0812 \tQuestion Loss: 34.0233 \t Time Taken: 5 seconds\n",
      "Batch: 3 \t Epoch : 20\tNet Loss: 32.2756 \tAnswer Loss: 0.0991 \tQuestion Loss: 32.1765 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 20\tNet Loss: 29.2799 \tAnswer Loss: 0.1303 \tQuestion Loss: 29.1496 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 20\tNet Loss: 34.6561 \tAnswer Loss: 0.0803 \tQuestion Loss: 34.5758 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 20\tNet Loss: 30.9071 \tAnswer Loss: 0.0799 \tQuestion Loss: 30.8272 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 20 : 33.5665 Time Taken: 32 seconds\n",
      "Batch: 0 \t Epoch : 21\tNet Loss: 34.0471 \tAnswer Loss: 0.0880 \tQuestion Loss: 33.9592 \t Time Taken: 4 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 21\tNet Loss: 31.8616 \tAnswer Loss: 0.0813 \tQuestion Loss: 31.7803 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 21\tNet Loss: 28.9569 \tAnswer Loss: 0.0843 \tQuestion Loss: 28.8726 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 21\tNet Loss: 26.5270 \tAnswer Loss: 0.0875 \tQuestion Loss: 26.4396 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 21\tNet Loss: 26.0640 \tAnswer Loss: 0.1214 \tQuestion Loss: 25.9427 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 21\tNet Loss: 31.6695 \tAnswer Loss: 0.0755 \tQuestion Loss: 31.5941 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 21\tNet Loss: 26.2904 \tAnswer Loss: 0.0786 \tQuestion Loss: 26.2118 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 21 : 29.3452 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 22\tNet Loss: 27.3374 \tAnswer Loss: 0.0871 \tQuestion Loss: 27.2503 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 22\tNet Loss: 26.2150 \tAnswer Loss: 0.0829 \tQuestion Loss: 26.1322 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 22\tNet Loss: 24.2686 \tAnswer Loss: 0.0740 \tQuestion Loss: 24.1945 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 22\tNet Loss: 24.9385 \tAnswer Loss: 0.0810 \tQuestion Loss: 24.8576 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 22\tNet Loss: 24.5252 \tAnswer Loss: 0.1003 \tQuestion Loss: 24.4249 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 22\tNet Loss: 30.7743 \tAnswer Loss: 0.0731 \tQuestion Loss: 30.7011 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 22\tNet Loss: 27.7227 \tAnswer Loss: 0.0684 \tQuestion Loss: 27.6543 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 22 : 26.5403 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 23\tNet Loss: 25.0755 \tAnswer Loss: 0.0767 \tQuestion Loss: 24.9989 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 23\tNet Loss: 23.5426 \tAnswer Loss: 0.0761 \tQuestion Loss: 23.4665 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 23\tNet Loss: 23.1024 \tAnswer Loss: 0.0699 \tQuestion Loss: 23.0324 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 23\tNet Loss: 21.0165 \tAnswer Loss: 0.0701 \tQuestion Loss: 20.9465 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 23\tNet Loss: 20.3132 \tAnswer Loss: 0.0868 \tQuestion Loss: 20.2264 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 23\tNet Loss: 26.8240 \tAnswer Loss: 0.0704 \tQuestion Loss: 26.7536 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 23\tNet Loss: 20.7863 \tAnswer Loss: 0.0665 \tQuestion Loss: 20.7197 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 23 : 22.9515 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 24\tNet Loss: 22.8728 \tAnswer Loss: 0.0702 \tQuestion Loss: 22.8027 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 24\tNet Loss: 21.2878 \tAnswer Loss: 0.0660 \tQuestion Loss: 21.2218 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 24\tNet Loss: 20.9694 \tAnswer Loss: 0.0625 \tQuestion Loss: 20.9069 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 24\tNet Loss: 18.1771 \tAnswer Loss: 0.0702 \tQuestion Loss: 18.1069 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 24\tNet Loss: 17.1669 \tAnswer Loss: 0.0780 \tQuestion Loss: 17.0889 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 24\tNet Loss: 20.8882 \tAnswer Loss: 0.0610 \tQuestion Loss: 20.8272 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 24\tNet Loss: 19.1951 \tAnswer Loss: 0.0622 \tQuestion Loss: 19.1329 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 24 : 20.0796 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 25\tNet Loss: 22.3853 \tAnswer Loss: 0.0713 \tQuestion Loss: 22.3139 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 25\tNet Loss: 19.5316 \tAnswer Loss: 0.0664 \tQuestion Loss: 19.4652 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 25\tNet Loss: 19.4288 \tAnswer Loss: 0.0567 \tQuestion Loss: 19.3721 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 25\tNet Loss: 16.9467 \tAnswer Loss: 0.0580 \tQuestion Loss: 16.8887 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 25\tNet Loss: 17.0773 \tAnswer Loss: 0.0763 \tQuestion Loss: 17.0010 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 25\tNet Loss: 21.7530 \tAnswer Loss: 0.0716 \tQuestion Loss: 21.6814 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 25\tNet Loss: 16.2855 \tAnswer Loss: 0.0540 \tQuestion Loss: 16.2315 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 25 : 19.0583 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 26\tNet Loss: 19.2867 \tAnswer Loss: 0.0577 \tQuestion Loss: 19.2290 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 26\tNet Loss: 22.1631 \tAnswer Loss: 0.0623 \tQuestion Loss: 22.1008 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 26\tNet Loss: 19.1544 \tAnswer Loss: 0.0647 \tQuestion Loss: 19.0897 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 26\tNet Loss: 18.1056 \tAnswer Loss: 0.0652 \tQuestion Loss: 18.0404 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 26\tNet Loss: 17.6416 \tAnswer Loss: 0.0729 \tQuestion Loss: 17.5688 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 26\tNet Loss: 20.4424 \tAnswer Loss: 0.0549 \tQuestion Loss: 20.3874 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 26\tNet Loss: 17.0087 \tAnswer Loss: 0.0615 \tQuestion Loss: 16.9472 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 26 : 19.1146 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 27\tNet Loss: 19.2785 \tAnswer Loss: 0.0897 \tQuestion Loss: 19.1888 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 27\tNet Loss: 22.4688 \tAnswer Loss: 0.0649 \tQuestion Loss: 22.4039 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 27\tNet Loss: 28.7715 \tAnswer Loss: 0.0488 \tQuestion Loss: 28.7227 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 27\tNet Loss: 28.8435 \tAnswer Loss: 0.0607 \tQuestion Loss: 28.7828 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 27\tNet Loss: 22.6528 \tAnswer Loss: 0.0887 \tQuestion Loss: 22.5641 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 27\tNet Loss: 29.8976 \tAnswer Loss: 0.0805 \tQuestion Loss: 29.8171 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 27\tNet Loss: 21.2938 \tAnswer Loss: 0.0613 \tQuestion Loss: 21.2325 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 27 : 24.7438 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 28\tNet Loss: 28.1695 \tAnswer Loss: 0.0572 \tQuestion Loss: 28.1122 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 28\tNet Loss: 30.5849 \tAnswer Loss: 0.0578 \tQuestion Loss: 30.5271 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 28\tNet Loss: 26.4280 \tAnswer Loss: 0.0755 \tQuestion Loss: 26.3525 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 28\tNet Loss: 28.4736 \tAnswer Loss: 0.0661 \tQuestion Loss: 28.4075 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 28\tNet Loss: 24.3708 \tAnswer Loss: 0.0682 \tQuestion Loss: 24.3026 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 28\tNet Loss: 27.7471 \tAnswer Loss: 0.0552 \tQuestion Loss: 27.6919 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 28\tNet Loss: 24.4028 \tAnswer Loss: 0.0536 \tQuestion Loss: 24.3492 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 28 : 27.1681 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 29\tNet Loss: 30.9195 \tAnswer Loss: 0.0687 \tQuestion Loss: 30.8508 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 29\tNet Loss: 28.3659 \tAnswer Loss: 0.0630 \tQuestion Loss: 28.3029 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 29\tNet Loss: 22.0953 \tAnswer Loss: 0.0539 \tQuestion Loss: 22.0415 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 29\tNet Loss: 20.4412 \tAnswer Loss: 0.0494 \tQuestion Loss: 20.3918 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 29\tNet Loss: 19.7069 \tAnswer Loss: 0.0588 \tQuestion Loss: 19.6481 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 29\tNet Loss: 26.9700 \tAnswer Loss: 0.0651 \tQuestion Loss: 26.9049 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 29\tNet Loss: 18.0439 \tAnswer Loss: 0.0561 \tQuestion Loss: 17.9878 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 29 : 23.7918 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 30\tNet Loss: 21.7909 \tAnswer Loss: 0.0542 \tQuestion Loss: 21.7366 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 30\tNet Loss: 26.4418 \tAnswer Loss: 0.0495 \tQuestion Loss: 26.3922 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 30\tNet Loss: 20.9244 \tAnswer Loss: 0.0483 \tQuestion Loss: 20.8761 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 30\tNet Loss: 18.5856 \tAnswer Loss: 0.0531 \tQuestion Loss: 18.5325 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 30\tNet Loss: 17.9485 \tAnswer Loss: 0.0663 \tQuestion Loss: 17.8822 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 30\tNet Loss: 21.7105 \tAnswer Loss: 0.0562 \tQuestion Loss: 21.6543 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 30\tNet Loss: 14.5744 \tAnswer Loss: 0.0470 \tQuestion Loss: 14.5274 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 30 : 20.2823 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 31\tNet Loss: 16.8624 \tAnswer Loss: 0.0503 \tQuestion Loss: 16.8121 \t Time Taken: 4 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 31\tNet Loss: 21.7127 \tAnswer Loss: 0.0540 \tQuestion Loss: 21.6588 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 31\tNet Loss: 19.7194 \tAnswer Loss: 0.0479 \tQuestion Loss: 19.6715 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 31\tNet Loss: 16.6439 \tAnswer Loss: 0.0512 \tQuestion Loss: 16.5926 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 31\tNet Loss: 13.7047 \tAnswer Loss: 0.0560 \tQuestion Loss: 13.6487 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 31\tNet Loss: 17.0293 \tAnswer Loss: 0.0501 \tQuestion Loss: 16.9792 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 31\tNet Loss: 12.6424 \tAnswer Loss: 0.0433 \tQuestion Loss: 12.5991 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 31 : 16.9021 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 32\tNet Loss: 16.3518 \tAnswer Loss: 0.0530 \tQuestion Loss: 16.2987 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 32\tNet Loss: 19.0603 \tAnswer Loss: 0.0540 \tQuestion Loss: 19.0063 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 32\tNet Loss: 15.4563 \tAnswer Loss: 0.0421 \tQuestion Loss: 15.4142 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 32\tNet Loss: 13.8965 \tAnswer Loss: 0.0429 \tQuestion Loss: 13.8535 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 32\tNet Loss: 12.9081 \tAnswer Loss: 0.0525 \tQuestion Loss: 12.8557 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 32\tNet Loss: 15.5552 \tAnswer Loss: 0.0470 \tQuestion Loss: 15.5082 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 32\tNet Loss: 12.0861 \tAnswer Loss: 0.0390 \tQuestion Loss: 12.0471 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 32 : 15.0449 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 33\tNet Loss: 14.4087 \tAnswer Loss: 0.0431 \tQuestion Loss: 14.3656 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 33\tNet Loss: 17.2352 \tAnswer Loss: 0.0451 \tQuestion Loss: 17.1900 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 33\tNet Loss: 11.3809 \tAnswer Loss: 0.0384 \tQuestion Loss: 11.3425 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 33\tNet Loss: 11.9504 \tAnswer Loss: 0.0455 \tQuestion Loss: 11.9049 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 33\tNet Loss: 14.2108 \tAnswer Loss: 0.0525 \tQuestion Loss: 14.1582 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 33\tNet Loss: 17.6297 \tAnswer Loss: 0.0417 \tQuestion Loss: 17.5880 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 33\tNet Loss: 9.8661 \tAnswer Loss: 0.0317 \tQuestion Loss: 9.8343 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 33 : 13.8117 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 34\tNet Loss: 10.2593 \tAnswer Loss: 0.0464 \tQuestion Loss: 10.2129 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 34\tNet Loss: 13.5221 \tAnswer Loss: 0.0496 \tQuestion Loss: 13.4725 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 34\tNet Loss: 16.4401 \tAnswer Loss: 0.0381 \tQuestion Loss: 16.4021 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 34\tNet Loss: 13.9055 \tAnswer Loss: 0.0394 \tQuestion Loss: 13.8661 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 34\tNet Loss: 10.2423 \tAnswer Loss: 0.0432 \tQuestion Loss: 10.1992 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 34\tNet Loss: 12.1935 \tAnswer Loss: 0.0471 \tQuestion Loss: 12.1464 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 34\tNet Loss: 12.1864 \tAnswer Loss: 0.0396 \tQuestion Loss: 12.1468 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 34 : 12.6785 Time Taken: 32 seconds\n",
      "Batch: 0 \t Epoch : 35\tNet Loss: 18.2278 \tAnswer Loss: 0.0490 \tQuestion Loss: 18.1788 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 35\tNet Loss: 16.8922 \tAnswer Loss: 0.0419 \tQuestion Loss: 16.8503 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 35\tNet Loss: 11.8257 \tAnswer Loss: 0.0291 \tQuestion Loss: 11.7966 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 35\tNet Loss: 9.0174 \tAnswer Loss: 0.0384 \tQuestion Loss: 8.9790 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 35\tNet Loss: 12.3448 \tAnswer Loss: 0.0528 \tQuestion Loss: 12.2920 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 35\tNet Loss: 17.5778 \tAnswer Loss: 0.0417 \tQuestion Loss: 17.5361 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 35\tNet Loss: 11.0529 \tAnswer Loss: 0.0268 \tQuestion Loss: 11.0262 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 35 : 13.8484 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 36\tNet Loss: 12.7686 \tAnswer Loss: 0.0353 \tQuestion Loss: 12.7333 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 36\tNet Loss: 12.9678 \tAnswer Loss: 0.0433 \tQuestion Loss: 12.9245 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 36\tNet Loss: 11.1571 \tAnswer Loss: 0.0418 \tQuestion Loss: 11.1153 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 36\tNet Loss: 11.9362 \tAnswer Loss: 0.0394 \tQuestion Loss: 11.8968 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 36\tNet Loss: 11.3448 \tAnswer Loss: 0.0491 \tQuestion Loss: 11.2956 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 36\tNet Loss: 10.9940 \tAnswer Loss: 0.0363 \tQuestion Loss: 10.9578 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 36\tNet Loss: 7.4846 \tAnswer Loss: 0.0357 \tQuestion Loss: 7.4490 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 36 : 11.2362 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 37\tNet Loss: 9.1299 \tAnswer Loss: 0.0507 \tQuestion Loss: 9.0792 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 37\tNet Loss: 11.2522 \tAnswer Loss: 0.0415 \tQuestion Loss: 11.2107 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 37\tNet Loss: 8.1289 \tAnswer Loss: 0.0276 \tQuestion Loss: 8.1014 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 37\tNet Loss: 7.9441 \tAnswer Loss: 0.0299 \tQuestion Loss: 7.9142 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 37\tNet Loss: 9.0116 \tAnswer Loss: 0.0457 \tQuestion Loss: 8.9660 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 37\tNet Loss: 9.9336 \tAnswer Loss: 0.0430 \tQuestion Loss: 9.8905 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 37\tNet Loss: 6.7460 \tAnswer Loss: 0.0307 \tQuestion Loss: 6.7153 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 37 : 8.8781 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 38\tNet Loss: 7.8036 \tAnswer Loss: 0.0317 \tQuestion Loss: 7.7719 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 38\tNet Loss: 8.4509 \tAnswer Loss: 0.0301 \tQuestion Loss: 8.4208 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 38\tNet Loss: 8.5425 \tAnswer Loss: 0.0273 \tQuestion Loss: 8.5152 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 38\tNet Loss: 8.0581 \tAnswer Loss: 0.0320 \tQuestion Loss: 8.0261 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 38\tNet Loss: 7.4259 \tAnswer Loss: 0.0406 \tQuestion Loss: 7.3853 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 38\tNet Loss: 6.7965 \tAnswer Loss: 0.0326 \tQuestion Loss: 6.7639 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 38\tNet Loss: 6.2994 \tAnswer Loss: 0.0213 \tQuestion Loss: 6.2781 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 38 : 7.6253 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 39\tNet Loss: 8.8555 \tAnswer Loss: 0.0289 \tQuestion Loss: 8.8266 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 39\tNet Loss: 10.2770 \tAnswer Loss: 0.0351 \tQuestion Loss: 10.2419 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 39\tNet Loss: 6.8178 \tAnswer Loss: 0.0270 \tQuestion Loss: 6.7909 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 39\tNet Loss: 5.0804 \tAnswer Loss: 0.0261 \tQuestion Loss: 5.0543 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 39\tNet Loss: 6.0024 \tAnswer Loss: 0.0309 \tQuestion Loss: 5.9715 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 39\tNet Loss: 9.2302 \tAnswer Loss: 0.0271 \tQuestion Loss: 9.2031 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 39\tNet Loss: 6.5959 \tAnswer Loss: 0.0217 \tQuestion Loss: 6.5742 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 39 : 7.5513 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 40\tNet Loss: 7.1599 \tAnswer Loss: 0.0296 \tQuestion Loss: 7.1304 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 40\tNet Loss: 7.9987 \tAnswer Loss: 0.0316 \tQuestion Loss: 7.9671 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 40\tNet Loss: 6.9163 \tAnswer Loss: 0.0212 \tQuestion Loss: 6.8951 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 40\tNet Loss: 6.7299 \tAnswer Loss: 0.0231 \tQuestion Loss: 6.7068 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 40\tNet Loss: 7.3871 \tAnswer Loss: 0.0315 \tQuestion Loss: 7.3557 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 40\tNet Loss: 8.3110 \tAnswer Loss: 0.0368 \tQuestion Loss: 8.2742 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 40\tNet Loss: 4.8152 \tAnswer Loss: 0.0284 \tQuestion Loss: 4.7868 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 40 : 7.0455 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 41\tNet Loss: 5.9643 \tAnswer Loss: 0.0300 \tQuestion Loss: 5.9342 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 41\tNet Loss: 8.1416 \tAnswer Loss: 0.0274 \tQuestion Loss: 8.1142 \t Time Taken: 4 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 \t Epoch : 41\tNet Loss: 6.9994 \tAnswer Loss: 0.0239 \tQuestion Loss: 6.9756 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 41\tNet Loss: 4.7456 \tAnswer Loss: 0.0241 \tQuestion Loss: 4.7214 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 41\tNet Loss: 4.2684 \tAnswer Loss: 0.0348 \tQuestion Loss: 4.2336 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 41\tNet Loss: 5.2605 \tAnswer Loss: 0.0331 \tQuestion Loss: 5.2273 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 41\tNet Loss: 3.7979 \tAnswer Loss: 0.0219 \tQuestion Loss: 3.7760 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 41 : 5.5968 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 42\tNet Loss: 4.0210 \tAnswer Loss: 0.0239 \tQuestion Loss: 3.9971 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 42\tNet Loss: 4.7974 \tAnswer Loss: 0.0255 \tQuestion Loss: 4.7719 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 42\tNet Loss: 3.5843 \tAnswer Loss: 0.0260 \tQuestion Loss: 3.5582 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 42\tNet Loss: 3.0743 \tAnswer Loss: 0.0255 \tQuestion Loss: 3.0487 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 42\tNet Loss: 3.1154 \tAnswer Loss: 0.0315 \tQuestion Loss: 3.0838 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 42\tNet Loss: 3.5341 \tAnswer Loss: 0.0250 \tQuestion Loss: 3.5091 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 42\tNet Loss: 2.6172 \tAnswer Loss: 0.0176 \tQuestion Loss: 2.5996 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 42 : 3.5348 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 43\tNet Loss: 3.2366 \tAnswer Loss: 0.0242 \tQuestion Loss: 3.2123 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 43\tNet Loss: 3.9685 \tAnswer Loss: 0.0260 \tQuestion Loss: 3.9426 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 43\tNet Loss: 2.3927 \tAnswer Loss: 0.0210 \tQuestion Loss: 2.3716 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 43\tNet Loss: 2.0982 \tAnswer Loss: 0.0177 \tQuestion Loss: 2.0805 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 43\tNet Loss: 2.3256 \tAnswer Loss: 0.0216 \tQuestion Loss: 2.3040 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 43\tNet Loss: 2.5797 \tAnswer Loss: 0.0220 \tQuestion Loss: 2.5577 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 43\tNet Loss: 2.1256 \tAnswer Loss: 0.0161 \tQuestion Loss: 2.1095 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 43 : 2.6753 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 44\tNet Loss: 2.1760 \tAnswer Loss: 0.0202 \tQuestion Loss: 2.1558 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 44\tNet Loss: 3.2175 \tAnswer Loss: 0.0214 \tQuestion Loss: 3.1961 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 44\tNet Loss: 1.9740 \tAnswer Loss: 0.0158 \tQuestion Loss: 1.9582 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 44\tNet Loss: 1.6883 \tAnswer Loss: 0.0141 \tQuestion Loss: 1.6742 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 44\tNet Loss: 2.0592 \tAnswer Loss: 0.0177 \tQuestion Loss: 2.0415 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 44\tNet Loss: 2.0065 \tAnswer Loss: 0.0205 \tQuestion Loss: 1.9860 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 44\tNet Loss: 1.6549 \tAnswer Loss: 0.0135 \tQuestion Loss: 1.6414 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 44 : 2.1109 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 45\tNet Loss: 1.8090 \tAnswer Loss: 0.0158 \tQuestion Loss: 1.7931 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 45\tNet Loss: 3.2907 \tAnswer Loss: 0.0167 \tQuestion Loss: 3.2739 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 45\tNet Loss: 1.5433 \tAnswer Loss: 0.0129 \tQuestion Loss: 1.5305 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 45\tNet Loss: 1.2019 \tAnswer Loss: 0.0131 \tQuestion Loss: 1.1888 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 45\tNet Loss: 1.5200 \tAnswer Loss: 0.0158 \tQuestion Loss: 1.5042 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 45\tNet Loss: 1.6317 \tAnswer Loss: 0.0172 \tQuestion Loss: 1.6145 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 45\tNet Loss: 1.5782 \tAnswer Loss: 0.0107 \tQuestion Loss: 1.5675 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 45 : 1.7964 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 46\tNet Loss: 1.8137 \tAnswer Loss: 0.0123 \tQuestion Loss: 1.8013 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 46\tNet Loss: 2.7500 \tAnswer Loss: 0.0145 \tQuestion Loss: 2.7355 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 46\tNet Loss: 1.1170 \tAnswer Loss: 0.0116 \tQuestion Loss: 1.1054 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 46\tNet Loss: 1.3471 \tAnswer Loss: 0.0106 \tQuestion Loss: 1.3365 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 46\tNet Loss: 2.1880 \tAnswer Loss: 0.0130 \tQuestion Loss: 2.1750 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 46\tNet Loss: 1.7932 \tAnswer Loss: 0.0144 \tQuestion Loss: 1.7788 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 46\tNet Loss: 1.2743 \tAnswer Loss: 0.0091 \tQuestion Loss: 1.2652 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 46 : 1.7548 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 47\tNet Loss: 1.3394 \tAnswer Loss: 0.0110 \tQuestion Loss: 1.3284 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 47\tNet Loss: 2.5717 \tAnswer Loss: 0.0130 \tQuestion Loss: 2.5587 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 47\tNet Loss: 1.5006 \tAnswer Loss: 0.0098 \tQuestion Loss: 1.4908 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 47\tNet Loss: 1.3804 \tAnswer Loss: 0.0085 \tQuestion Loss: 1.3719 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 47\tNet Loss: 1.7053 \tAnswer Loss: 0.0109 \tQuestion Loss: 1.6944 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 47\tNet Loss: 1.2147 \tAnswer Loss: 0.0124 \tQuestion Loss: 1.2022 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 47\tNet Loss: 1.1057 \tAnswer Loss: 0.0078 \tQuestion Loss: 1.0979 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 47 : 1.5454 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 48\tNet Loss: 1.6011 \tAnswer Loss: 0.0101 \tQuestion Loss: 1.5910 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 48\tNet Loss: 3.3556 \tAnswer Loss: 0.0112 \tQuestion Loss: 3.3444 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 48\tNet Loss: 1.0455 \tAnswer Loss: 0.0079 \tQuestion Loss: 1.0376 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 48\tNet Loss: 0.7003 \tAnswer Loss: 0.0073 \tQuestion Loss: 0.6930 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 48\tNet Loss: 1.2582 \tAnswer Loss: 0.0092 \tQuestion Loss: 1.2490 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 48\tNet Loss: 1.3441 \tAnswer Loss: 0.0115 \tQuestion Loss: 1.3326 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 48\tNet Loss: 1.3357 \tAnswer Loss: 0.0068 \tQuestion Loss: 1.3288 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 48 : 1.5201 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 49\tNet Loss: 1.5368 \tAnswer Loss: 0.0088 \tQuestion Loss: 1.5281 \t Time Taken: 5 seconds\n",
      "Batch: 1 \t Epoch : 49\tNet Loss: 2.1094 \tAnswer Loss: 0.0092 \tQuestion Loss: 2.1003 \t Time Taken: 5 seconds\n",
      "Batch: 2 \t Epoch : 49\tNet Loss: 0.7770 \tAnswer Loss: 0.0070 \tQuestion Loss: 0.7700 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 49\tNet Loss: 0.9717 \tAnswer Loss: 0.0065 \tQuestion Loss: 0.9651 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 49\tNet Loss: 1.5964 \tAnswer Loss: 0.0085 \tQuestion Loss: 1.5879 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 49\tNet Loss: 1.8732 \tAnswer Loss: 0.0099 \tQuestion Loss: 1.8633 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 49\tNet Loss: 1.1846 \tAnswer Loss: 0.0056 \tQuestion Loss: 1.1790 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 49 : 1.4356 Time Taken: 32 seconds\n",
      "Batch: 0 \t Epoch : 50\tNet Loss: 0.9102 \tAnswer Loss: 0.0076 \tQuestion Loss: 0.9026 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 50\tNet Loss: 1.7707 \tAnswer Loss: 0.0083 \tQuestion Loss: 1.7625 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 50\tNet Loss: 0.8770 \tAnswer Loss: 0.0064 \tQuestion Loss: 0.8705 \t Time Taken: 4 seconds\n",
      "Batch: 3 \t Epoch : 50\tNet Loss: 0.8845 \tAnswer Loss: 0.0058 \tQuestion Loss: 0.8788 \t Time Taken: 4 seconds\n",
      "Batch: 4 \t Epoch : 50\tNet Loss: 1.3034 \tAnswer Loss: 0.0084 \tQuestion Loss: 1.2950 \t Time Taken: 4 seconds\n",
      "Batch: 5 \t Epoch : 50\tNet Loss: 1.0096 \tAnswer Loss: 0.0088 \tQuestion Loss: 1.0008 \t Time Taken: 4 seconds\n",
      "Batch: 6 \t Epoch : 50\tNet Loss: 0.8339 \tAnswer Loss: 0.0049 \tQuestion Loss: 0.8289 \t Time Taken: 4 seconds\n",
      "Average Loss after Epoch 50 : 1.0842 Time Taken: 30 seconds\n",
      "Batch: 0 \t Epoch : 51\tNet Loss: 0.7771 \tAnswer Loss: 0.0066 \tQuestion Loss: 0.7704 \t Time Taken: 4 seconds\n",
      "Batch: 1 \t Epoch : 51\tNet Loss: 1.6621 \tAnswer Loss: 0.0078 \tQuestion Loss: 1.6543 \t Time Taken: 4 seconds\n",
      "Batch: 2 \t Epoch : 51\tNet Loss: 0.7761 \tAnswer Loss: 0.0062 \tQuestion Loss: 0.7699 \t Time Taken: 4 seconds\n"
     ]
    }
   ],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "question_encoder_1_hidden = None\n",
    "question_encoder_2_hidden = None\n",
    "question_decoder_hidden = None\n",
    "attn_output = None\n",
    "attention_hidden = None\n",
    "Attention_Weights = None\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    epoch_time_start = time.time()\n",
    "    for batch_num in range(len(batch_input)):\n",
    "        batch_time_start = time.time()\n",
    "        answer_encoder_hidden = answerEncoder.initHidden(batch_size)\n",
    "        \n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        if type(question_encoder_1_hidden) == Variable:\n",
    "            question_encoder_1_hidden = repackage_hidden(question_encoder_1_hidden)\n",
    "        if type(question_encoder_2_hidden) == Variable:\n",
    "            question_encoder_2_hidden = repackage_hidden(question_encoder_2_hidden)\n",
    "        if type(question_decoder_hidden) == Variable:\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        if(type(attn_output) == Variable):\n",
    "            attn_output = repackage_hidden(attn_output)\n",
    "        if(type(attention_hidden) == Variable):\n",
    "            attention_hidden = repackage_hidden(attention_hidden)\n",
    "        if(type(Attention_Weights) == Variable):\n",
    "            Attention_Weights = repackage_hidden(Attention_Weights)\n",
    "        \n",
    "        current_batch_size = len(batch_input[batch_num][\"document_tokens\"])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        maxDocLenForBatch = max_document_len\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        \n",
    "        for i in range(current_batch_size):\n",
    "            mask[i,0:batch_input[batch_num][\"document_lengths\"][i]] = 1\n",
    "            \n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[batch_num][\"document_tokens\"]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels_all\"])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        \n",
    "        #answer_outputs = Variable(torch.zeros(current_batch_size, max_document_len, hidden_size))\n",
    "        #answer_tags = Variable(torch.zeros(current_batch_size, max_document_len, 1))\n",
    "        \n",
    "        if use_cuda:\n",
    "            #answer_outputs = answer_outputs.cuda()\n",
    "            #answer_tags = answer_tags.cuda()\n",
    "            embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)              \n",
    "\n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        \n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "\n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        answer_mask_1 = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        answer_mask_2 = Variable(torch.from_numpy(1 - batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_mask_1 = answer_mask_1.cuda()\n",
    "            answer_mask_2 = answer_mask_2.cuda()\n",
    "        \n",
    "        question_encoder_input1 = torch.mul(answer_mask_1, answer_outputs.float())\n",
    "        question_encoder_input2 = torch.mul(answer_mask_2, answer_outputs.float())\n",
    "        \n",
    "        question_encoder_1_hidden = answer_encoder_hidden.view(1,answer_encoder_hidden.shape[1],2*answer_encoder_hidden.shape[2])\n",
    "        question_encoder_2_hidden = answer_encoder_hidden.view(1,answer_encoder_hidden.shape[1],2*answer_encoder_hidden.shape[2])\n",
    "        \n",
    "        question_encoder_1_outputs , question_encoder_1_hidden = questionEncoder1(question_encoder_input1, question_encoder_1_hidden)\n",
    "        question_encoder_2_outputs , question_encoder_2_hidden = questionEncoder2(question_encoder_input2, question_encoder_2_hidden)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        question_loss = 0\n",
    "        question_decoder_hidden = question_encoder_2_hidden\n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[batch_num][\"question_input_tokens\"]).long())\n",
    "        output_labels = Variable(torch.from_numpy(batch_input[batch_num][\"question_output_tokens\"]).long())\n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "\n",
    "        for quesL in range(batch_input[batch_num][\"question_input_tokens\"].shape[1]):\n",
    "            if use_attention:\n",
    "                attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "                decoder_output, attention_hidden = questionDecoder(\n",
    "                    embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "            else:\n",
    "                '''\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    embedded_inputs[:,quesL:quesL+1,:],\n",
    "                    question_decoder_hidden)\n",
    "                '''\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    torch.cat((embedded_inputs[:,quesL:quesL+1,:], embedded_inp), dim=1),\n",
    "                    question_decoder_hidden)\n",
    "\n",
    "                decoder_output = fcLayer(decoder_output.resize(batch_size,300,max_document_len+1)).resize(batch_size, 1, 300)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output.squeeze(1))\n",
    "            \n",
    "            '''\n",
    "            idx = np.where (output_labels[:, quesL:quesL+1].squeeze(1).data == END_TOKEN)[0].tolist()\n",
    "            final_copy = final_output.clone()\n",
    "            if len(idx):\n",
    "                print(final_copy.data[idx])\n",
    "                final_copy.data[idx] = 0\n",
    "                final_copy.data[idx][:,END_TOKEN] = 1\n",
    "                print(final_copy[idx])\n",
    "                print(\"IDX\", idx)\n",
    "                print(\"Where:\", np.where(final_copy[idx].data == 1)) \n",
    "            '''\n",
    "            question_loss += criterion2(final_output,\n",
    "                                       output_labels[:, quesL:quesL+1].squeeze(1))\n",
    "            \n",
    "\n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        batch_time_end = time.time()\n",
    "        \n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f \\t Time Taken: %d seconds'\n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0], batch_time_end-batch_time_start))\n",
    "    epoch_time_end = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f Time Taken: %d seconds'\n",
    "                   %(epoch, avg_loss/len(batch_input), (epoch_time_end-epoch_time_start)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcLayer(x.cpu().view(32,300,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/data/ra2630/qgen_base_all_data.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_glove.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "answerEncoder2 = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder2_1 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionEncoder2_2 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder2 = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator2 = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention2 = AttnDecoderRNN(hidden_size= hidden_size,max_document_len = max_document_len)\n",
    "fcLayer2 = FCLayer(max_document_len+1, 1)\n",
    "optimizer2 = torch.optim.Adam(train_param, 0.01)\n",
    "\n",
    "\n",
    "#answerEncoder2.load_state_dict(checkpoint[\"answerEncoder\"])\n",
    "#questionEncoder2_1.load_state_dict(checkpoint[\"questionEncoder1\"])\n",
    "#questionEncoder2_2.load_state_dict(checkpoint[\"questionEncoder2\"])\n",
    "#questionDecoder2.load_state_dict(checkpoint[\"questionDecoder\"])\n",
    "#questionGenerator2.load_state_dict(checkpoint[\"questionGenerator\"])\n",
    "#optimizer2.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "\n",
    "answerEncoder2 = answerEncoder\n",
    "questionEncoder2_1 = questionEncoder1\n",
    "questionEncoder2_2 = questionEncoder2\n",
    "questionDecoder2 = questionDecoder\n",
    "questionGenerator2 = questionGenerator\n",
    "attention2 = attention\n",
    "fcLayer2 = fcLayer\n",
    "optimizer2 = optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSetForInferenceFromBatch(batch_number):\n",
    "    data_dict = batch_input[batch_number]\n",
    "    data_dict['answer_mask_1'] = data_dict['answer_labels']\n",
    "    data_dict['answer_mask_2'] = 1 - data_dict['answer_mask_1']\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSetForInference(context, answer, maxDocLength, batch_size):\n",
    "    \n",
    "    passage = word_tokenize(context.lower())\n",
    "    doc_len = len(passage)\n",
    "    \n",
    "    ans_condition = word_tokenize(answer.lower())\n",
    "    \n",
    "    ans_len = len(ans_condition)\n",
    "        \n",
    "    start,end = find_sub_list(ans_condition,passage)\n",
    "\n",
    "    if start == -1:\n",
    "            print(\"Couldn't Find answer in text, Please try again !!\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    document_token = np.full((1, maxDocLength), END_TOKEN, dtype=np.int32)\n",
    "    document_length = np.zeros(1, dtype=np.int32)\n",
    "    answer_mask_1 = np.zeros((1,maxDocLength), dtype=np.int32)\n",
    "    answer_mask_2 = np.zeros((1,maxDocLength), dtype=np.int32)\n",
    "    answer_length = np.zeros(1, dtype=np.int32)\n",
    "    \n",
    "    document_length[0] = doc_len\n",
    "    answer_length[0] = ans_len\n",
    "    answer_mask_1[0,start:end] = 1\n",
    "    answer_mask_2 = 1 - answer_mask_1\n",
    "    \n",
    "    \n",
    "    for i in range(doc_len):\n",
    "        document_token[:,i] = look_up_word_reduced(passage[i])\n",
    "    \n",
    "    document_token = document_token.repeat(batch_size, axis = 0)\n",
    "    document_length = document_length.repeat(batch_size, axis = 0)\n",
    "    answer_mask_1 = answer_mask_1.repeat(batch_size, axis = 0)\n",
    "    answer_mask_2 = answer_mask_2.repeat(batch_size, axis = 0)\n",
    "    answer_length = answer_length.repeat(batch_size, axis = 0)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"document_tokens\" : document_token,\n",
    "        \"document_length\" : document_length,\n",
    "        \"answer_mask_1\" : answer_mask_1,\n",
    "        \"answer_mask_2\" : answer_mask_2,\n",
    "        \"answer_length\" : answer_length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(context, answer, maxDocLen, batch_num = 0):\n",
    "\n",
    "    #data_dict = prepareSetForInference(context, answer, maxDocLen, batch_size)\n",
    "    data_dict = prepareSetForInferenceFromBatch(batch_num)\n",
    "    use_attention = False\n",
    "    use_cuda = True\n",
    "\n",
    "    answer_encoder_hidden_inf = answerEncoder2.initHidden(batch_size)\n",
    "\n",
    "\n",
    "    inp = Variable(torch.from_numpy(data_dict[\"document_tokens\"]).long())\n",
    "    labels = Variable(torch.from_numpy(data_dict[\"answer_mask_1\"])).long() #Let's see if we want to use or not\n",
    "    embedded_inp = embedder(inp)\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "        labels = labels.cuda()\n",
    "        embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "\n",
    "    answer_encoder_hidden_inf = repackage_hidden(answer_encoder_hidden_inf)\n",
    "    answer_tags, answer_outputs, answer_encoder_hidden_inf = answerEncoder(embedded_inp, answer_encoder_hidden_inf)\n",
    "\n",
    "    if use_cuda:\n",
    "        answer_outputs = answer_outputs.cuda()\n",
    "        answer_tags = answer_tags.cuda()\n",
    "\n",
    "    ####USE ANSWER_TAGS TO CONDITION QUESTIONS RHATHRE THAN PASSING ANSWERS\n",
    "\n",
    "    answer_mask_1 = Variable(torch.from_numpy(data_dict[\"answer_mask_1\"])).float().unsqueeze(-1)\n",
    "    answer_mask_2 = Variable(torch.from_numpy(data_dict[\"answer_mask_2\"])).float().unsqueeze(-1)\n",
    "\n",
    "    if use_cuda:\n",
    "        answer_mask_1 = answer_mask_1.cuda()\n",
    "        answer_mask_2 = answer_mask_2.cuda()\n",
    "\n",
    "    question_encoder_input1 = torch.mul(answer_mask_1, answer_outputs.float())\n",
    "    question_encoder_input2 = torch.mul(answer_mask_2, answer_outputs.float())\n",
    "\n",
    "    if use_cuda:\n",
    "        question_encoder_input1 = question_encoder_input1.cuda()\n",
    "        question_encoder_input2 = question_encoder_input2.cuda()\n",
    "\n",
    "    question_encoder_hidden_1_inf = answer_encoder_hidden_inf.view(1,answer_encoder_hidden_inf.shape[1],2*answer_encoder_hidden_inf.shape[2])\n",
    "    question_encoder_hidden_2_inf = answer_encoder_hidden_inf.view(1,answer_encoder_hidden_inf.shape[1],2*answer_encoder_hidden_inf.shape[2])\n",
    "\n",
    "    question_encoder_1_outputs , question_encoder_1_hidden = questionEncoder1(question_encoder_input1, question_encoder_hidden_1_inf)\n",
    "    question_encoder_2_outputs , question_encoder_2_hidden = questionEncoder2(question_encoder_input2, question_encoder_hidden_2_inf)\n",
    "\n",
    "\n",
    "\n",
    "    maxGenQuesLen = 20\n",
    "    currGenQuestionLen = 0\n",
    "\n",
    "    current_question_token = torch.from_numpy(np.full((batch_size), START_TOKEN)).long()\n",
    "    questionGenerated = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        questionGenerated.append([])\n",
    "\n",
    "    question_decoder_hidden_inf = question_encoder_2_hidden\n",
    "    while currGenQuestionLen < maxGenQuesLen:\n",
    "        question_token_embedding = embedder(current_question_token)\n",
    "        if use_cuda:\n",
    "            question_token_embedding = question_token_embedding.cuda()\n",
    "\n",
    "        if use_attention:\n",
    "            attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "            decoder_output, attention_hidden = questionDecoder(\n",
    "                embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "        else:\n",
    "            decoder_output, question_decoder_hidden_inf = questionDecoder(\n",
    "                    torch.cat((question_token_embedding.unsqueeze(1), embedded_inp), dim=1),\n",
    "                    question_decoder_hidden_inf)\n",
    "\n",
    "            decoder_output = fcLayer(decoder_output.resize(batch_size,300,max_document_len+1)).resize(batch_size, 1, 300)\n",
    "\n",
    "\n",
    "        final_output = questionGenerator2(decoder_output.squeeze(1))\n",
    "        current_question_token = np.argmax(final_output.data, axis = 1)\n",
    "        for i in range(batch_size):\n",
    "            questionGenerated[i].append(look_up_token_reduced(current_question_token[i]))\n",
    "        currGenQuestionLen += 1\n",
    "\n",
    "    return questionGenerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_n = 8\n",
    "\n",
    "questionGenerated = inference(' '.join(X_train_comp_all_shuffled[10]),\n",
    "                              ' '.join(X_train_comp_all_shuffled[10]),\n",
    "                              max_document_len, b_n)\n",
    "\n",
    "for i in range(32):\n",
    "\n",
    "    #print(' '.join(X_train_comp_all_shuffled[10]))\n",
    "    #print(' '.join(X_train_comp_all_shuffled[10]))\n",
    "    print(' '.join(Y_train_ques_all_shuffled[32*b_n + i]))\n",
    "    \n",
    "    for questions in questionGenerated[i:i+1]:\n",
    "        print(\"Generated Question : \")\n",
    "        for word in questions:\n",
    "            print(word, sep = ' ', end = ' ')\n",
    "        print(\"\")\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for questions in questionGenerated[0:1]:\n",
    "    print(\"Question : \")\n",
    "    for word in questions:\n",
    "        print(word, sep = ' ', end = ' ')\n",
    "    print(\"\")\n",
    "    \n",
    "for questions in questionGenerated[-2:-1]:\n",
    "    print(\"Question : \")\n",
    "    for word in questions:\n",
    "        print(word, sep = ' ', end = ' ')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(final_output.data, axis = 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_dict[\"document_tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.argmax(final_output.data, axis = 1):\n",
    "    print(look_up_token_reduced(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.full((batch_size), START_TOKEN).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 3\n",
    "example_num = 0\n",
    "printQuestion(batch_num, example_num)\n",
    "printAnswer(batch_num, example_num)\n",
    "printContext(batch_num, example_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(X_train_ans_all_shuffled[0 + 19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(Y_train_ques_all_shuffled[0 + 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(X_train_comp_all_shuffled[0 + 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[0]['question_input_tokens'][0], batch_input[0]['question_output_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in output_labels:\n",
    "    for i in b:\n",
    "        print(look_up_token_reduced(i.cpu().data[0]), sep = \" \", end = \" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "    epoch_time_start = time.time()\n",
    "    for batch_num in range(len(batch_input)):\n",
    "        batch_time_start = time.time()\n",
    "        answer_encoder_hidden = answerEncoder.initHidden(batch_size)\n",
    "        \n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        if type(question_encoder_1_hidden) == Variable:\n",
    "            question_encoder_1_hidden = repackage_hidden(question_encoder_1_hidden)\n",
    "        if type(question_encoder_2_hidden) == Variable:\n",
    "            question_encoder_2_hidden = repackage_hidden(question_encoder_2_hidden)\n",
    "        if type(question_decoder_hidden) == Variable:\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        if(type(attn_output) == Variable):\n",
    "            attn_output = repackage_hidden(attn_output)\n",
    "        if(type(attention_hidden) == Variable):\n",
    "            attention_hidden = repackage_hidden(attention_hidden)\n",
    "        if(type(Attention_Weights) == Variable):\n",
    "            Attention_Weights = repackage_hidden(Attention_Weights)\n",
    "        \n",
    "        current_batch_size = len(batch_input[batch_num][\"document_tokens\"])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        maxDocLenForBatch = max_document_len\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        \n",
    "        for i in range(current_batch_size):\n",
    "            mask[i,0:batch_input[batch_num][\"document_lengths\"][i]] = 1\n",
    "            \n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[batch_num][\"document_tokens\"]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels_all\"])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        \n",
    "        #answer_outputs = Variable(torch.zeros(current_batch_size, max_document_len, hidden_size))\n",
    "        #answer_tags = Variable(torch.zeros(current_batch_size, max_document_len, 1))\n",
    "        \n",
    "        if use_cuda:\n",
    "            #answer_outputs = answer_outputs.cuda()\n",
    "            #answer_tags = answer_tags.cuda()\n",
    "            embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)              \n",
    "\n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        \n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "\n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        answer_mask_1 = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        answer_mask_2 = Variable(torch.from_numpy(1 - batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_mask_1 = answer_mask_1.cuda()\n",
    "            answer_mask_2 = answer_mask_2.cuda()\n",
    "        \n",
    "        question_encoder_input1 = torch.mul(answer_mask_1, answer_outputs.float())\n",
    "        question_encoder_input2 = torch.mul(answer_mask_2, answer_outputs.float())\n",
    "        \n",
    "        question_encoder_1_hidden = answer_encoder_hidden.view(1,answer_encoder_hidden.shape[1],2*answer_encoder_hidden.shape[2])\n",
    "        question_encoder_2_hidden = answer_encoder_hidden.view(1,answer_encoder_hidden.shape[1],2*answer_encoder_hidden.shape[2])\n",
    "        \n",
    "        question_encoder_1_outputs , question_encoder_1_hidden = questionEncoder1(question_encoder_input1, question_encoder_1_hidden)\n",
    "        question_encoder_2_outputs , question_encoder_2_hidden = questionEncoder2(question_encoder_input2, question_encoder_2_hidden)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        question_loss = 0\n",
    "        question_decoder_hidden = question_encoder_2_hidden\n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[batch_num][\"question_input_tokens\"]).long())\n",
    "        output_labels = Variable(torch.from_numpy(batch_input[batch_num][\"question_output_tokens\"]).long())\n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "\n",
    "        for quesL in range(batch_input[batch_num][\"question_input_tokens\"].shape[1]):\n",
    "            if use_attention:\n",
    "                attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "                decoder_output, attention_hidden = questionDecoder(\n",
    "                    embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "            else:\n",
    "                '''\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    embedded_inputs[:,quesL:quesL+1,:],\n",
    "                    question_decoder_hidden)\n",
    "                '''\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    torch.cat((embedded_inputs[:,quesL:quesL+1,:], embedded_inp), dim=1),\n",
    "                    question_decoder_hidden)\n",
    "\n",
    "                decoder_output = fcLayer(x.resize(batch_size,300,max_document_len+1)).resize(batch_size, 1, 300)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output.squeeze(1))\n",
    "            \n",
    "            '''\n",
    "            idx = np.where (output_labels[:, quesL:quesL+1].squeeze(1).data == END_TOKEN)[0].tolist()\n",
    "            final_copy = final_output.clone()\n",
    "            if len(idx):\n",
    "                print(final_copy.data[idx])\n",
    "                final_copy.data[idx] = 0\n",
    "                final_copy.data[idx][:,END_TOKEN] = 1\n",
    "                print(final_copy[idx])\n",
    "                print(\"IDX\", idx)\n",
    "                print(\"Where:\", np.where(final_copy[idx].data == 1)) \n",
    "            '''\n",
    "            question_loss += criterion2(final_output,\n",
    "                                       output_labels[:, quesL:quesL+1].squeeze(1))\n",
    "            \n",
    "\n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        batch_time_end = time.time()\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f \\t Time Taken: %d seconds'\n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0], batch_time_end-batch_time_start))\n",
    "    epoch_time_end = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f Time Taken: %d seconds'\n",
    "                   %(epoch, avg_loss/number_of_batches, (epoch_time_end-epoch_time_start)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
