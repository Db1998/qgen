{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!export LC_ALL=en_US.UTF-8\n",
    "#!CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "#import tensorflow as tf\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "from embedding import *\n",
    "import torch\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-207e6a8f7211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train-v1.1.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "data = json.load(open('train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordToTake = 2000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESKTOP-Q771UKJ\n",
      "1493\n",
      "rev\n",
      ".\n",
      "john\n",
      "j.\n",
      "cavanaugh\n",
      ",\n",
      "c.s.c\n",
      ".\n",
      "served\n",
      "more\n",
      "than\n",
      "half\n",
      ",\n",
      "lobund\n",
      "institute\n",
      "for\n",
      "animal\n",
      "studies\n",
      "and\n",
      "medieval\n",
      "institute\n",
      ".\n",
      "hall\n",
      "of\n",
      "liberal\n",
      "arts\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'s\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '``', 'venite', 'ad', 'me', 'omnes', \"''\", '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in', '1858.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n",
      "['saint', 'bernadette', 'soubirous']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'soubirous'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comp_all[0][104]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102, 105)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comp_ans_all[0][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,\n",
       "        1.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,\n",
       "        1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,\n",
       "        1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_ans_label_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'whom',\n",
       " 'did',\n",
       " 'the',\n",
       " 'virgin',\n",
       " 'mary',\n",
       " 'allegedly',\n",
       " 'appear',\n",
       " 'in',\n",
       " '1858',\n",
       " 'in',\n",
       " 'lourdes',\n",
       " 'france',\n",
       " '?']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_ques_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493\n",
      "['the', 'success', 'of', 'its', 'football', 'team', 'made', 'notre', 'dame', 'a', 'household', 'name', '.', 'the', 'success', 'of', 'note', 'dame', 'reflected', 'rising', 'status', 'of', 'irish', 'americans', 'and', 'catholics', 'in', 'the', '1920s', '.', 'catholics', 'rallied', 'up', 'around', 'the', 'team', 'and', 'listen', 'to', 'the', 'games', 'on', 'the', 'radio', ',', 'especially', 'when', 'it', 'knocked', 'off', 'the', 'schools', 'that', 'symbolized', 'the', 'protestant', 'establishment', 'in', 'america', '—', 'harvard', ',', 'yale', ',', 'princeton', ',', 'and', 'army', '.', 'yet', 'this', 'role', 'as', 'high-profile', 'flagship', 'institution', 'of', 'catholicism', 'made', 'it', 'an', 'easy', 'target', 'of', 'anti-catholicism', '.', 'the', 'most', 'remarkable', 'episode', 'of', 'violence', 'was', 'the', 'clash', 'between', 'notre', 'dame', 'students', 'and', 'the', 'ku', 'klux', 'klan', 'in', '1924.', 'nativism', 'and', 'anti-catholicism', ',', 'especially', 'when', 'directed', 'towards', 'immigrants', ',', 'were', 'cornerstones', 'of', 'the', 'kkk', \"'s\", 'rhetoric', ',', 'and', 'notre', 'dame', 'was', 'seen', 'as', 'a', 'symbol', 'of', 'the', 'threat', 'posed', 'by', 'the', 'catholic', 'church', '.', 'the', 'klan', 'decided', 'to', 'have', 'a', 'week-long', 'klavern', 'in', 'south', 'bend', '.', 'clashes', 'with', 'the', 'student', 'body', 'started', 'on', 'march', '17', ',', 'when', 'students', ',', 'aware', 'of', 'the', 'anti-catholic', 'animosity', ',', 'blocked', 'the', 'klansmen', 'from', 'descending', 'from', 'their', 'trains', 'in', 'the', 'south', 'bend', 'station', 'and', 'ripped', 'the', 'kkk', 'clothes', 'and', 'regalia', '.', 'on', 'may', '19', 'thousands', 'of', 'students', 'massed', 'downtown', 'protesting', 'the', 'klavern', ',', 'and', 'only', 'the', 'arrival', 'of', 'college', 'president', 'fr', '.', 'matthew', 'walsh', 'prevented', 'any', 'further', 'clashes', '.', 'the', 'next', 'day', ',', 'football', 'coach', 'knute', 'rockne', 'spoke', 'at', 'a', 'campus', 'rally', 'and', 'implored', 'the', 'students', 'to', 'obey', 'the', 'college', 'president', 'and', 'refrain', 'from', 'further', 'violence', '.', 'a', 'few', 'days', 'later', 'the', 'klavern', 'broke', 'up', ',', 'but', 'the', 'hostility', 'shown', 'by', 'the', 'students', 'was', 'an', 'omen', 'and', 'a', 'contribution', 'to', 'the', 'downfall', 'of', 'the', 'kkk', 'in', 'indiana', '.']\n",
      "['the', 'ku', 'klux', 'klan']\n",
      "['notre', 'dame', 'students', 'had', 'a', 'showdown', 'in', '1924', 'with', 'which', 'anti-catholic', 'group', '?']\n",
      "['fryderyk', 'chopin', 'was', 'born', 'in', 'żelazowa', 'wola', ',', '46', 'kilometres', '(', '29', 'miles', ')', 'west', 'of', 'warsaw', ',', 'in', 'what', 'was', 'then', 'the', 'duchy', 'of', 'warsaw', ',', 'a', 'polish', 'state', 'established', 'by', 'napoleon', '.', 'the', 'parish', 'baptismal', 'record', 'gives', 'his', 'birthday', 'as', '22', 'february', '1810', ',', 'and', 'cites', 'his', 'given', 'names', 'in', 'the', 'latin', 'form', 'fridericus', 'franciscus', '(', 'in', 'polish', ',', 'he', 'was', 'fryderyk', 'franciszek', ')', '.', 'however', ',', 'the', 'composer', 'and', 'his', 'family', 'used', 'the', 'birthdate', '1', 'march', ',', '[', 'n', '2', ']', 'which', 'is', 'now', 'generally', 'accepted', 'as', 'the', 'correct', 'date', '.']\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.]\n",
      "['fridericus', 'franciscus']\n",
      "[ 0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.]\n",
      "['chopin', \"'s\", 'given', 'names', 'in', 'latin', 'are', 'what', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "examples_to_take_train = 32\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 282)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1\n",
    "            \n",
    "    words_to_consider_expression = set(X_train_comp[i] + nltkStopWords + punctuations)\n",
    "\n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_contexts[i,:,look_up_word_reduced(word)] = 1\n",
    "        \n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_probabilities[i,:,look_up_word_reduced(word)] = len(np.where(expression_contexts[i][0] == 1)[0]) / float(wordToTake)\n",
    "    expression_probabilities[i,:,np.where(expression_probabilities[i][0] == 0)[0]] = len(np.where(expression_contexts[i][0] == 0)[0]) / float(wordToTake)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " '-',\n",
       " '.',\n",
       " '1993',\n",
       " '2013',\n",
       " '64–40',\n",
       " '7,189.',\n",
       " '?',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'all-star',\n",
       " 'allstar',\n",
       " 'am',\n",
       " 'american',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'area',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'barnstormers',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beijing',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'china',\n",
       " 'conference',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'crowd',\n",
       " 'd',\n",
       " 'defeated',\n",
       " 'des',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'first',\n",
       " 'flood',\n",
       " 'for',\n",
       " 'from',\n",
       " 'front',\n",
       " 'fundraiser',\n",
       " 'further',\n",
       " 'future',\n",
       " 'game',\n",
       " 'games',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " \"hawai'i\",\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'home',\n",
       " 'honolulu',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'iowa',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'league',\n",
       " 'll',\n",
       " 'long-running',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'moines',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'national',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'oct.',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'second',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'staged',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'two',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'victims',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_consider_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1828\n",
      "172\n",
      "1828\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "print(len(np.where(expression_contexts[10][0] == 0)[0]))\n",
    "print(len(np.where(expression_contexts[10][0] == 1)[0]))\n",
    "\n",
    "print(len(np.where(expression_probabilities[10][0] > 0.5)[0]))\n",
    "print(len(np.where(expression_probabilities[10][0] < 0.5)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 300)\n",
      "1716\n",
      "130\n",
      "1658\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 148,  586,  355,    0,   73,    3,    0,    5,    0,    0,   20,\n",
       "          0,  718,    7,  799,  227,    0,    0,    4, 1660,   22,    0,\n",
       "          6,    0,  243,    8,  236,    4,  573,   18, 1568,  642,    0,\n",
       "          8,    0,   17,   95, 1878,    4,   12,   13,    0,   13,  142,\n",
       "        140,  243,    4,  279,  690,    0,    0,    4,   13,    0,    0,\n",
       "        136,    6,    0,    7,   46,  295,    0,  586,  152,   34,   13,\n",
       "          0,   13,    3,    0,    7,    0,   19,   29, 1110, 1244,  145,\n",
       "         18,    3,  185,    5,    3,  133,    6,   51,   13,    3,  133,\n",
       "        335,    9,   33,  185,   53,  616, 1991,   29, 1034,    0,  145,\n",
       "          4,    7,    0,    4,    9,  240,   24, 1762,    8,    0,   25,\n",
       "          4,  414,   71,  410,   39,  611,  457,    9,    3,    0,  633,\n",
       "          5,   24,    0,   25,    6,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 282)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train,max_answer_len),dtype=np.int32)\n",
    "#expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "#expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 21)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_input_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index==9 or index==10:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ,:])\n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        maxD = max(inputs[1][start:])\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ])\n",
    "        elif index==9 or index==10:\n",
    "            output.append(inp[start:,0:maxQ,:]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer,expression_contexts,expression_probabilities]\n",
    "                    ,batch_size)\n",
    "number_of_batches = len(batch_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.FloatTensor(reduced_glove))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(AnswerEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first= True, bidirectional=True) #Input_size = Hidden_Size\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, batch_size, self.hidden_size)) #2 for BiDirectional\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    \n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = F.log_softmax(output.view(-1,1,1)).view(1,1,-1)\n",
    "        return output\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, output_size, max_passage_length, dropout_p=0.1, max_length=25):\n",
    "        \n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        self.passage_length = max_passage_length\n",
    "\n",
    "        #self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.passage_length + self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        #self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        #self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        #embedded_inputs[quesL:quesL+1], question_encoder_hidden_batch[i], answer_outputs[i]\n",
    "        # 300    |  1 x 300    |    301x600\n",
    "        hidden = hidden.squeeze(0)\n",
    "        # 300\n",
    "        hidden = self.dropout(hidden)\n",
    "        #print(\"Inp:\", input.unsqueeze(0).size())\n",
    "        #print(\"Hidden: \",hidden.unsqueeze(0).size())\n",
    "        #print(\"Hidden: \",hidden.unsqueeze(0).size())\n",
    "        concat = torch.cat((input.unsqueeze(0), hidden.unsqueeze(0)), 1)\n",
    "        #print(concat.size())\n",
    "        #reduce = self.attn(concat)\n",
    "        #print(reduce.size())\n",
    "        attn_weights = F.softmax(concat, dim=1)\n",
    "        #print(attn_weights.size())\n",
    "        #print(attn_weights.squeeze(0).unsqueeze(1).size())        \n",
    "        attn_applied = torch.mm(encoder_outputs, attn_weights.squeeze(0).unsqueeze(1))\n",
    "        #print(embedded.unsqueeze(0).size())\n",
    "        #print(attn_applied.squeeze(1).unsqueeze(0).size())\n",
    "        output = torch.cat((hidden.unsqueeze(0), attn_applied.squeeze(1).unsqueeze(0)), 1)\n",
    "        #print(\"out: \",output.size())\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        #output, hidden = self.gru(output, hidden)\n",
    "\n",
    "       # output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "   \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "   if type(h) == Variable:\n",
    "       return Variable(h.data)\n",
    "   else:\n",
    "       return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters =  18\n"
     ]
    }
   ],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "#Embedding(words_to_take, 300)\n",
    "fcLayer = FCLayer(hidden_size*2, hidden_size)\n",
    "answerEncoder = AnswerEncoderRNN(input_size = reduced_glove.shape[0], hidden_size=hidden_size)\n",
    "questionEncoder = QuestionEncoderRNN(input_size=hidden_size*2, hidden_size=hidden_size*2)\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention = AttnDecoderRNN(hidden_size,max_question_len,max_document_len)\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [embedder, answerEncoder, questionEncoder, questionDecoder, questionGenerator]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 0.0001)\n",
    "\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedder(\n",
      "  (embedding): Embedding(2000, 300)\n",
      ")\n",
      "FCLayer(\n",
      "  (fc): Linear(in_features=600, out_features=300, bias=True)\n",
      ")\n",
      "AnswerEncoderRNN(\n",
      "  (gru): GRU(300, 300, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=600, out_features=1, bias=True)\n",
      ")\n",
      "QuestionEncoderRNN(\n",
      "  (gru): GRU(600, 600, batch_first=True)\n",
      ")\n",
      "QuestionDecoderRNN(\n",
      "  (gru): GRU(300, 300, batch_first=True)\n",
      ")\n",
      "QuestionGenerationFC(\n",
      "  (fc): Linear(in_features=300, out_features=2000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(embedder)\n",
    "print(fcLayer)\n",
    "print(answerEncoder)\n",
    "print(questionEncoder)\n",
    "print(questionDecoder)\n",
    "print(questionGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples_to_take_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kumar\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1189: UserWarning: Using a target size (torch.Size([32, 282, 1])) that is different to the input size (torch.Size([32, 282])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "C:\\Users\\Kumar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (1, 2000), got torch.Size([1, 1, 2000])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-07c0da4e1364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[1;31m#Batch[5] -> Question input tokens (num_of_examples * max_question_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m                 question_loss += criterion2(final_output, \n\u001b[1;32m---> 92\u001b[1;33m                                            output_labels[quesL:quesL+1]) # fOR NLL Loss\n\u001b[0m\u001b[0;32m     93\u001b[0m                 \u001b[1;31m#question_loss += criterion2(final_output.squeeze(0), output_label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 357\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    358\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         return F.nll_loss(input, target, self.weight, self.size_average,\n\u001b[1;32m--> 170\u001b[1;33m                           self.ignore_index, self.reduce)\n\u001b[0m\u001b[0;32m    171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[1;32m-> 1061\u001b[1;33m                 out_size, input.size()))\n\u001b[0m\u001b[0;32m   1062\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1063\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected target size (1, 2000), got torch.Size([1, 1, 2000])"
     ]
    }
   ],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 10\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "Attention_Weights = None\n",
    "attn_output = None\n",
    "attention_hidden = attention.initHidden()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "        \n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        # batch_size(32) * max_passage_length(301)\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "            \n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "        # batch_size(32) * max_passage_length(301)\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num]))\n",
    "        \n",
    "        labels = labels.long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        # batch_size(32) * max_passage_length(301) * embedding_dim (300)\n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "        # answer_outputs 32 * 301 * 600\n",
    "        # answer_outputs -> Linear_layer -> answer_tags\n",
    "        # answer_tags: batch_size(32) * max_passage_length(301) * 1\n",
    "        \n",
    "        outputs = torch.mul(answer_tags.squeeze(2), Variable(torch.from_numpy(mask)).float())\n",
    "        #answer_tags.squeeze(2) -> 32 * 301 * 1 --> 32 * 301\n",
    "        answer_loss = criterion1(outputs, labels.unsqueeze(2).float())\n",
    "            \n",
    "        \n",
    "        question_encoder_input = torch.matmul(Variable(torch.from_numpy(batch_input[3][batch_num])).float(), answer_outputs.float())\n",
    "        # batch_size(32) x max_question_len(15) x embedding_dim(600)\n",
    "        question_encoder_hidden_batch = Variable(torch.zeros(1,current_batch_size,questionEncoder.hidden_size))\n",
    "        # 1 * 32 * 300\n",
    "        \n",
    "        question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "        for i in range(current_batch_size):\n",
    "            _ , question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "            #question encoder hidden -> 1 x 1 x 600\n",
    "            question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden\n",
    "            \n",
    "        question_encoder_hidden_batch = fcLayer(question_encoder_hidden_batch)\n",
    "            \n",
    "        question_loss = 0\n",
    "        if(type(question_decoder_hidden) == Variable):\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        if(type(attn_output) == Variable):\n",
    "            attn_output = repackage_hidden(attn_output)\n",
    "        if(type(attention_hidden) == Variable):\n",
    "            attention_hidden = repackage_hidden(attention_hidden)\n",
    "        if(type(Attention_Weights) == Variable):\n",
    "            Attention_Weights = repackage_hidden(Attention_Weights)\n",
    "        for i in range(current_batch_size):\n",
    "            question_decoder_hidden = question_encoder_hidden_batch[:,i:i+1,:].clone()\n",
    "            # 1 x 1 x 300\n",
    "            embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long())\n",
    "            # 19x300\n",
    "            output_labels = Variable(torch.from_numpy(batch_input[5][batch_num][i]).long()) # fOR NLL Loss\n",
    "            for quesL in range(batch_input[7][batch_num][i]):\n",
    "                #input(), prev_hidden(decoder), Doc_encoder_outputs(hidden)\n",
    "                # 1 x 300    |   300    |    301x600\n",
    "                attn_output, Attention_Weights = attention(question_encoder_hidden_batch[:,i:i+1,:].squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "                decoder_output, attention_hidden = questionDecoder(\n",
    "                    embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "                #question_decoder_hidden -> 1x1x300\n",
    "                #decoder_output -> 1x1x300\n",
    "                \n",
    "                \n",
    "                final_output = questionGenerator(decoder_output)\n",
    "                # 1 x 1 x 2000\n",
    "                output_label = Variable(torch.zeros(1,2000))\n",
    "                output_label[:,batch_input[5][batch_num][i][quesL]] = 1\n",
    "                # 1 x 2000\n",
    "                #Batch[5] -> Question input tokens (num_of_examples * max_question_length)\n",
    "                question_loss += criterion2(final_output.squeeze(0), \n",
    "                                           output_labels[quesL:quesL+1]) # fOR NLL Loss\n",
    "                #question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "        \n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f' \n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "        \n",
    "        \n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))\n",
    "        \n",
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train,max_answer_len),dtype=np.int32)\n",
    "#expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "#expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "attention_hidden = attention.initHidden()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1.post2'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder_hidden_batch[:,0:0+1,:].squeeze(0).squeeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_decoder_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder_hidden_batch[0][i].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Question Loss without attention in 10 epochs on 32 training examples: 3313 --> 3224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.cat((embedded[0], hidden[0]), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_inputs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_label = Variable(torch.zeros(1,2000))\n",
    "output_label[:,batch_input[5][batch_num][i][quesL]] = 1\n",
    "output_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion2(final_output.squeeze(0), output_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cellFlag == 'LSTM':\n",
    "    forward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "elif cellFlag == 'GRU':\n",
    "    forward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "\n",
    "answer_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, document_emb, d_lengths, dtype=tf.float64,\n",
    "    scope=\"answer_rnn\")\n",
    "\n",
    "answer_outputs = tf.concat(answer_outputs, 2, name=\"answer_output_concat\")\n",
    "\n",
    "answer_outputs = tf.cast(answer_outputs,tf.float32, name=\"answer_output_concat\")\n",
    "\n",
    "answer_tags = tf.layers.dense(inputs=answer_outputs, units=2, name=\"answer_tags\")\n",
    "\n",
    "\n",
    "answer_mask = tf.sequence_mask(d_lengths, dtype=tf.float32, name=\"answer_mask\")\n",
    "\n",
    "answer_loss = seq2seq.sequence_loss(\n",
    "    logits=answer_tags, targets=a_labels, weights=answer_mask, name=\"answer_loss\")\n",
    "\n",
    "answer_loss = tf.Print(answer_loss, [answer_loss], message=\"answer_loss: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.matmul(encoder_input_mask, answer_outputs, name=\"encoder_inputs\")\n",
    "encoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"encoder_lengths\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    encoder_cell = tf.contrib.rnn.GRUCell(forward_cell.state_size + backward_cell.state_size)\n",
    "elif cellFlag == 'LSTM':\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs, encoder_lengths, dtype=tf.float32, scope=\"encoder_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "decoder_emb = tf.nn.embedding_lookup(embedding, decoder_inputs,name=\"decoder_embedding\")\n",
    "decoder_emb = tf.cast(decoder_emb,tf.float32,name=\"decoder_embedding_cast\")\n",
    "\n",
    "helper = seq2seq.TrainingHelper(decoder_emb , decoder_lengths, name=\"helper\")\n",
    "\n",
    "\n",
    "projection = Dense(embedding.shape[0], use_bias=False, name=\"projection\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    decoder_cell = tf.contrib.rnn.GRUCell(encoder_cell.state_size)\n",
    "elif cellFlag == \"LSTM\":\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)\n",
    "\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, scope=\"decoder\")\n",
    "decoder_outputs = decoder_outputs.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_state.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLL Loss\n",
    "question_mask = tf.sequence_mask(decoder_lengths ,dtype=tf.float32)\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "question_loss = tf.Print(question_loss, [question_loss], message=\"question_loss: \")\n",
    "\n",
    "#Suppression Loss\n",
    "lambdaSuppress = 1\n",
    "\n",
    "suppression_loss = lambdaSuppress * tf.reduce_sum(tf.matmul(tf.nn.softmax(decoder_outputs), s_answer))\n",
    "suppression_loss = tf.Print(suppression_loss, [suppression_loss], message=\"suppression_loss: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Expression Loss\n",
    "\n",
    "express_loss_f = tf.reduce_sum(-tf.log(tf.multiply(tf.sigmoid(decoder_outputs),e_context) + 10e-7))\n",
    "suppress_loss_f = tf.reduce_sum(-tf.log(tf.multiply((1 - tf.sigmoid(decoder_outputs)),(1 - e_context)) + 10e-7))\n",
    "expression_loss = (express_loss_f + suppress_loss_f)/(32 * 20 * wordToTake)\n",
    "expression_loss = tf.Print(expression_loss, [expression_loss], message=\"expression_loss: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Maximize Entropy Loss\n",
    "entropy_loss = tf.matmul(tf.transpose(dense_output),dense_output)\n",
    "print(dense_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.stack([question_loss,answer_loss,suppression_loss,expression_loss])\n",
    "loss = tf.reduce_sum(tf.multiply(x, lossFlags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_list(*ls):\n",
    "    l =list(zip(*ls))\n",
    "    np.random.shuffle(l)\n",
    "    return zip(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expression_contexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"No of features:\",len( batch_input))\n",
    "print(\"No of batches:\",len( batch_input[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saved_vars = []\n",
    "l = len(tf.all_variables())\n",
    "for i,var in enumerate(tf.all_variables()):\n",
    "    print(i,\"/\",l)\n",
    "    saved_vars.append(var)\n",
    "        \n",
    "print(len(saved_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-3).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.variables_initializer(saved_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "loss_flag = np.array([1,1,1,1])\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    batch_loss = 0\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    start_time = time.time()\n",
    "    for batchNum in range(len(batch_input[0])):\n",
    "        print(\"Batch : \",batchNum)\n",
    "        t = session.run([optimizer, loss, question_loss, answer_loss, suppression_loss, expression_loss], {\n",
    "            d_tokens: batch_input[0][batchNum],\n",
    "            d_lengths: batch_input[1][batchNum],\n",
    "            a_labels: batch_input[2][batchNum],\n",
    "            encoder_input_mask: batch_input[3][batchNum],\n",
    "            encoder_lengths: batch_input[4][batchNum],\n",
    "            decoder_inputs: batch_input[5][batchNum],\n",
    "            decoder_labels: batch_input[6][batchNum],\n",
    "            decoder_lengths: batch_input[7][batchNum],\n",
    "            s_answer: batch_input[8][batchNum],\n",
    "            e_context: batch_input[9][batchNum],\n",
    "            e_probs: batch_input[10][batchNum],\n",
    "            lossFlags : loss_flag,\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(t[1]))\n",
    "        batch_loss += t[2]\n",
    "    batch_loss /= len(batch_input[0])\n",
    "    end_time = time.time()\n",
    "    print(\"Average Batch Question Loss: {0}\".format(batch_loss))\n",
    "    print(\"Time taken to complete epoch : \" , (end_time-start_time)/60 , \" minutes\")\n",
    "    if batch_loss < minQuestionLoss:\n",
    "        print(\"Turning on all losses\")\n",
    "        #loss_flag = np.array([1,1,1,1])\n",
    "    if(epoch%5 == 0):\n",
    "        print(\"Saving model\")\n",
    "        #saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_glove.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[5][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(session, 'qgen-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = session.run(answer_tags, {\n",
    "    d_tokens: batch_input[0][0],\n",
    "    d_lengths: batch_input[1][0],\n",
    "})\n",
    "print(answers.shape)\n",
    "print(answers[0])\n",
    "answers = np.argmax(answers, 2)\n",
    "print(answers.shape)\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(276):\n",
    "    print(\"Prediction\")\n",
    "    printAllAns(answers,2,0)\n",
    "    print(\"Ground Truth\")\n",
    "    printAllAns(batch_input[2][2],2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printDoc(batch,num):\n",
    "    for i in batch_input[0][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "def printQues(batch,num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAnsForQuestion(batch, num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAllAns(answers, batch, num):\n",
    "    for i,word in enumerate(batch_input[0][batch][num]):\n",
    "        if answers[num][i] == 1 :\n",
    "            print(look_up_token_reduced(word),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batchNum = 0\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch_input[0][batchNum].shape[0]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    d_tokens: batch_input[0][batchNum],\n",
    "    d_lengths: batch_input[1][batchNum],\n",
    "    a_labels: batch_input[2][batchNum],\n",
    "    encoder_input_mask: batch_input[3][batchNum],\n",
    "    encoder_lengths: batch_input[4][batchNum],\n",
    "    e_context: batch_input[9][batchNum],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[9][batchNum].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#questions[:,:,END_TOKEN] = 0\n",
    "qs = np.argmax(questions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in np.argsort(questions[0][0])[-100:]:\n",
    "    print(sigmoid(questions[0][0][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.argsort(questions[0][10])[-100:]:\n",
    "    print(sigmoid(questions[0][10][i]),look_up_token_reduced(i), sep=\" \", end= \" \")\n",
    "print(\"\")\n",
    "print(X_train_comp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = 0\n",
    "p2 = 0\n",
    "\n",
    "for i in np.where(batch_input[9][0][0][0] == 1)[0]:\n",
    "    p1 += -np.log(sigmoid(questions[0][0][i]) + 10e-7)\n",
    "    \n",
    "for i in np.where(batch_input[9][0][0][0] == 0)[0]:\n",
    "    p2 += -np.log(sigmoid(questions[0][0][i]) + 10e-7)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p2-p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(batch_input[0][batchNum].shape[0]):\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, qs[i])\n",
    "    print(\"Generated Question: \" + \" \".join(look_up_token_reduced(token) for token in question))\n",
    "    print(\"Ground Truth Question: \")\n",
    "    printQues(batchNum,i)\n",
    "    print(\"Ground Truth Answer: \", X_train_ans_shuffled[batch_size*batchNum + i])\n",
    "    print(\"Context:\")\n",
    "    printDoc(batchNum,i)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[5][18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions[:,0:14,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = tf.constant(batch_input[9][0], dtype=tf.float32)\n",
    "a2 = tf.constant(questions[:,0:14,:], dtype=tf.float32)\n",
    "l1 = tf.reduce_sum(-tf.log(tf.multiply(tf.sigmoid(a2),a1) + 10e-7)) #y = 1\n",
    "l2 = tf.reduce_sum(-tf.log(tf.multiply(tf.sigmoid(1-a2),(1 - a1)) + 10e-7)) #y= 0\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(l1.eval() + l2.eval())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[9][0][0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in batch_input[0][0][0]:\n",
    "    print(look_up_token_reduced(i), sep = \" \", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
