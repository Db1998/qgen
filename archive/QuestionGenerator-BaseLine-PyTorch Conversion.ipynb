{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 50000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "1493\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 1000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 495)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1\n",
    "            \n",
    "    words_to_consider_expression = set(X_train_comp[i] + nltkStopWords + punctuations)\n",
    "\n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_contexts[i,:,look_up_word_reduced(word)] = 1\n",
    "        \n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_probabilities[i,:,look_up_word_reduced(word)] = len(np.where(expression_contexts[i][0] == 1)[0]) / float(wordToTake)\n",
    "    expression_probabilities[i,:,np.where(expression_probabilities[i][0] == 0)[0]] = len(np.where(expression_contexts[i][0] == 0)[0]) / float(wordToTake)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "18192\n",
      "1356\n",
      "17654\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = context_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "495"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def create_batch(inputs,batch_size,shuffle=False):\n",
    "    num_batches = len(inputs[0]) // batch_size + 1\n",
    "    outputs = []\n",
    "    for index,inp in enumerate(inputs):\n",
    "    \n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_masks': [],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[],\n",
    "                    'suppression_answer':[],\n",
    "                    'expression_contexts': [],\n",
    "                    'expression_probabilities':[]}\n",
    "    \n",
    "        start = 0\n",
    "        for i in range(num_batches):\n",
    "            if i == num_batches - 1:\n",
    "                end = None\n",
    "            else:\n",
    "                end = start+batch_size\n",
    "            #maxD = max(inputs[1][start:end])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[4][start:end])\n",
    "            maxQ = max(inputs[7][start:end])\n",
    "            if index == 0:\n",
    "                outputs['document_tokens'] = inp[start:end,:maxD]\n",
    "            elif index==1:\n",
    "                outputs['document_lengths'] = inp[start:end]\n",
    "            elif index == 2:\n",
    "                outputs['answer_labels']=inp[start:end,:maxD]\n",
    "            elif index==3:\n",
    "                outputs['answer_masks']=inp[start:end,:maxA,:maxD]\n",
    "            elif index==4:\n",
    "                outputs['answer_lengths']=inp[start:end]\n",
    "            elif index==5:\n",
    "                output['question_input_tokens']=inp[start:end,:maxQ]\n",
    "            elif index==6:\n",
    "                output['question_output_tokens']=inp[start:end,:maxQ]\n",
    "            elif index==7:\n",
    "                output['question_lengths'] = inp[start:end]\n",
    "            elif index==8:\n",
    "                output['suppression_answer'] = inp[start:end]\n",
    "            elif index==9:\n",
    "                output['expression_contexts'] = inp[start:end,0:maxQ,:]\n",
    "            elif index==10: \n",
    "                output['expression_probabilities'] = inp[start:end,0:maxQ,:]\n",
    "            start = start + batch_size\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            #maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index==9 or index==10:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ,:])\n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        #maxD = max(inputs[1][start:])\n",
    "        maxD = max_document_len\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ])\n",
    "        elif index==9 or index==10:\n",
    "            output.append(inp[start:,0:maxQ,:]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches =  32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer,expression_contexts,expression_probabilities]\n",
    "                    ,batch_size)\n",
    "number_of_batches = len(batch_input[0])\n",
    "print(\"Number of batches = \", number_of_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        \n",
    "        # TODO: Verify\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(AnswerEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True, bidirectional=True) #Input_size = Hidden_Size\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        self.hiddenState = hidden\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, batch_size, self.hidden_size)) #2 for BiDirectional\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result\n",
    "    \n",
    "'''\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "'''\n",
    "\n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, max_document_len, dropout_p=0.1):\n",
    "        \n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_document_len = max_document_len\n",
    "\n",
    "        self.attn_reduce = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.max_document_len + self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.attn_reduce = self.attn_reduce.cuda()\n",
    "            self.attn_combine = self.attn_combine.cuda()\n",
    "            self.dropout = self.dropout.cuda()\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.dropout(hidden)\n",
    "        concat = torch.cat((input.unsqueeze(0), hidden.unsqueeze(0)), 1)\n",
    "        concat_reduced = self.attn_reduce(concat)\n",
    "        attn_weights = F.softmax(concat_reduced, dim=1)\n",
    "        attn_applied = torch.mm(encoder_outputs, attn_weights.squeeze(0).unsqueeze(1))\n",
    "        output = torch.cat((hidden.unsqueeze(0), attn_applied.squeeze(1).unsqueeze(0)), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        return output, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters =  24\n"
     ]
    }
   ],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "#fcLayer = FCLayer(hidden_size, hidden_size)\n",
    "answerEncoder = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention = AttnDecoderRNN(hidden_size= hidden_size,max_document_len = max_document_len)\n",
    "\n",
    "answerEncoder.train()\n",
    "questionEncoder.train()\n",
    "questionDecoder.train()\n",
    "questionGenerator.train()\n",
    "attention.train()\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [answerEncoder, questionEncoder, questionDecoder, questionGenerator, attention]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 0.01)\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "#criterion2 = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 1\tNet Loss: 4331.4443 \tAnswer Loss: 0.1946 \tQuestion Loss: 4331.2500\n",
      "Batch: 1 \t Epoch : 1\tNet Loss: 4048.8394 \tAnswer Loss: 0.2927 \tQuestion Loss: 4048.5466\n",
      "Batch: 2 \t Epoch : 1\tNet Loss: 3249.6189 \tAnswer Loss: 0.2281 \tQuestion Loss: 3249.3909\n",
      "Batch: 3 \t Epoch : 1\tNet Loss: 6511.0171 \tAnswer Loss: 0.2667 \tQuestion Loss: 6510.7505\n",
      "Batch: 4 \t Epoch : 1\tNet Loss: 6210.5737 \tAnswer Loss: 0.3230 \tQuestion Loss: 6210.2510\n",
      "Batch: 5 \t Epoch : 1\tNet Loss: 3065.8022 \tAnswer Loss: 0.2933 \tQuestion Loss: 3065.5090\n",
      "Batch: 6 \t Epoch : 1\tNet Loss: 5012.9780 \tAnswer Loss: 0.3228 \tQuestion Loss: 5012.6553\n",
      "Batch: 7 \t Epoch : 1\tNet Loss: 3935.1260 \tAnswer Loss: 0.2287 \tQuestion Loss: 3934.8972\n",
      "Batch: 8 \t Epoch : 1\tNet Loss: 2470.7983 \tAnswer Loss: 0.1509 \tQuestion Loss: 2470.6475\n",
      "Batch: 9 \t Epoch : 1\tNet Loss: 2613.9558 \tAnswer Loss: 0.2740 \tQuestion Loss: 2613.6819\n",
      "Batch: 10 \t Epoch : 1\tNet Loss: 2279.8730 \tAnswer Loss: 0.3314 \tQuestion Loss: 2279.5417\n",
      "Batch: 11 \t Epoch : 1\tNet Loss: 2676.0503 \tAnswer Loss: 0.4266 \tQuestion Loss: 2675.6238\n",
      "Batch: 12 \t Epoch : 1\tNet Loss: 2432.1523 \tAnswer Loss: 0.1914 \tQuestion Loss: 2431.9609\n",
      "Batch: 13 \t Epoch : 1\tNet Loss: 2518.4531 \tAnswer Loss: 0.3251 \tQuestion Loss: 2518.1279\n",
      "Batch: 14 \t Epoch : 1\tNet Loss: 2308.8667 \tAnswer Loss: 0.2806 \tQuestion Loss: 2308.5862\n",
      "Batch: 15 \t Epoch : 1\tNet Loss: 2267.0237 \tAnswer Loss: 0.2023 \tQuestion Loss: 2266.8213\n",
      "Batch: 16 \t Epoch : 1\tNet Loss: 2117.9258 \tAnswer Loss: 0.2833 \tQuestion Loss: 2117.6426\n",
      "Batch: 17 \t Epoch : 1\tNet Loss: 2481.0132 \tAnswer Loss: 0.2226 \tQuestion Loss: 2480.7905\n",
      "Batch: 18 \t Epoch : 1\tNet Loss: 2308.6951 \tAnswer Loss: 0.2457 \tQuestion Loss: 2308.4495\n",
      "Batch: 19 \t Epoch : 1\tNet Loss: 3220.6770 \tAnswer Loss: 0.2269 \tQuestion Loss: 3220.4502\n",
      "Batch: 20 \t Epoch : 1\tNet Loss: 1871.8462 \tAnswer Loss: 0.2427 \tQuestion Loss: 1871.6035\n",
      "Batch: 21 \t Epoch : 1\tNet Loss: 2114.7808 \tAnswer Loss: 0.3376 \tQuestion Loss: 2114.4431\n",
      "Batch: 22 \t Epoch : 1\tNet Loss: 2184.1326 \tAnswer Loss: 0.2669 \tQuestion Loss: 2183.8657\n",
      "Batch: 23 \t Epoch : 1\tNet Loss: 1941.3314 \tAnswer Loss: 0.2386 \tQuestion Loss: 1941.0928\n",
      "Batch: 24 \t Epoch : 1\tNet Loss: 1949.1682 \tAnswer Loss: 0.2659 \tQuestion Loss: 1948.9022\n",
      "Batch: 25 \t Epoch : 1\tNet Loss: 1876.3060 \tAnswer Loss: 0.2831 \tQuestion Loss: 1876.0229\n",
      "Batch: 26 \t Epoch : 1\tNet Loss: 1991.7412 \tAnswer Loss: 0.2376 \tQuestion Loss: 1991.5037\n",
      "Batch: 27 \t Epoch : 1\tNet Loss: 1823.6339 \tAnswer Loss: 0.2785 \tQuestion Loss: 1823.3555\n",
      "Batch: 28 \t Epoch : 1\tNet Loss: 1740.7068 \tAnswer Loss: 0.3284 \tQuestion Loss: 1740.3783\n",
      "Batch: 29 \t Epoch : 1\tNet Loss: 1835.0455 \tAnswer Loss: 0.2450 \tQuestion Loss: 1834.8005\n",
      "Batch: 30 \t Epoch : 1\tNet Loss: 1878.0790 \tAnswer Loss: 0.2341 \tQuestion Loss: 1877.8448\n",
      "Average Loss after Epoch 1 : 2727.1142\n",
      "Batch: 0 \t Epoch : 2\tNet Loss: 1537.4113 \tAnswer Loss: 0.1382 \tQuestion Loss: 1537.2731\n",
      "Batch: 1 \t Epoch : 2\tNet Loss: 1553.7559 \tAnswer Loss: 0.2679 \tQuestion Loss: 1553.4879\n",
      "Batch: 2 \t Epoch : 2\tNet Loss: 1427.0400 \tAnswer Loss: 0.2131 \tQuestion Loss: 1426.8269\n",
      "Batch: 3 \t Epoch : 2\tNet Loss: 1432.1936 \tAnswer Loss: 0.2413 \tQuestion Loss: 1431.9523\n",
      "Batch: 4 \t Epoch : 2\tNet Loss: 1264.8739 \tAnswer Loss: 0.2719 \tQuestion Loss: 1264.6021\n",
      "Batch: 5 \t Epoch : 2\tNet Loss: 1514.3250 \tAnswer Loss: 0.2417 \tQuestion Loss: 1514.0833\n",
      "Batch: 6 \t Epoch : 2\tNet Loss: 1458.1564 \tAnswer Loss: 0.3032 \tQuestion Loss: 1457.8531\n",
      "Batch: 7 \t Epoch : 2\tNet Loss: 1415.2106 \tAnswer Loss: 0.2234 \tQuestion Loss: 1414.9872\n",
      "Batch: 8 \t Epoch : 2\tNet Loss: 1136.5638 \tAnswer Loss: 0.1478 \tQuestion Loss: 1136.4161\n",
      "Batch: 9 \t Epoch : 2\tNet Loss: 1228.8804 \tAnswer Loss: 0.2645 \tQuestion Loss: 1228.6158\n",
      "Batch: 10 \t Epoch : 2\tNet Loss: 1015.9457 \tAnswer Loss: 0.3284 \tQuestion Loss: 1015.6174\n",
      "Batch: 11 \t Epoch : 2\tNet Loss: 1255.3466 \tAnswer Loss: 0.4254 \tQuestion Loss: 1254.9211\n",
      "Batch: 12 \t Epoch : 2\tNet Loss: 1152.3641 \tAnswer Loss: 0.1900 \tQuestion Loss: 1152.1742\n",
      "Batch: 13 \t Epoch : 2\tNet Loss: 1132.7449 \tAnswer Loss: 0.3261 \tQuestion Loss: 1132.4188\n",
      "Batch: 14 \t Epoch : 2\tNet Loss: 991.4779 \tAnswer Loss: 0.2654 \tQuestion Loss: 991.2125\n",
      "Batch: 15 \t Epoch : 2\tNet Loss: 1011.3215 \tAnswer Loss: 0.1933 \tQuestion Loss: 1011.1281\n",
      "Batch: 16 \t Epoch : 2\tNet Loss: 969.9951 \tAnswer Loss: 0.2810 \tQuestion Loss: 969.7141\n",
      "Batch: 17 \t Epoch : 2\tNet Loss: 1146.0808 \tAnswer Loss: 0.2190 \tQuestion Loss: 1145.8618\n",
      "Batch: 18 \t Epoch : 2\tNet Loss: 1008.2457 \tAnswer Loss: 0.2382 \tQuestion Loss: 1008.0074\n",
      "Batch: 19 \t Epoch : 2\tNet Loss: 1100.8413 \tAnswer Loss: 0.2242 \tQuestion Loss: 1100.6171\n",
      "Batch: 20 \t Epoch : 2\tNet Loss: 813.4931 \tAnswer Loss: 0.2390 \tQuestion Loss: 813.2541\n",
      "Batch: 21 \t Epoch : 2\tNet Loss: 894.2493 \tAnswer Loss: 0.3359 \tQuestion Loss: 893.9134\n",
      "Batch: 22 \t Epoch : 2\tNet Loss: 882.1553 \tAnswer Loss: 0.2638 \tQuestion Loss: 881.8914\n",
      "Batch: 23 \t Epoch : 2\tNet Loss: 796.2675 \tAnswer Loss: 0.2353 \tQuestion Loss: 796.0322\n",
      "Batch: 24 \t Epoch : 2\tNet Loss: 784.1477 \tAnswer Loss: 0.2625 \tQuestion Loss: 783.8852\n",
      "Batch: 25 \t Epoch : 2\tNet Loss: 816.2902 \tAnswer Loss: 0.2825 \tQuestion Loss: 816.0078\n",
      "Batch: 26 \t Epoch : 2\tNet Loss: 836.1920 \tAnswer Loss: 0.2341 \tQuestion Loss: 835.9579\n",
      "Batch: 27 \t Epoch : 2\tNet Loss: 746.0984 \tAnswer Loss: 0.2725 \tQuestion Loss: 745.8259\n",
      "Batch: 28 \t Epoch : 2\tNet Loss: 742.0402 \tAnswer Loss: 0.3259 \tQuestion Loss: 741.7143\n",
      "Batch: 29 \t Epoch : 2\tNet Loss: 739.7596 \tAnswer Loss: 0.2430 \tQuestion Loss: 739.5167\n",
      "Batch: 30 \t Epoch : 2\tNet Loss: 747.7052 \tAnswer Loss: 0.2323 \tQuestion Loss: 747.4728\n",
      "Average Loss after Epoch 2 : 1048.4742\n",
      "Batch: 0 \t Epoch : 3\tNet Loss: 680.0359 \tAnswer Loss: 0.1324 \tQuestion Loss: 679.9035\n",
      "Batch: 1 \t Epoch : 3\tNet Loss: 861.5213 \tAnswer Loss: 0.2663 \tQuestion Loss: 861.2550\n",
      "Batch: 2 \t Epoch : 3\tNet Loss: 834.7755 \tAnswer Loss: 0.2123 \tQuestion Loss: 834.5632\n",
      "Batch: 3 \t Epoch : 3\tNet Loss: 872.4851 \tAnswer Loss: 0.2383 \tQuestion Loss: 872.2468\n",
      "Batch: 4 \t Epoch : 3\tNet Loss: 815.3497 \tAnswer Loss: 0.2719 \tQuestion Loss: 815.0778\n",
      "Batch: 5 \t Epoch : 3\tNet Loss: 693.6864 \tAnswer Loss: 0.2404 \tQuestion Loss: 693.4460\n",
      "Batch: 6 \t Epoch : 3\tNet Loss: 605.4870 \tAnswer Loss: 0.3020 \tQuestion Loss: 605.1851\n",
      "Batch: 7 \t Epoch : 3\tNet Loss: 678.4519 \tAnswer Loss: 0.2241 \tQuestion Loss: 678.2278\n",
      "Batch: 8 \t Epoch : 3\tNet Loss: 441.0945 \tAnswer Loss: 0.1462 \tQuestion Loss: 440.9483\n",
      "Batch: 9 \t Epoch : 3\tNet Loss: 502.7521 \tAnswer Loss: 0.2611 \tQuestion Loss: 502.4909\n",
      "Batch: 10 \t Epoch : 3\tNet Loss: 416.8587 \tAnswer Loss: 0.3264 \tQuestion Loss: 416.5323\n",
      "Batch: 11 \t Epoch : 3\tNet Loss: 507.7400 \tAnswer Loss: 0.4241 \tQuestion Loss: 507.3159\n",
      "Batch: 12 \t Epoch : 3\tNet Loss: 481.8104 \tAnswer Loss: 0.1885 \tQuestion Loss: 481.6219\n",
      "Batch: 13 \t Epoch : 3\tNet Loss: 455.7713 \tAnswer Loss: 0.3233 \tQuestion Loss: 455.4481\n",
      "Batch: 14 \t Epoch : 3\tNet Loss: 408.7733 \tAnswer Loss: 0.2645 \tQuestion Loss: 408.5088\n",
      "Batch: 15 \t Epoch : 3\tNet Loss: 410.8945 \tAnswer Loss: 0.1921 \tQuestion Loss: 410.7024\n",
      "Batch: 16 \t Epoch : 3\tNet Loss: 369.1873 \tAnswer Loss: 0.2796 \tQuestion Loss: 368.9077\n",
      "Batch: 17 \t Epoch : 3\tNet Loss: 438.5929 \tAnswer Loss: 0.2165 \tQuestion Loss: 438.3764\n",
      "Batch: 18 \t Epoch : 3\tNet Loss: 388.8217 \tAnswer Loss: 0.2367 \tQuestion Loss: 388.5850\n",
      "Batch: 19 \t Epoch : 3\tNet Loss: 387.5494 \tAnswer Loss: 0.2225 \tQuestion Loss: 387.3269\n",
      "Batch: 20 \t Epoch : 3\tNet Loss: 327.4456 \tAnswer Loss: 0.2381 \tQuestion Loss: 327.2075\n",
      "Batch: 21 \t Epoch : 3\tNet Loss: 377.7672 \tAnswer Loss: 0.3349 \tQuestion Loss: 377.4323\n",
      "Batch: 22 \t Epoch : 3\tNet Loss: 368.5918 \tAnswer Loss: 0.2624 \tQuestion Loss: 368.3294\n",
      "Batch: 23 \t Epoch : 3\tNet Loss: 316.8104 \tAnswer Loss: 0.2342 \tQuestion Loss: 316.5762\n",
      "Batch: 24 \t Epoch : 3\tNet Loss: 327.8857 \tAnswer Loss: 0.2617 \tQuestion Loss: 327.6240\n",
      "Batch: 25 \t Epoch : 3\tNet Loss: 306.1789 \tAnswer Loss: 0.2819 \tQuestion Loss: 305.8971\n",
      "Batch: 26 \t Epoch : 3\tNet Loss: 320.7441 \tAnswer Loss: 0.2336 \tQuestion Loss: 320.5106\n",
      "Batch: 27 \t Epoch : 3\tNet Loss: 244.9557 \tAnswer Loss: 0.2716 \tQuestion Loss: 244.6841\n",
      "Batch: 28 \t Epoch : 3\tNet Loss: 238.7323 \tAnswer Loss: 0.3260 \tQuestion Loss: 238.4064\n",
      "Batch: 29 \t Epoch : 3\tNet Loss: 234.3726 \tAnswer Loss: 0.2418 \tQuestion Loss: 234.1308\n",
      "Batch: 30 \t Epoch : 3\tNet Loss: 268.9042 \tAnswer Loss: 0.2317 \tQuestion Loss: 268.6725\n",
      "Average Loss after Epoch 3 : 455.7509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 4\tNet Loss: 163.9716 \tAnswer Loss: 0.1310 \tQuestion Loss: 163.8406\n",
      "Batch: 1 \t Epoch : 4\tNet Loss: 324.5998 \tAnswer Loss: 0.2668 \tQuestion Loss: 324.3329\n",
      "Batch: 2 \t Epoch : 4\tNet Loss: 370.7748 \tAnswer Loss: 0.2145 \tQuestion Loss: 370.5604\n",
      "Batch: 3 \t Epoch : 4\tNet Loss: 323.2723 \tAnswer Loss: 0.2379 \tQuestion Loss: 323.0344\n",
      "Batch: 4 \t Epoch : 4\tNet Loss: 272.5392 \tAnswer Loss: 0.2737 \tQuestion Loss: 272.2656\n",
      "Batch: 5 \t Epoch : 4\tNet Loss: 215.4164 \tAnswer Loss: 0.2423 \tQuestion Loss: 215.1741\n",
      "Batch: 6 \t Epoch : 4\tNet Loss: 186.8780 \tAnswer Loss: 0.2999 \tQuestion Loss: 186.5781\n",
      "Batch: 7 \t Epoch : 4\tNet Loss: 243.7474 \tAnswer Loss: 0.2251 \tQuestion Loss: 243.5223\n",
      "Batch: 8 \t Epoch : 4\tNet Loss: 150.5847 \tAnswer Loss: 0.1463 \tQuestion Loss: 150.4384\n",
      "Batch: 9 \t Epoch : 4\tNet Loss: 116.5801 \tAnswer Loss: 0.2618 \tQuestion Loss: 116.3183\n",
      "Batch: 10 \t Epoch : 4\tNet Loss: 107.7460 \tAnswer Loss: 0.3266 \tQuestion Loss: 107.4195\n",
      "Batch: 11 \t Epoch : 4\tNet Loss: 125.4510 \tAnswer Loss: 0.4251 \tQuestion Loss: 125.0259\n",
      "Batch: 12 \t Epoch : 4\tNet Loss: 129.5747 \tAnswer Loss: 0.1878 \tQuestion Loss: 129.3869\n",
      "Batch: 13 \t Epoch : 4\tNet Loss: 142.8521 \tAnswer Loss: 0.3243 \tQuestion Loss: 142.5278\n",
      "Batch: 14 \t Epoch : 4\tNet Loss: 104.4309 \tAnswer Loss: 0.2639 \tQuestion Loss: 104.1669\n",
      "Batch: 15 \t Epoch : 4\tNet Loss: 111.0561 \tAnswer Loss: 0.1922 \tQuestion Loss: 110.8640\n",
      "Batch: 16 \t Epoch : 4\tNet Loss: 99.2995 \tAnswer Loss: 0.2802 \tQuestion Loss: 99.0193\n",
      "Batch: 17 \t Epoch : 4\tNet Loss: 116.2155 \tAnswer Loss: 0.2155 \tQuestion Loss: 116.0000\n",
      "Batch: 18 \t Epoch : 4\tNet Loss: 128.6900 \tAnswer Loss: 0.2370 \tQuestion Loss: 128.4530\n",
      "Batch: 19 \t Epoch : 4\tNet Loss: 169.3728 \tAnswer Loss: 0.2239 \tQuestion Loss: 169.1489\n",
      "Batch: 20 \t Epoch : 4\tNet Loss: 77.1652 \tAnswer Loss: 0.2384 \tQuestion Loss: 76.9268\n",
      "Batch: 21 \t Epoch : 4\tNet Loss: 69.4722 \tAnswer Loss: 0.3356 \tQuestion Loss: 69.1366\n",
      "Batch: 22 \t Epoch : 4\tNet Loss: 71.7514 \tAnswer Loss: 0.2626 \tQuestion Loss: 71.4888\n",
      "Batch: 23 \t Epoch : 4\tNet Loss: 57.3225 \tAnswer Loss: 0.2351 \tQuestion Loss: 57.0874\n",
      "Batch: 24 \t Epoch : 4\tNet Loss: 70.7256 \tAnswer Loss: 0.2611 \tQuestion Loss: 70.4645\n",
      "Batch: 25 \t Epoch : 4\tNet Loss: 58.7554 \tAnswer Loss: 0.2808 \tQuestion Loss: 58.4745\n",
      "Batch: 26 \t Epoch : 4\tNet Loss: 61.5496 \tAnswer Loss: 0.2344 \tQuestion Loss: 61.3152\n",
      "Batch: 27 \t Epoch : 4\tNet Loss: 42.1573 \tAnswer Loss: 0.2735 \tQuestion Loss: 41.8839\n",
      "Batch: 28 \t Epoch : 4\tNet Loss: 51.2684 \tAnswer Loss: 0.3277 \tQuestion Loss: 50.9407\n",
      "Batch: 29 \t Epoch : 4\tNet Loss: 35.4560 \tAnswer Loss: 0.2427 \tQuestion Loss: 35.2133\n",
      "Batch: 30 \t Epoch : 4\tNet Loss: 37.5253 \tAnswer Loss: 0.2327 \tQuestion Loss: 37.2926\n",
      "Average Loss after Epoch 4 : 132.3813\n",
      "Batch: 0 \t Epoch : 5\tNet Loss: 41.1293 \tAnswer Loss: 0.1324 \tQuestion Loss: 40.9970\n",
      "Batch: 1 \t Epoch : 5\tNet Loss: 57.6021 \tAnswer Loss: 0.2669 \tQuestion Loss: 57.3352\n",
      "Batch: 2 \t Epoch : 5\tNet Loss: 97.6549 \tAnswer Loss: 0.2162 \tQuestion Loss: 97.4387\n",
      "Batch: 3 \t Epoch : 5\tNet Loss: 68.6402 \tAnswer Loss: 0.2385 \tQuestion Loss: 68.4018\n",
      "Batch: 4 \t Epoch : 5\tNet Loss: 81.2219 \tAnswer Loss: 0.2735 \tQuestion Loss: 80.9484\n",
      "Batch: 5 \t Epoch : 5\tNet Loss: 59.5808 \tAnswer Loss: 0.2422 \tQuestion Loss: 59.3386\n",
      "Batch: 6 \t Epoch : 5\tNet Loss: 30.9142 \tAnswer Loss: 0.2991 \tQuestion Loss: 30.6151\n",
      "Batch: 7 \t Epoch : 5\tNet Loss: 37.2797 \tAnswer Loss: 0.2240 \tQuestion Loss: 37.0558\n",
      "Batch: 8 \t Epoch : 5\tNet Loss: 20.3483 \tAnswer Loss: 0.1457 \tQuestion Loss: 20.2026\n",
      "Batch: 9 \t Epoch : 5\tNet Loss: 47.6923 \tAnswer Loss: 0.2614 \tQuestion Loss: 47.4309\n",
      "Batch: 10 \t Epoch : 5\tNet Loss: 48.7515 \tAnswer Loss: 0.3256 \tQuestion Loss: 48.4259\n",
      "Batch: 11 \t Epoch : 5\tNet Loss: 31.6867 \tAnswer Loss: 0.4241 \tQuestion Loss: 31.2626\n",
      "Batch: 12 \t Epoch : 5\tNet Loss: 26.2248 \tAnswer Loss: 0.1882 \tQuestion Loss: 26.0366\n",
      "Batch: 13 \t Epoch : 5\tNet Loss: 32.4357 \tAnswer Loss: 0.3238 \tQuestion Loss: 32.1119\n",
      "Batch: 14 \t Epoch : 5\tNet Loss: 13.8166 \tAnswer Loss: 0.2635 \tQuestion Loss: 13.5531\n",
      "Batch: 15 \t Epoch : 5\tNet Loss: 14.5279 \tAnswer Loss: 0.1919 \tQuestion Loss: 14.3359\n",
      "Batch: 16 \t Epoch : 5\tNet Loss: 18.6276 \tAnswer Loss: 0.2805 \tQuestion Loss: 18.3472\n",
      "Batch: 17 \t Epoch : 5\tNet Loss: 24.5755 \tAnswer Loss: 0.2151 \tQuestion Loss: 24.3604\n",
      "Batch: 18 \t Epoch : 5\tNet Loss: 28.0453 \tAnswer Loss: 0.2366 \tQuestion Loss: 27.8088\n",
      "Batch: 19 \t Epoch : 5\tNet Loss: 9.4357 \tAnswer Loss: 0.2233 \tQuestion Loss: 9.2124\n",
      "Batch: 20 \t Epoch : 5\tNet Loss: 21.8804 \tAnswer Loss: 0.2379 \tQuestion Loss: 21.6425\n"
     ]
    }
   ],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "\n",
    "if use_attention:\n",
    "    Attention_Weights = None\n",
    "    attn_output = None\n",
    "    attention_hidden = attention.initHidden()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "\n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "\n",
    "\n",
    "        #maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        maxDocLenForBatch = max_document_len\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp).cuda()\n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "\n",
    "\n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "\n",
    "\n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "\n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "\n",
    "\n",
    "        t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "        if use_cuda:\n",
    "            t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "        question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "        question_encoder_hidden_batch = Variable(torch.zeros(1,current_batch_size,questionEncoder.hidden_size))\n",
    "        if use_cuda:\n",
    "            question_encoder_hidden_batch = question_encoder_hidden_batch.cuda()\n",
    "\n",
    "        question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "        for i in range(current_batch_size):\n",
    "            _ , question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "            question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden\n",
    "\n",
    "        if type(question_decoder_hidden) == Variable:\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        if(type(attn_output) == Variable):\n",
    "            attn_output = repackage_hidden(attn_output)\n",
    "        if(type(attention_hidden) == Variable):\n",
    "            attention_hidden = repackage_hidden(attention_hidden)\n",
    "        if(type(Attention_Weights) == Variable):\n",
    "            Attention_Weights = repackage_hidden(Attention_Weights)\n",
    "        \n",
    "        question_loss = 0\n",
    "        for i in range(current_batch_size):\n",
    "            question_decoder_hidden = question_encoder_hidden_batch[:,i:i+1,:].clone()\n",
    "            embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long()).cuda()\n",
    "            output_labels = Variable(torch.from_numpy(batch_input[5][batch_num][i]).long())\n",
    "            if use_cuda:\n",
    "                output_labels = output_labels.cuda()\n",
    "\n",
    "            for quesL in range(batch_input[7][batch_num][i]):\n",
    "                if use_attention:\n",
    "                    attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "                    decoder_output, attention_hidden = questionDecoder(\n",
    "                        embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "                else:\n",
    "                    decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                        embedded_inputs[quesL:quesL+1].unsqueeze(1),\n",
    "                        question_decoder_hidden)\n",
    "\n",
    "                final_output = questionGenerator(decoder_output)\n",
    "                output_label = Variable(torch.zeros(1,reduced_glove.shape[0]))\n",
    "                if use_cuda:\n",
    "                    output_label = output_label.cuda()\n",
    "                output_label[:,batch_input[5][batch_num][i][quesL]] = 1\n",
    "                question_loss += criterion2(final_output.squeeze(0),\n",
    "                                           output_labels[quesL:quesL+1])\n",
    "                ##question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "\n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f'\n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 495, 1]), torch.Size([32, 224]))"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_tags.shape, t_document_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/data/ra2630/qgen_base_40k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerEncoder2 = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder2 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder2 = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator2 = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "optimizer2 = torch.optim.Adam(train_param, 0.01)\n",
    "\n",
    "#answerEncoder2.load_state_dict(checkpoint[\"answerEncoder\"])\n",
    "#questionEncoder2.load_state_dict(checkpoint[\"questionEncoder\"])\n",
    "#questionDecoder2.load_state_dict(checkpoint[\"questionDecoder\"])\n",
    "#questionGenerator2.load_state_dict(checkpoint[\"questionGenerator\"])\n",
    "#optimizer2.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "answerEncoder2 = answerEncoder\n",
    "questionEncoder2 = questionEncoder\n",
    "questionDecoder2 = questionDecoder\n",
    "questionGenerator2 = questionGenerator\n",
    "optimizer2 = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedAnswerOutputs = answerEncoder(embedded_inp[:,:,:], answerEncoder.initHidden())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 6, 19, 20, 21, 22, 23, 24, 41]), array([0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(encodedAnswerOutputs[0][0].data > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 5,  6, 19, 20, 21, 22, 23, 24, 34, 35]),)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(batch_input[2][0][0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printQuestion(batch_num, example_num):\n",
    "    for i in batch_input[6][batch_num][example_num]:\n",
    "        print(look_up_token_reduced(i), end = ' ', sep = ' ')\n",
    "    print(\"\")\n",
    "        \n",
    "def printAnswer(batch_num, example_num):\n",
    "    for i in batch_input[3][batch_num][example_num][0 : batch_input[4][batch_num][example_num]]:\n",
    "        for j in (np.where(i==1)[0]):\n",
    "            print(look_up_token_reduced(batch_input[0][batch_num][example_num][j]))\n",
    "    print(\"\")\n",
    "        \n",
    "def printContext(batch_num, example_num):\n",
    "    for i in batch_input[0][batch_num][example_num]:\n",
    "        print(look_up_token_reduced(i), end = ' ', sep = ' ')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 0\n",
    "example_num=1\n",
    "print(\"Context : \")\n",
    "printContext(batch_num, example_num)\n",
    "print(\"Question : \")\n",
    "printQuestion(batch_num, example_num)\n",
    "print(\"Answer : \")\n",
    "printAnswer(batch_num, example_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_token = START_TOKEN\n",
    "question_token_embedding = embedder(Variable(torch.from_numpy(np.array([question_token])).long())).cuda()\n",
    "qLen = 0\n",
    "while qLen <= 20:\n",
    "    decoder_output, question_decoder_hidden = questionDecoder(\n",
    "        question_token_embedding.unsqueeze(1),\n",
    "        question_decoder_hidden)\n",
    "    final_output = questionGenerator(decoder_output)\n",
    "    question_token = np.argmax(final_output.data)\n",
    "    print(question_token)\n",
    "    question_token_embedding = embedder(Variable(torch.from_numpy(np.array([question_token])).long())).cuda()\n",
    "    qLen=qLen + 1\n",
    "    print(look_up_token_reduced(question_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch_number, example_number, context, answer = None):\n",
    "\n",
    "    inp = Variable(torch.from_numpy(batch_input[0][batch_number][example_number]).long())\n",
    "    embedded_inp = embedder(inp).cuda()\n",
    "    answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "    answer_tags, answer_outputs, _ = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "        answer_outputs = answer_outputs.cuda()\n",
    "        answer_tags = answer_tags.cuda()\n",
    "\n",
    "\n",
    "    t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "    if use_cuda:\n",
    "        t_document_mask = t_document_mask.cuda()\n",
    "    outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "\n",
    "    answer_loss = criterion1(outputs, labels.float())\n",
    "\n",
    "\n",
    "    t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "    if use_cuda:\n",
    "        t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "    question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "    question_encoder_hidden_batch = Variable(torch.zeros(1,current_batch_size,questionEncoder.hidden_size))\n",
    "    if use_cuda:\n",
    "        question_encoder_hidden_batch = question_encoder_hidden_batch.cuda()\n",
    "\n",
    "    question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "    for i in range(current_batch_size):\n",
    "        _ , question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "        question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden\n",
    "\n",
    "    if type(question_decoder_hidden) == Variable:\n",
    "        question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "    question_loss = 0\n",
    "    for i in range(current_batch_size):\n",
    "        question_decoder_hidden = question_encoder_hidden_batch[:,i:i+1,:].clone()\n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long()).cuda()\n",
    "        output_labels = Variable(torch.from_numpy(batch_input[5][batch_num][i]).long())\n",
    "        if use_cuda:\n",
    "            output_labels = output_labels.cuda()\n",
    "\n",
    "        for quesL in range(batch_input[7][batch_num][i]):\n",
    "            decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                embedded_inputs[quesL:quesL+1].unsqueeze(1),\n",
    "                question_decoder_hidden)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output)\n",
    "            output_label = Variable(torch.zeros(1,reduced_glove.shape[0]))\n",
    "            if use_cuda:\n",
    "                output_label = output_label.cuda()\n",
    "            output_label[:,batch_input[5][batch_num][i][quesL]] = 1\n",
    "            question_loss += criterion2(final_output.squeeze(0),\n",
    "                                       output_labels[quesL:quesL+1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##########ONly Batch\n",
    "\n",
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "        \n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "\n",
    "            \n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        ################################### Answer Encoder + Tagging    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        if use_cuda:\n",
    "            embedded_inp = embedder(inp).cuda()\n",
    "\n",
    "            \n",
    "        \n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "\n",
    "        #answer_tags.requires_grad=False\n",
    "        #answer_outputs.requires_grad=False\n",
    "        \n",
    "        \n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        #t_document_mask.requires_grade = False\n",
    "        \n",
    "        outputs = answer_tags.squeeze(-1) * t_document_mask\n",
    "        #outputs.requires_grad = False\n",
    "        \n",
    "        \n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "        ################################### Q Encoder    \n",
    "        \n",
    "        t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "        if use_cuda:\n",
    "            t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "        #t_answer_mask.requires_gradui = False\n",
    "        \n",
    "        # masking the non-answer embeddings\n",
    "        question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "        #question_encoder_input.requires_grad = False\n",
    "        question_encoder_output = Variable(torch.zeros(1,batch_size,questionEncoder.hidden_size))\n",
    "        #question_encoder_output.requires_grad = False\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            question_encoder_hidden = question_encoder_hidden_batch.cuda()\n",
    "            \n",
    "        question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "        question_encoder_output, question_encoder_hidden = questionEncoder(question_encoder_input, question_encoder_hidden)\n",
    "        \n",
    "        #question_encoder_output.requires_grad = False\n",
    "\n",
    "        \n",
    "#         question_encoder_output = Variable(torch.zeros(batch_size, max_question_len, questionEncoder.hidden_size))\n",
    "#         question_encoder_hidden = Variable(torch.zeros(batch_size, max_question_len, questionEncoder.hidden_size))\n",
    "        \n",
    "#         for i in range(current_batch_size):\n",
    "#             question_encoder_output, question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "#             question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden    \n",
    "\n",
    "        ################################### Q Decoder    \n",
    "        \n",
    "        question_loss = 0\n",
    "        \n",
    "        question_decoder_hidden = question_encoder_hidden\n",
    "        question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        \n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num]).long())\n",
    "        #embedded_inputs.requires_grad = False\n",
    "        \n",
    "        output_labels = Variable(torch.from_numpy(batch_input[6][batch_num]).long())\n",
    "        \n",
    "        decoder_output = Variable(torch.zeros(1,batch_size,questionDecoder.hidden_size))\n",
    "        \n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "            \n",
    "        \n",
    "        question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        for quesL in range(max(batch_input[7][batch_num])):\n",
    "            decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                embedded_inputs[:,quesL:quesL+1,:],\n",
    "                question_decoder_hidden)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output)\n",
    "\n",
    "            output_label = Variable(torch.zeros(32,2000))\n",
    "            if use_cuda:\n",
    "                output_label = output_label.cuda()\n",
    "            for b in range(len(batch_input[5][0])):\n",
    "                output_label[b,batch_input[5][batch_num][b][quesL]] = 1\n",
    "            #question_loss += criterion2(final_output.squeeze(1), \n",
    "            #                           output_labels[:,quesL:quesL+1])\n",
    "            question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f' \n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "          \n",
    "        \n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
