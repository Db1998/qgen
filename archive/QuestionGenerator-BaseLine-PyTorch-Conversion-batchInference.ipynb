{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from nltk.corpus import stopwords\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSetForInference(context, answer, ground_truth_answer=None, useGroundTruth=False):\n",
    "    \n",
    "    passage = word_tokenize(context.lower())\n",
    "    doc_len = len(passage)\n",
    "    \n",
    "    if(useGroundTruth):\n",
    "        ans_condition = word_tokenize(ground_truth_answer.lower())\n",
    "    else:\n",
    "        ans_condition = word_tokenize(answer.lower())\n",
    "    \n",
    "    ans_len = len(ans_condition)\n",
    "        \n",
    "    start,end = find_sub_list(ans_condition,passage)\n",
    "\n",
    "    if start == -1:\n",
    "            print(\"Couldn' Find answer in text, Please try again !!\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    document_token = np.zeros((1, doc_len), dtype=np.int32)\n",
    "    document_length = np.zeros(1, dtype=np.int32)\n",
    "    answer_label = np.zeros((1, doc_len), dtype=np.int32)\n",
    "    answer_mask = np.zeros((1, ans_len, doc_len), dtype=np.int32)\n",
    "    answer_length = np.zeros(1, dtype=np.int32)\n",
    "    \n",
    "    document_length[0] = doc_len\n",
    "    answer_length[0] = ans_len\n",
    "    answer_label[0,start:end+1] = 1\n",
    "    \n",
    "    for i in range(doc_len):\n",
    "        document_token[:,i] = look_up_word(passage[i])\n",
    "    for i in range(ans_len):\n",
    "        answer_mask[:,i,start+i] = 1\n",
    "    \n",
    "    return {\n",
    "        \"document_token\" : document_token,\n",
    "        \"document_length\" : document_length,\n",
    "        \"answer_label\" : answer_label,\n",
    "        \"answer_mask\" : answer_mask,\n",
    "        \"answer_length\" : answer_length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(context, answer = []):\n",
    "    \n",
    "    data_dict = prepareSetForInference(context, answer)\n",
    "    \n",
    "    use_attention = True\n",
    "    use_ground_truth = True\n",
    "    maxDocLenForBatch = max_document_len\n",
    "\n",
    "    answer_encoder_hidden_inf = answerEncoder2.hiddenState\n",
    "    question_encoder_hidden_inf = questionEncoder2.hiddenState\n",
    "    question_decoder_hidden_inf = questionDecoder2.hiddenState\n",
    "    attention_hidden_inf = attention.hidden_state\n",
    "    question_decoder_hidden_inf = questionDecoder2.hiddenState\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    inp = Variable(torch.from_numpy(data_dict[\"document_token\"]).long())\n",
    "    labels = Variable(torch.from_numpy(data_dict[\"answer_label\"])).long() #Let's see if we want to use or not\n",
    "    if use_cuda:\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    embedded_inp = embedder(inp)\n",
    "\n",
    "    if use_cuda:\n",
    "        embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "\n",
    "    answer_encoder_hidden_inf = repackage_hidden(answer_encoder_hidden_inf)\n",
    "    answer_tags, answer_outputs, answer_encoder_hidden_inf = answerEncoder(embedded_inp, answer_encoder_hidden_inf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_cuda:\n",
    "        answer_outputs = answer_outputs.cuda()\n",
    "        answer_tags = answer_tags.cuda()\n",
    "\n",
    "\n",
    "    question_encoder_input = Variable(torch.from_numpy(data_dict[\"answer_mask\"]))\n",
    "    if use_cuda:\n",
    "        question_encoder_input = question_encoder_input.cuda()   #CHECK IF THIS IS RIGHT\n",
    "\n",
    "    question_encoder_hidden_inf = repackage_hidden(question_encoder_hidden) ##Get this from saved model\n",
    "\n",
    "    t_answer_mask = Variable(torch.from_numpy(data_dict[\"answer_mask\"])).float()\n",
    "    if use_cuda:\n",
    "        t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "    question_encoder_input = torch.matmul(t_answer_mask[:,0:data_dict[\"answer_length\"],:], answer_outputs.float())\n",
    "    \n",
    "    _ , question_encoder_hidden_inf = questionEncoder(question_encoder_input[:,0:data_dict[\"answer_length\"],:], question_encoder_hidden_inf)\n",
    "\n",
    "    if type(question_decoder_hidden_inf) == Variable:\n",
    "        question_decoder_hidden_inf = repackage_hidden(question_decoder_hidden_inf)\n",
    "    if(type(attention_hidden) == Variable):\n",
    "        attention_hidden_inf = repackage_hidden(attention_hidden_inf)\n",
    "\n",
    "    question_loss = 0\n",
    "    question_decoder_hidden_inf = question_encoder_hidden_inf\n",
    "\n",
    "    ###embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long()).cuda()\n",
    "\n",
    "    maxGenQuesLen = 20\n",
    "    currGenQuestionLen = 0\n",
    "\n",
    "    current_question_token = START_TOKEN\n",
    "    questionGenerated = []\n",
    "    while currGenQuestionLen < maxGenQuesLen and current_question_token != END_TOKEN:\n",
    "        question_token_embedding = embedder(Variable(torch.from_numpy(np.array([current_question_token])).long()))\n",
    "        if use_cuda:\n",
    "            question_token_embedding = question_token_embedding.cuda()\n",
    "\n",
    "        if use_attention:\n",
    "            attn_output, Attention_Weights = attention(question_decoder_hidden_inf.squeeze(0).squeeze(0), attention_hidden_inf.squeeze(0), answer_outputs.squeeze(0))\n",
    "            decoder_output, attention_hidden_inf = questionDecoder(\n",
    "                question_token_embedding.unsqueeze(1), attn_output)\n",
    "        else:\n",
    "            decoder_output, question_decoder_hidden_inf = questionDecoder(\n",
    "                question_token_embedding.unsqueeze(1),\n",
    "                question_decoder_hidden_inf)\n",
    "\n",
    "        final_output = questionGenerator(decoder_output)\n",
    "        current_question_token = np.argmax(final_output.data)\n",
    "        questionGenerated.append(look_up_token_reduced(current_question_token))\n",
    "        currGenQuestionLen += 1\n",
    "        \n",
    "    return questionGenerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_1_hidden = questionEncoder1.initHidden()\n",
    "question_encoder_2_hidden = questionEncoder2.initHidden()\n",
    "question_decoder_hidden = None\n",
    "\n",
    "Attention_Weights = None\n",
    "attn_output = None\n",
    "attention_hidden = attention.initHidden()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for batch_num in range(number_of_batches):\n",
    "        \n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        question_encoder_1_hidden = repackage_hidden(question_encoder_1_hidden)\n",
    "        question_encoder_2_hidden = repackage_hidden(question_encoder_2_hidden)\n",
    "        if type(question_decoder_hidden) == Variable:\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        if(type(attn_output) == Variable):\n",
    "            attn_output = repackage_hidden(attn_output)\n",
    "        if(type(attention_hidden) == Variable):\n",
    "            attention_hidden = repackage_hidden(attention_hidden)\n",
    "        if(type(Attention_Weights) == Variable):\n",
    "            Attention_Weights = repackage_hidden(Attention_Weights)\n",
    "        \n",
    "        current_batch_size = len(batch_input[batch_num][\"document_tokens\"])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        maxDocLenForBatch = max_document_len\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[batch_num][\"document_lengths\"][i]] = 1\n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[batch_num][\"document_tokens\"]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels_all\"])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        \n",
    "        answer_outputs = Variable(torch.zeros(current_batch_size, max_document_len, hidden_size))\n",
    "        answer_tags = Variable(torch.zeros(current_batch_size, max_document_len, 1))\n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "            embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)              \n",
    "\n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "\n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        answer_mask_1 = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        answer_mask_2 = Variable(torch.from_numpy(1 - batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_mask_1 = answer_mask_1.cuda()\n",
    "            answer_mask_2 = answer_mask_2.cuda()\n",
    "        \n",
    "        question_encoder_input1 = torch.mul(answer_mask_1, answer_outputs.float())\n",
    "        question_encoder_input2 = torch.mul(answer_mask_2, answer_outputs.float())\n",
    "        \n",
    "        question_encoder_1_outputs , question_encoder_1_hidden = questionEncoder1(question_encoder_input1, question_encoder_1_hidden)\n",
    "        question_encoder_2_outputs , question_encoder_2_hidden = questionEncoder2(question_encoder_input2, question_encoder_2_hidden)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        question_loss = 0\n",
    "        question_decoder_hidden = question_encoder_2_hidden\n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[batch_num][\"question_input_tokens\"]).long())\n",
    "        output_labels = Variable(torch.from_numpy(batch_input[batch_num][\"question_output_tokens\"]).long())\n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "\n",
    "        for quesL in range(batch_input[batch_num][\"question_input_tokens\"].shape[1]):\n",
    "            if use_attention:\n",
    "                attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "                decoder_output, attention_hidden = questionDecoder(\n",
    "                    embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "            else:\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    embedded_inputs[:,quesL:quesL+1,:],\n",
    "                    question_decoder_hidden)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output.squeeze(1))\n",
    "            \n",
    "            question_loss += criterion2(final_output,\n",
    "                                       output_labels[:, quesL:quesL+1].squeeze(1))\n",
    "\n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f'\n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 2000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hostname\n",
    "print(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 32\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1\n",
    "            \n",
    "    words_to_consider_expression = set(X_train_comp[i] + nltkStopWords + punctuations)\n",
    "\n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_contexts[i,:,look_up_word_reduced(word)] = 1\n",
    "        \n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_probabilities[i,:,look_up_word_reduced(word)] = len(np.where(expression_contexts[i][0] == 1)[0]) / float(wordToTake)\n",
    "    expression_probabilities[i,:,np.where(expression_probabilities[i][0] == 0)[0]] = len(np.where(expression_contexts[i][0] == 0)[0]) / float(wordToTake)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = context_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def create_batch(inputs,batch_size,shuffle=False):\n",
    "    num_batches = len(inputs[0]) // batch_size + 1\n",
    "    outputs = []\n",
    "    for index,inp in enumerate(inputs):\n",
    "    \n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_masks': [],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[],\n",
    "                    'suppression_answer':[],\n",
    "                    'expression_contexts': [],\n",
    "                    'expression_probabilities':[]}\n",
    "    \n",
    "        start = 0\n",
    "        for i in range(num_batches):\n",
    "            if i == num_batches - 1:\n",
    "                end = None\n",
    "            else:\n",
    "                end = start+batch_size\n",
    "            #maxD = max(inputs[1][start:end])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[4][start:end])\n",
    "            maxQ = max(inputs[7][start:end])\n",
    "            if index == 0:\n",
    "                outputs['document_tokens'] = inp[start:end,:maxD]\n",
    "            elif index==1:\n",
    "                outputs['document_lengths'] = inp[start:end]\n",
    "            elif index == 2:\n",
    "                outputs['answer_labels']=inp[start:end,:maxD]\n",
    "            elif index==3:\n",
    "                outputs['answer_masks']=inp[start:end,:maxA,:maxD]\n",
    "            elif index==4:\n",
    "                outputs['answer_lengths']=inp[start:end]\n",
    "            elif index==5:\n",
    "                output['question_input_tokens']=inp[start:end,:maxQ]\n",
    "            elif index==6:\n",
    "                output['question_output_tokens']=inp[start:end,:maxQ]\n",
    "            elif index==7:\n",
    "                output['question_lengths'] = inp[start:end]\n",
    "            elif index==8:\n",
    "                output['suppression_answer'] = inp[start:end]\n",
    "            elif index==9:\n",
    "                output['expression_contexts'] = inp[start:end,0:maxQ,:]\n",
    "            elif index==10: \n",
    "                output['expression_probabilities'] = inp[start:end,0:maxQ,:]\n",
    "            start = start + batch_size\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            #maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index==9 or index==10:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ,:])\n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        #maxD = max(inputs[1][start:])\n",
    "        maxD = max_document_len\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ])\n",
    "        elif index==9 or index==10:\n",
    "            output.append(inp[start:,0:maxQ,:]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer,expression_contexts,expression_probabilities]\n",
    "                    ,batch_size)\n",
    "number_of_batches = len(batch_input[0])\n",
    "print(\"Number of batches = \", number_of_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        \n",
    "        # TODO: Verify\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(AnswerEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True, bidirectional=True) #Input_size = Hidden_Size\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        self.hiddenState = hidden\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, 1, self.hidden_size)) #2 for BiDirectional\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result\n",
    "    \n",
    "'''\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "'''\n",
    "\n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, max_document_len, dropout_p=0.1):\n",
    "        \n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_document_len = max_document_len\n",
    "\n",
    "        self.attn_reduce = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.max_document_len + self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.attn_reduce = self.attn_reduce.cuda()\n",
    "            self.attn_combine = self.attn_combine.cuda()\n",
    "            self.dropout = self.dropout.cuda()\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.dropout(hidden)\n",
    "        concat = torch.cat((input.unsqueeze(0), hidden.unsqueeze(0)), 1)\n",
    "        concat_reduced = self.attn_reduce(concat)\n",
    "        attn_weights = F.softmax(concat_reduced, dim=1)\n",
    "        attn_applied = torch.mm(encoder_outputs, attn_weights.squeeze(0).unsqueeze(1))\n",
    "        output = torch.cat((hidden.unsqueeze(0), attn_applied.squeeze(1).unsqueeze(0)), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        self.hidden_state = hidden\n",
    "        return output, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "#fcLayer = FCLayer(hidden_size, hidden_size)\n",
    "answerEncoder = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention = AttnDecoderRNN(hidden_size= hidden_size,max_document_len = max_document_len)\n",
    "\n",
    "answerEncoder.train()\n",
    "questionEncoder.train()\n",
    "questionDecoder.train()\n",
    "questionGenerator.train()\n",
    "attention.train()\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [answerEncoder, questionEncoder, questionDecoder, questionGenerator, attention]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 0.01)\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "#criterion2 = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "\n",
    "if use_attention:\n",
    "    Attention_Weights = None\n",
    "    attn_output = None\n",
    "    attention_hidden = attention.initHidden()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "\n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "\n",
    "\n",
    "        #maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        maxDocLenForBatch = max_document_len\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp).cuda()\n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        \n",
    "        \n",
    "        answer_outputs = Variable(torch.zeros(current_batch_size, max_document_len, hidden_size))\n",
    "        answer_tags = Variable(torch.zeros(current_batch_size, max_document_len, 1))\n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "\n",
    "        for i in range(current_batch_size):\n",
    "            answer_tags[i], answer_outputs[i], answer_encoder_hidden = answerEncoder(embedded_inp[i:i+1], answer_encoder_hidden)   \n",
    "            \n",
    "\n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "\n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "\n",
    "\n",
    "        t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "        if use_cuda:\n",
    "            t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "        question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "        question_encoder_hidden_batch = Variable(torch.zeros(1,current_batch_size,questionEncoder.hidden_size))\n",
    "        if use_cuda:\n",
    "            question_encoder_hidden_batch = question_encoder_hidden_batch.cuda()\n",
    "\n",
    "        question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "        for i in range(current_batch_size):\n",
    "            _ , question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "            question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden\n",
    "\n",
    "        if type(question_decoder_hidden) == Variable:\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        if(type(attn_output) == Variable):\n",
    "            attn_output = repackage_hidden(attn_output)\n",
    "        if(type(attention_hidden) == Variable):\n",
    "            attention_hidden = repackage_hidden(attention_hidden)\n",
    "        if(type(Attention_Weights) == Variable):\n",
    "            Attention_Weights = repackage_hidden(Attention_Weights)\n",
    "        \n",
    "        question_loss = 0\n",
    "        for i in range(current_batch_size):\n",
    "            question_decoder_hidden = question_encoder_hidden_batch[:,i:i+1,:].clone()\n",
    "            embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long())\n",
    "            output_labels = Variable(torch.from_numpy(batch_input[6][batch_num][i]).long())\n",
    "            if use_cuda:\n",
    "                embedded_inputs = embedded_inputs.cuda()\n",
    "                output_labels = output_labels.cuda()\n",
    "\n",
    "            for quesL in range(batch_input[7][batch_num][i]):\n",
    "                if use_attention:\n",
    "                    attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "                    decoder_output, attention_hidden = questionDecoder(\n",
    "                        embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "                else:\n",
    "                    decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                        embedded_inputs[quesL:quesL+1].unsqueeze(1),\n",
    "                        question_decoder_hidden)\n",
    "\n",
    "                final_output = questionGenerator(decoder_output)\n",
    "                output_label = Variable(torch.zeros(1,reduced_glove.shape[0]))\n",
    "                if use_cuda:\n",
    "                    output_label = output_label.cuda()\n",
    "                output_label[:,batch_input[5][batch_num][i][quesL]] = 1\n",
    "                question_loss += criterion2(final_output.squeeze(0),\n",
    "                                           output_labels[quesL:quesL+1])\n",
    "                ##question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "\n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f'\n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/data/ra2630/qgen_base_40k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerEncoder2 = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder2 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder2 = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator2 = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "optimizer2 = torch.optim.Adam(train_param, 0.01)\n",
    "\n",
    "#answerEncoder2.load_state_dict(checkpoint[\"answerEncoder\"])\n",
    "#questionEncoder2.load_state_dict(checkpoint[\"questionEncoder\"])\n",
    "#questionDecoder2.load_state_dict(checkpoint[\"questionDecoder\"])\n",
    "#questionGenerator2.load_state_dict(checkpoint[\"questionGenerator\"])\n",
    "#optimizer2.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "answerEncoder2 = answerEncoder\n",
    "questionEncoder2 = questionEncoder\n",
    "questionDecoder2 = questionDecoder\n",
    "questionGenerator2 = questionGenerator\n",
    "optimizer2 = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printQuestion(batch_num, example_num):\n",
    "    for i in batch_input[6][batch_num][example_num]:\n",
    "        print(look_up_token_reduced(i), end = ' ', sep = ' ')\n",
    "    print(\"\")\n",
    "        \n",
    "def printAnswer(batch_num, example_num):\n",
    "    for i in batch_input[3][batch_num][example_num][0 : batch_input[4][batch_num][example_num]]:\n",
    "        for j in (np.where(i==1)[0]):\n",
    "            print(look_up_token_reduced(batch_input[0][batch_num][example_num][j]))\n",
    "    print(\"\")\n",
    "        \n",
    "def printContext(batch_num, example_num):\n",
    "    for i in batch_input[0][batch_num][example_num]:\n",
    "        print(look_up_token_reduced(i), end = ' ', sep = ' ')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 0\n",
    "example_num=0\n",
    "print(\"Context : \")\n",
    "printContext(batch_num, example_num)\n",
    "print(\"Question : \")\n",
    "printQuestion(batch_num, example_num)\n",
    "print(\"Answer : \")\n",
    "printAnswer(batch_num, example_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_token = START_TOKEN\n",
    "question_token_embedding = embedder(Variable(torch.from_numpy(np.array([question_token])).long())).cuda()\n",
    "qLen = 0\n",
    "while qLen <= 20:\n",
    "    decoder_output, question_decoder_hidden = questionDecoder(\n",
    "        question_token_embedding.unsqueeze(1),\n",
    "        question_decoder_hidden)\n",
    "    final_output = questionGenerator(decoder_output)\n",
    "    question_token = np.argmax(final_output.data)\n",
    "    print(question_token)\n",
    "    question_token_embedding = embedder(Variable(torch.from_numpy(np.array([question_token])).long())).cuda()\n",
    "    qLen=qLen + 1\n",
    "    print(look_up_token_reduced(question_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "#question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "#expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "#expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerEncoder2 = answerEncoder\n",
    "questionEncoder2 = questionEncoder\n",
    "questionDecoder2 = questionDecoder\n",
    "questionGenerator2 = questionGenerator\n",
    "attention2 = attention\n",
    "optimizer2 = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    printQuestion(0,i)\n",
    "    print(inference(0,i,\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(batch_number, example_number, context, answer = []):\n",
    "    \n",
    "    use_attention = True\n",
    "    use_ground_truth = True\n",
    "    maxDocLenForBatch = max_document_len\n",
    "\n",
    "    answer_encoder_hidden_inf = answerEncoder2.hiddenState\n",
    "    question_encoder_hidden_inf = questionEncoder2.hiddenState\n",
    "    question_decoder_hidden_inf = questionDecoder2.hiddenState\n",
    "    attention_hidden_inf = attention.hidden_state\n",
    "    question_decoder_hidden_inf = questionDecoder2.hiddenState\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    inp = Variable(torch.from_numpy(batch_input[0][batch_number][example_number:example_number+1]).long())\n",
    "    labels = Variable(torch.from_numpy(batch_input[2][batch_number][example_number:example_number+1])).long() #Let's see if we want to use or not\n",
    "    if use_cuda:\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    embedded_inp = embedder(inp)\n",
    "\n",
    "    if use_cuda:\n",
    "        embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "\n",
    "    answer_encoder_hidden_inf = repackage_hidden(answer_encoder_hidden_inf)\n",
    "    answer_tags, answer_outputs, answer_encoder_hidden_inf = answerEncoder(embedded_inp, answer_encoder_hidden_inf)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if use_cuda:\n",
    "        answer_outputs = answer_outputs.cuda()\n",
    "        answer_tags = answer_tags.cuda()\n",
    "\n",
    "\n",
    "    question_encoder_input = Variable(torch.from_numpy(batch_input[3][batch_number][example_number:example_number+1]))\n",
    "    if use_cuda:\n",
    "        question_encoder_input = question_encoder_input.cuda()   #CHECK IF THIS IS RIGHT\n",
    "\n",
    "    question_encoder_hidden_inf = repackage_hidden(question_encoder_hidden) ##Get this from saved model\n",
    "\n",
    "    t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_number][example_number:example_number+1])).float()\n",
    "    if use_cuda:\n",
    "        t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "    question_encoder_input = torch.matmul(t_answer_mask[:,0:batch_input[4][batch_num][example_number],:], answer_outputs.float())\n",
    "    \n",
    "    _ , question_encoder_hidden_inf = questionEncoder(question_encoder_input[:,0:batch_input[4][batch_number][example_number],:], question_encoder_hidden_inf)\n",
    "\n",
    "    if type(question_decoder_hidden_inf) == Variable:\n",
    "        question_decoder_hidden_inf = repackage_hidden(question_decoder_hidden_inf)\n",
    "    if(type(attention_hidden) == Variable):\n",
    "        attention_hidden_inf = repackage_hidden(attention_hidden_inf)\n",
    "\n",
    "    question_loss = 0\n",
    "    question_decoder_hidden_inf = question_encoder_hidden_inf\n",
    "\n",
    "    ###embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long()).cuda()\n",
    "\n",
    "    maxGenQuesLen = 20\n",
    "    currGenQuestionLen = 0\n",
    "\n",
    "    current_question_token = START_TOKEN\n",
    "    questionGenerated = []\n",
    "    while currGenQuestionLen < maxGenQuesLen and current_question_token != END_TOKEN:\n",
    "        question_token_embedding = embedder(Variable(torch.from_numpy(np.array([current_question_token])).long()))\n",
    "        if use_cuda:\n",
    "            question_token_embedding = question_token_embedding.cuda()\n",
    "\n",
    "        if use_attention:\n",
    "            attn_output, Attention_Weights = attention(question_decoder_hidden_inf.squeeze(0).squeeze(0), attention_hidden_inf.squeeze(0), answer_outputs.squeeze(0))\n",
    "            decoder_output, attention_hidden_inf = questionDecoder(\n",
    "                question_token_embedding.unsqueeze(1), attn_output)\n",
    "        else:\n",
    "            decoder_output, question_decoder_hidden_inf = questionDecoder(\n",
    "                question_token_embedding.unsqueeze(1),\n",
    "                question_decoder_hidden_inf)\n",
    "\n",
    "        final_output = questionGenerator(decoder_output)\n",
    "        current_question_token = np.argmax(final_output.data)\n",
    "        questionGenerated.append(look_up_token_reduced(current_question_token))\n",
    "        currGenQuestionLen += 1\n",
    "        \n",
    "    return questionGenerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_attention = True\n",
    "use_ground_truth = True\n",
    "maxDocLenForBatch = max_document_len\n",
    "\n",
    "answer_encoder_hidden_inf = answerEncoder2.hiddenState\n",
    "question_encoder_hidden_inf = questionEncoder2.hiddenState\n",
    "question_decoder_hidden_inf = questionDecoder2.hiddenState\n",
    "attention_hidden_inf = attention.hidden_state\n",
    "question_decoder_hidden_inf = questionDecoder2.hiddenState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "inp = Variable(torch.from_numpy(batch_input[0][batch_number][example_number:example_number+1]).long())\n",
    "labels = Variable(torch.from_numpy(batch_input[2][batch_number][example_number:example_number+1])).long() #Let's see if we want to use or not\n",
    "if use_cuda:\n",
    "    labels = labels.cuda()\n",
    "\n",
    "embedded_inp = embedder(inp)\n",
    "\n",
    "if use_cuda:\n",
    "    embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "\n",
    "answer_encoder_hidden_inf = repackage_hidden(answer_encoder_hidden_inf)\n",
    "answer_tags, answer_outputs, answer_encoder_hidden_inf = answerEncoder(embedded_inp, answer_encoder_hidden_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    answer_outputs = answer_outputs.cuda()\n",
    "    answer_tags = answer_tags.cuda()\n",
    "\n",
    "\n",
    "question_encoder_input = Variable(torch.from_numpy(batch_input[3][batch_number][example_number:example_number+1]))\n",
    "if use_cuda:\n",
    "    question_encoder_input = question_encoder_input.cuda()   #CHECK IF THIS IS RIGHT\n",
    "\n",
    "question_encoder_hidden_inf = repackage_hidden(question_encoder_hidden) ##Get this from saved model\n",
    "\n",
    "t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_number][example_number:example_number+1])).float()\n",
    "if use_cuda:\n",
    "    t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "question_encoder_input = torch.matmul(t_answer_mask[:,0:batch_input[4][batch_num][example_number],:], answer_outputs.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ , question_encoder_hidden_inf = questionEncoder(question_encoder_input[:,0:batch_input[4][batch_number][example_number],:], question_encoder_hidden_inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(question_decoder_hidden_inf) == Variable:\n",
    "        question_decoder_hidden_inf = repackage_hidden(question_decoder_hidden_inf)\n",
    "if(type(attention_hidden) == Variable):\n",
    "    attention_hidden_inf = repackage_hidden(attention_hidden_inf)\n",
    "\n",
    "question_loss = 0\n",
    "question_decoder_hidden_inf = question_encoder_hidden_inf\n",
    "\n",
    "###embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long()).cuda()\n",
    "\n",
    "maxGenQuesLen = 20\n",
    "currGenQuestionLen = 0\n",
    "\n",
    "current_question_token = look_up_word_reduced(\"what\")\n",
    "questionGenerated = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_token_embedding = embedder(Variable(torch.from_numpy(np.array([current_question_token])).long()))\n",
    "if use_cuda:\n",
    "    question_token_embedding = question_token_embedding.cuda()\n",
    "\n",
    "if use_attention:\n",
    "    attn_output, Attention_Weights = attention(question_decoder_hidden_inf.squeeze(0).squeeze(0), attention_hidden_inf.squeeze(0), answer_outputs.squeeze(0))\n",
    "    decoder_output, attention_hidden_inf = questionDecoder(\n",
    "        question_token_embedding.unsqueeze(1), attn_output)\n",
    "else:\n",
    "    decoder_output, question_decoder_hidden_inf = questionDecoder(\n",
    "        question_token_embedding.unsqueeze(1),\n",
    "        question_decoder_hidden_inf)\n",
    "\n",
    "final_output = questionGenerator(decoder_output)\n",
    "current_question_token = np.argmax(final_output.data)\n",
    "questionGenerated.append(look_up_token_reduced(current_question_token))\n",
    "currGenQuestionLen += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "look_up_token_reduced(np.argsort(final_output.data[0][0])[-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printQuestion(batch_number, example_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = question_decoder_hidden_inf.squeeze(0).squeeze(0)\n",
    "hidden = attention_hidden_inf.squeeze(0)\n",
    "encoder_outputs = answer_outputs.squeeze(0)\n",
    "hidden = hidden.squeeze(0)\n",
    "hidden = attention2.dropout(hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = torch.cat((input.unsqueeze(0), hidden.unsqueeze(0)), 1)\n",
    "concat_reduced = attention.attn_reduce(concat)\n",
    "attn_weights = F.softmax(concat_reduced, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights.squeeze(0).unsqueeze(1).shape, encoder_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_applied = torch.mm(encoder_outputs, attn_weights.squeeze(0).unsqueeze(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##########ONly Batch\n",
    "\n",
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "        \n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "\n",
    "            \n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        ################################### Answer Encoder + Tagging    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        if use_cuda:\n",
    "            embedded_inp = embedder(inp).cuda()\n",
    "\n",
    "            \n",
    "        \n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "\n",
    "        #answer_tags.requires_grad=False\n",
    "        #answer_outputs.requires_grad=False\n",
    "        \n",
    "        \n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        #t_document_mask.requires_grade = False\n",
    "        \n",
    "        outputs = answer_tags.squeeze(-1) * t_document_mask\n",
    "        #outputs.requires_grad = False\n",
    "        \n",
    "        \n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "        ################################### Q Encoder    \n",
    "        \n",
    "        t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "        if use_cuda:\n",
    "            t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "        #t_answer_mask.requires_gradui = False\n",
    "        \n",
    "        # masking the non-answer embeddings\n",
    "        question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "        #question_encoder_input.requires_grad = False\n",
    "        question_encoder_output = Variable(torch.zeros(1,batch_size,questionEncoder.hidden_size))\n",
    "        #question_encoder_output.requires_grad = False\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            question_encoder_hidden = question_encoder_hidden_batch.cuda()\n",
    "            \n",
    "        question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "        question_encoder_output, question_encoder_hidden = questionEncoder(question_encoder_input, question_encoder_hidden)\n",
    "        \n",
    "        #question_encoder_output.requires_grad = False\n",
    "\n",
    "        \n",
    "#         question_encoder_output = Variable(torch.zeros(batch_size, max_question_len, questionEncoder.hidden_size))\n",
    "#         question_encoder_hidden = Variable(torch.zeros(batch_size, max_question_len, questionEncoder.hidden_size))\n",
    "        \n",
    "#         for i in range(current_batch_size):\n",
    "#             question_encoder_output, question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "#             question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden    \n",
    "\n",
    "        ################################### Q Decoder    \n",
    "        \n",
    "        question_loss = 0\n",
    "        \n",
    "        question_decoder_hidden = question_encoder_hidden\n",
    "        question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        \n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num]).long())\n",
    "        #embedded_inputs.requires_grad = False\n",
    "        \n",
    "        output_labels = Variable(torch.from_numpy(batch_input[6][batch_num]).long())\n",
    "        \n",
    "        decoder_output = Variable(torch.zeros(1,batch_size,questionDecoder.hidden_size))\n",
    "        \n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "            \n",
    "        \n",
    "        question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        for quesL in range(max(batch_input[7][batch_num])):\n",
    "            decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                embedded_inputs[:,quesL:quesL+1,:],\n",
    "                question_decoder_hidden)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output)\n",
    "\n",
    "            output_label = Variable(torch.zeros(32,2000))\n",
    "            if use_cuda:\n",
    "                output_label = output_label.cuda()\n",
    "            for b in range(len(batch_input[5][0])):\n",
    "                output_label[b,batch_input[5][batch_num][b][quesL]] = 1\n",
    "            #question_loss += criterion2(final_output.squeeze(1), \n",
    "            #                           output_labels[:,quesL:quesL+1])\n",
    "            question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f' \n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "          \n",
    "        \n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
