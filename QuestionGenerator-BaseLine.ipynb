{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "from embedding import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "nltkStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 50000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "1510\n",
      "rev\n",
      ".\n",
      "john\n",
      "j.\n",
      "cavanaugh\n",
      ",\n",
      "c.s.c\n",
      ".\n",
      "served\n",
      "more\n",
      "than\n",
      "half\n",
      ",\n",
      "lobund\n",
      "institute\n",
      "for\n",
      "animal\n",
      "studies\n",
      "and\n",
      "medieval\n",
      "institute\n",
      ".\n",
      "hall\n",
      "of\n",
      "liberal\n",
      "arts\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'s\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '``', 'ad', 'me', 'omnes', \"''\", '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in', '1858.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n",
      "['saint', 'bernadette', 'soubirous']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 104)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510\n",
      "['the', 'success', 'of', 'its', 'football', 'team', 'made', 'notre', 'dame', 'a', 'household', 'name', '.', 'the', 'success', 'of', 'note', 'dame', 'reflected', 'rising', 'status', 'of', 'irish', 'americans', 'and', 'catholics', 'in', 'the', '1920s', '.', 'catholics', 'rallied', 'up', 'around', 'the', 'team', 'and', 'listen', 'to', 'the', 'games', 'on', 'the', 'radio', ',', 'especially', 'when', 'it', 'knocked', 'off', 'the', 'schools', 'that', 'symbolized', 'the', 'protestant', 'establishment', 'in', 'america', 'â€”', 'harvard', ',', 'yale', ',', 'princeton', ',', 'and', 'army', '.', 'yet', 'this', 'role', 'as', 'high-profile', 'flagship', 'institution', 'of', 'catholicism', 'made', 'it', 'an', 'easy', 'target', 'of', 'anti-catholicism', '.', 'the', 'most', 'remarkable', 'episode', 'of', 'violence', 'was', 'the', 'clash', 'between', 'notre', 'dame', 'students', 'and', 'the', 'ku', 'klux', 'klan', 'in', '1924.', 'nativism', 'and', 'anti-catholicism', ',', 'especially', 'when', 'directed', 'towards', 'immigrants', ',', 'were', 'cornerstones', 'of', 'the', 'kkk', \"'s\", 'rhetoric', ',', 'and', 'notre', 'dame', 'was', 'seen', 'as', 'a', 'symbol', 'of', 'the', 'threat', 'posed', 'by', 'the', 'catholic', 'church', '.', 'the', 'klan', 'decided', 'to', 'have', 'a', 'week-long', 'klavern', 'in', 'south', 'bend', '.', 'clashes', 'with', 'the', 'student', 'body', 'started', 'on', 'march', '17', ',', 'when', 'students', ',', 'aware', 'of', 'the', 'anti-catholic', 'animosity', ',', 'blocked', 'the', 'klansmen', 'from', 'descending', 'from', 'their', 'trains', 'in', 'the', 'south', 'bend', 'station', 'and', 'ripped', 'the', 'kkk', 'clothes', 'and', 'regalia', '.', 'on', 'may', '19', 'thousands', 'of', 'students', 'massed', 'downtown', 'protesting', 'the', 'klavern', ',', 'and', 'only', 'the', 'arrival', 'of', 'college', 'president', 'fr', '.', 'matthew', 'walsh', 'prevented', 'any', 'further', 'clashes', '.', 'the', 'next', 'day', ',', 'football', 'coach', 'knute', 'rockne', 'spoke', 'at', 'a', 'campus', 'rally', 'and', 'implored', 'the', 'students', 'to', 'obey', 'the', 'college', 'president', 'and', 'refrain', 'from', 'further', 'violence', '.', 'a', 'few', 'days', 'later', 'the', 'klavern', 'broke', 'up', ',', 'but', 'the', 'hostility', 'shown', 'by', 'the', 'students', 'was', 'an', 'omen', 'and', 'a', 'contribution', 'to', 'the', 'downfall', 'of', 'the', 'kkk', 'in', 'indiana', '.']\n",
      "['the', 'ku', 'klux', 'klan']\n",
      "['notre', 'dame', 'students', 'had', 'a', 'showdown', 'in', '1924', 'with', 'which', 'anti-catholic', 'group', '?']\n",
      "['immediately', 'after', 'the', 'earthquake', 'event', ',', 'mobile', 'and', 'terrestrial', 'telecommunications', 'were', 'cut', 'to', 'the', 'affected', 'and', 'surrounding', 'area', ',', 'with', 'all', 'internet', 'capabilities', 'cut', 'to', 'the', 'sichuan', 'area', 'too', '.', 'elements', 'of', 'telecommunications', 'were', 'restored', 'by', 'the', 'government', 'piece', 'by', 'piece', 'over', 'the', 'next', 'number', 'of', 'months', 'as', 'the', 'situation', 'in', 'the', 'sichuan', 'province', 'gradually', 'improved', '.', 'eventually', ',', 'a', 'handful', 'of', 'major', 'news', 'and', 'media', 'websites', 'were', 'made', 'accessible', 'online', 'in', 'the', 'region', ',', 'albeit', 'with', 'dramatically', 'pared', 'back', 'webpages', '.']\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "['internet']\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  1.  0.  0.  0.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "['what', 'capabilities', 'were', 'cut', 'to', 'the', 'entire', 'sichuan', 'area', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 10000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "expression_contexts = np.ones((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 380)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1\n",
    "\n",
    "    for j,word in enumerate(X_train_comp[i]):\n",
    "        expression_contexts[i,look_up_word_reduced(word),:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "18501\n",
      "1435\n",
      "17986\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  49, 1219,   11, 2541,  109,   10, 2979,   95,    4,   90,   13,\n",
       "          3,   49, 1219, 2979,  353,   15,    0,   14,    6,   46,  732,\n",
       "        217,    5,    3, 7674,    5, 1219,   27, 3550,   18,    3, 2979,\n",
       "       1752,    5, 1219,   15,    0,   14,    6,  102,    4,    3,  938,\n",
       "       7674,    5, 1219,   11,  682,   90,   13,   49, 1219,    8, 1351,\n",
       "          9,  304, 1219,    6,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 380)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellFlag = 'GRU'\n",
    "QuestionLossFlag = True\n",
    "SupressionLossFlag = False\n",
    "ExpressionLossFlag = False\n",
    "minQuestionLoss = 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "embedding = tf.get_variable(\"embedding\", initializer=reduced_glove)\n",
    "embedding = tf.cast(embedding, dtype=tf.float32)\n",
    "EMBEDDING_DIMENS = glove.shape[1]\n",
    "\n",
    "d_tokens = tf.placeholder(tf.int32, shape=[None, None], name=\"d_tokens\")\n",
    "d_lengths = tf.placeholder(tf.int32, shape=[None], name=\"d_lengths\")\n",
    "\n",
    "\n",
    "document_emb = tf.nn.embedding_lookup(embedding, d_tokens, name=\"document_emb\")\n",
    "document_emb = tf.cast(document_emb, dtype=tf.float64, name=\"casted_document_emb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cellFlag == 'LSTM':\n",
    "    forward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "elif cellFlag == 'GRU':\n",
    "    forward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "\n",
    "answer_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, document_emb, d_lengths, dtype=tf.float64,\n",
    "    scope=\"answer_rnn\")\n",
    "\n",
    "answer_outputs = tf.concat(answer_outputs, 2, name=\"answer_output_concat\")\n",
    "\n",
    "answer_outputs = tf.cast(answer_outputs,tf.float32, name=\"answer_output_concat\")\n",
    "\n",
    "answer_tags = tf.layers.dense(inputs=answer_outputs, units=2, name=\"answer_tags\")\n",
    "\n",
    "\n",
    "a_labels = tf.placeholder(tf.int32, shape=[None, None], name=\"a_labels\")\n",
    "\n",
    "answer_mask = tf.sequence_mask(d_lengths, dtype=tf.float32, name=\"answer_mask\")\n",
    "\n",
    "answer_loss = seq2seq.sequence_loss(\n",
    "    logits=answer_tags, targets=a_labels, weights=answer_mask, name=\"answer_loss\")\n",
    "\n",
    "answer_loss = tf.Print(answer_loss, [answer_loss], message=\"This is answer_loss: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_mask = tf.placeholder(\n",
    "    tf.float32, shape=[None, None, None], name=\"encoder_input_mask\")\n",
    "encoder_inputs = tf.matmul(encoder_input_mask, answer_outputs, name=\"encoder_inputs\")\n",
    "encoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"encoder_lengths\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    encoder_cell = tf.contrib.rnn.GRUCell(forward_cell.state_size + backward_cell.state_size)\n",
    "elif cellFlag == 'LSTM':\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs, encoder_lengths, dtype=tf.float32, scope=\"encoder_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"decoder_inputs\")\n",
    "decoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"decoder_lengths\")\n",
    "\n",
    "decoder_emb = tf.nn.embedding_lookup(embedding, decoder_inputs,name=\"decoder_embedding\")\n",
    "decoder_emb = tf.cast(decoder_emb,tf.float32,name=\"decoder_embedding_cast\")\n",
    "\n",
    "helper = seq2seq.TrainingHelper(decoder_emb , decoder_lengths, name=\"helper\")\n",
    "\n",
    "\n",
    "projection = Dense(embedding.shape[0], use_bias=False, name=\"projection\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    decoder_cell = tf.contrib.rnn.GRUCell(encoder_cell.state_size)\n",
    "elif cellFlag == \"LSTM\":\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, scope=\"decoder\")\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "decoder_labels = tf.placeholder(tf.int64, shape=[None, None], name=\"decoder_labels\")\n",
    "question_mask = tf.sequence_mask(decoder_lengths ,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL Loss\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "question_loss = tf.Print(question_loss, [question_loss], message=\"This is question_loss: \")\n",
    "\n",
    "#Suppression Loss\n",
    "s_answer = tf.placeholder(tf.float32, shape=[None,None,None], name=\"suppression_answer\")\n",
    "lambdaSuppress = 0.5\n",
    "\n",
    "suppression_loss = lambdaSuppress * tf.reduce_sum(tf.matmul(tf.exp(decoder_outputs), s_answer))\n",
    "suppression_loss = tf.Print(suppression_loss, [suppression_loss], message=\"This is suppression_loss: \")\n",
    "\n",
    "\n",
    "#Expression Loss\n",
    "e_context = tf.placeholder(tf.float32, shape=[None,None,None], name=\"expression_answer\")\n",
    "lambdaExpress = 0.1\n",
    "\n",
    "expression_loss = lambdaExpress * tf.reduce_sum(tf.matmul(tf.exp(decoder_outputs), e_context))\n",
    "expression_loss = tf.Print(expression_loss, [expression_loss], message=\"This is expression loss: \")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Maximize Entropy Loss\n",
    "entropy_loss = tf.matmul(tf.transpose(decoder_outputs),decoder_outputs)\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = answer_loss\n",
    "if QuestionLossFlag:\n",
    "    loss = loss + question_loss\n",
    "if SupressionLossFlag:\n",
    "    loss = loss + suppression_loss\n",
    "if ExpressionLossFlag:\n",
    "    loss = loss + expression_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train,max_answer_len),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(*ls):\n",
    "    l =list(zip(*ls))\n",
    "    np.random.shuffle(l)\n",
    "    return zip(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index == 8: # \n",
    "                output.append(inp[start:start+batch_size,:,:]) \n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        maxD = max(inputs[1][start:])\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ]) \n",
    "        elif index ==8:\n",
    "            output.append(inp[start:,:, :]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer,expression_contexts]\n",
    "                    ,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of features: 10\n",
      "No of batches: 32\n"
     ]
    }
   ],
   "source": [
    "print(\"No of features:\",len( batch_input))\n",
    "print(\"No of batches:\",len( batch_input[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saved_vars = []\n",
    "l = len(tf.all_variables())\n",
    "for i,var in enumerate(tf.all_variables()):\n",
    "    print(i,\"/\",l)\n",
    "    saved_vars.append(var)\n",
    "        \n",
    "print(len(saved_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch :  0\n",
      "Loss: 11.520698547363281\n",
      "Batch :  1\n",
      "Loss: 11.572921752929688\n",
      "Batch :  2\n",
      "Loss: 9.996600151062012\n",
      "Batch :  3\n",
      "Loss: 8.306029319763184\n",
      "Batch :  4\n",
      "Loss: 7.818546295166016\n",
      "Batch :  5\n",
      "Loss: 8.085103034973145\n",
      "Batch :  6\n",
      "Loss: 8.181066513061523\n",
      "Batch :  7\n",
      "Loss: 8.76831340789795\n",
      "Batch :  8\n",
      "Loss: 8.270374298095703\n",
      "Batch :  9\n",
      "Loss: 8.027091026306152\n",
      "Batch :  10\n",
      "Loss: 8.10173511505127\n",
      "Batch :  11\n",
      "Loss: 7.902759552001953\n",
      "Batch :  12\n",
      "Loss: 7.485025882720947\n",
      "Batch :  13\n",
      "Loss: 7.334495544433594\n",
      "Batch :  14\n",
      "Loss: 7.30300760269165\n",
      "Batch :  15\n",
      "Loss: 7.475589752197266\n",
      "Batch :  16\n",
      "Loss: 7.3701701164245605\n",
      "Batch :  17\n",
      "Loss: 7.261754512786865\n",
      "Batch :  18\n",
      "Loss: 7.24669075012207\n",
      "Batch :  19\n",
      "Loss: 7.213845729827881\n",
      "Batch :  20\n",
      "Loss: 7.165980815887451\n",
      "Batch :  21\n",
      "Loss: 7.403918266296387\n",
      "Batch :  22\n",
      "Loss: 7.080841064453125\n",
      "Batch :  23\n",
      "Loss: 7.249973297119141\n",
      "Batch :  24\n",
      "Loss: 7.281829357147217\n",
      "Batch :  25\n",
      "Loss: 7.606300354003906\n",
      "Batch :  26\n",
      "Loss: 7.197308540344238\n",
      "Batch :  27\n",
      "Loss: 7.439516544342041\n",
      "Batch :  28\n",
      "Loss: 7.378582000732422\n",
      "Batch :  29\n",
      "Loss: 7.1491851806640625\n",
      "Batch :  30\n",
      "Loss: 7.280412197113037\n",
      "Batch :  31\n",
      "Loss: 7.010647773742676\n",
      "Epoch 2\n",
      "Batch :  0\n",
      "Loss: 6.589599132537842\n",
      "Batch :  1\n",
      "Loss: 6.224083423614502\n",
      "Batch :  2\n",
      "Loss: 5.971339702606201\n",
      "Batch :  3\n",
      "Loss: 5.9646782875061035\n",
      "Batch :  4\n",
      "Loss: 6.065187454223633\n",
      "Batch :  5\n",
      "Loss: 6.2799973487854\n",
      "Batch :  6\n",
      "Loss: 6.1565399169921875\n",
      "Batch :  7\n",
      "Loss: 6.562833309173584\n",
      "Batch :  8\n",
      "Loss: 6.394721031188965\n",
      "Batch :  9\n",
      "Loss: 6.376752853393555\n",
      "Batch :  10\n",
      "Loss: 6.575679779052734\n",
      "Batch :  11\n",
      "Loss: 6.420560359954834\n",
      "Batch :  12\n",
      "Loss: 6.336997032165527\n",
      "Batch :  13\n",
      "Loss: 6.216759204864502\n",
      "Batch :  14\n",
      "Loss: 6.228479862213135\n",
      "Batch :  15\n",
      "Loss: 6.472433567047119\n",
      "Batch :  16\n",
      "Loss: 6.27528715133667\n",
      "Batch :  17\n",
      "Loss: 6.214078903198242\n",
      "Batch :  18\n",
      "Loss: 6.283612251281738\n",
      "Batch :  19\n",
      "Loss: 6.269627571105957\n",
      "Batch :  20\n",
      "Loss: 6.130527496337891\n",
      "Batch :  21\n",
      "Loss: 6.3418755531311035\n",
      "Batch :  22\n",
      "Loss: 6.054477214813232\n",
      "Batch :  23\n",
      "Loss: 6.1263427734375\n",
      "Batch :  24\n",
      "Loss: 6.137965202331543\n",
      "Batch :  25\n",
      "Loss: 6.352241516113281\n",
      "Batch :  26\n",
      "Loss: 5.982707977294922\n",
      "Batch :  27\n",
      "Loss: 6.005195140838623\n",
      "Batch :  28\n",
      "Loss: 6.074950695037842\n",
      "Batch :  29\n",
      "Loss: 6.038995742797852\n",
      "Batch :  30\n",
      "Loss: 6.217769622802734\n",
      "Batch :  31\n",
      "Loss: 6.0137104988098145\n",
      "Epoch 3\n",
      "Batch :  0\n",
      "Loss: 6.035459518432617\n",
      "Batch :  1\n",
      "Loss: 5.686502456665039\n",
      "Batch :  2\n",
      "Loss: 5.722731590270996\n",
      "Batch :  3\n",
      "Loss: 5.7231245040893555\n",
      "Batch :  4\n",
      "Loss: 5.726558208465576\n",
      "Batch :  5\n",
      "Loss: 5.873055458068848\n",
      "Batch :  6\n",
      "Loss: 5.615411758422852\n",
      "Batch :  7\n",
      "Loss: 5.993980407714844\n",
      "Batch :  8\n",
      "Loss: 5.8712158203125\n",
      "Batch :  9\n",
      "Loss: 5.951200485229492\n",
      "Batch :  10\n",
      "Loss: 6.189069747924805\n",
      "Batch :  11\n",
      "Loss: 6.109272480010986\n",
      "Batch :  12\n",
      "Loss: 5.951673984527588\n",
      "Batch :  13\n",
      "Loss: 5.850650787353516\n",
      "Batch :  14\n",
      "Loss: 5.871762752532959\n",
      "Batch :  15\n",
      "Loss: 6.040384292602539\n",
      "Batch :  16\n",
      "Loss: 5.805588245391846\n",
      "Batch :  17\n",
      "Loss: 5.777975082397461\n",
      "Batch :  18\n",
      "Loss: 5.7968549728393555\n",
      "Batch :  19\n",
      "Loss: 5.856658935546875\n",
      "Batch :  20\n",
      "Loss: 5.667881011962891\n",
      "Batch :  21\n",
      "Loss: 6.305068492889404\n",
      "Batch :  22\n",
      "Loss: 5.9743571281433105\n",
      "Batch :  23\n",
      "Loss: 6.127865314483643\n",
      "Batch :  24\n",
      "Loss: 6.118419647216797\n",
      "Batch :  25\n",
      "Loss: 6.2837677001953125\n",
      "Batch :  26\n",
      "Loss: 5.924826622009277\n",
      "Batch :  27\n",
      "Loss: 5.973792552947998\n",
      "Batch :  28\n",
      "Loss: 6.097434997558594\n",
      "Batch :  29\n",
      "Loss: 5.992473125457764\n",
      "Batch :  30\n",
      "Loss: 5.99398136138916\n",
      "Batch :  31\n",
      "Loss: 5.459383010864258\n",
      "Epoch 4\n",
      "Batch :  0\n",
      "Loss: 5.760858535766602\n",
      "Batch :  1\n",
      "Loss: 5.648531913757324\n",
      "Batch :  2\n",
      "Loss: 5.707090854644775\n",
      "Batch :  3\n",
      "Loss: 5.743725776672363\n",
      "Batch :  4\n",
      "Loss: 5.840556621551514\n",
      "Batch :  5\n",
      "Loss: 5.968993663787842\n",
      "Batch :  6\n",
      "Loss: 5.642507553100586\n",
      "Batch :  7\n",
      "Loss: 5.985357761383057\n",
      "Batch :  8\n",
      "Loss: 5.633970737457275\n",
      "Batch :  9\n",
      "Loss: 5.593984127044678\n",
      "Batch :  10\n",
      "Loss: 5.770197868347168\n",
      "Batch :  11\n",
      "Loss: 5.689182758331299\n",
      "Batch :  12\n",
      "Loss: 5.491236686706543\n",
      "Batch :  13\n",
      "Loss: 5.5934038162231445\n",
      "Batch :  14\n",
      "Loss: 5.613623142242432\n",
      "Batch :  15\n",
      "Loss: 5.876675605773926\n",
      "Batch :  16\n",
      "Loss: 5.593854904174805\n",
      "Batch :  17\n",
      "Loss: 5.482177257537842\n",
      "Batch :  18\n",
      "Loss: 5.465190887451172\n",
      "Batch :  19\n",
      "Loss: 5.422396659851074\n",
      "Batch :  20\n",
      "Loss: 5.349231243133545\n",
      "Batch :  21\n",
      "Loss: 5.6455583572387695\n",
      "Batch :  22\n",
      "Loss: 5.095335006713867\n",
      "Batch :  23\n",
      "Loss: 5.090522766113281\n",
      "Batch :  24\n",
      "Loss: 5.138935089111328\n",
      "Batch :  25\n",
      "Loss: 5.443264961242676\n",
      "Batch :  26\n",
      "Loss: 5.17105770111084\n",
      "Batch :  27\n",
      "Loss: 5.1340861320495605\n",
      "Batch :  28\n",
      "Loss: 5.244542598724365\n",
      "Batch :  29\n",
      "Loss: 5.232945919036865\n",
      "Batch :  30\n",
      "Loss: 5.464345932006836\n",
      "Batch :  31\n",
      "Loss: 4.880892753601074\n",
      "Epoch 5\n",
      "Batch :  0\n",
      "Loss: 5.329124927520752\n",
      "Batch :  1\n",
      "Loss: 5.1429877281188965\n",
      "Batch :  2\n",
      "Loss: 5.3622894287109375\n",
      "Batch :  3\n",
      "Loss: 5.288081645965576\n",
      "Batch :  4\n",
      "Loss: 5.2364935874938965\n",
      "Batch :  5\n",
      "Loss: 5.405645847320557\n",
      "Batch :  6\n",
      "Loss: 4.945751667022705\n",
      "Batch :  7\n",
      "Loss: 5.22817850112915\n",
      "Batch :  8\n",
      "Loss: 4.908242225646973\n",
      "Batch :  9\n",
      "Loss: 5.000779151916504\n",
      "Batch :  10\n",
      "Loss: 5.121096611022949\n",
      "Batch :  11\n",
      "Loss: 5.115811347961426\n",
      "Batch :  12\n",
      "Loss: 4.937376976013184\n",
      "Batch :  13\n",
      "Loss: 5.096794605255127\n",
      "Batch :  14\n",
      "Loss: 5.098138332366943\n",
      "Batch :  15\n",
      "Loss: 5.431612014770508\n",
      "Batch :  16\n",
      "Loss: 5.1980719566345215\n",
      "Batch :  17\n",
      "Loss: 5.089862823486328\n",
      "Batch :  18\n",
      "Loss: 4.930451393127441\n",
      "Batch :  19\n",
      "Loss: 4.747570037841797\n",
      "Batch :  20\n",
      "Loss: 4.606400966644287\n",
      "Batch :  21\n",
      "Loss: 4.964109897613525\n",
      "Batch :  22\n",
      "Loss: 4.779975891113281\n",
      "Batch :  23\n",
      "Loss: 4.953782081604004\n",
      "Batch :  24\n",
      "Loss: 4.966599941253662\n",
      "Batch :  25\n",
      "Loss: 5.180585861206055\n",
      "Batch :  26\n",
      "Loss: 4.687280178070068\n",
      "Batch :  27\n",
      "Loss: 4.58983850479126\n",
      "Batch :  28\n",
      "Loss: 4.728678226470947\n",
      "Batch :  29\n",
      "Loss: 4.609487533569336\n",
      "Batch :  30\n",
      "Loss: 4.7549729347229\n",
      "Batch :  31\n",
      "Loss: 4.003307819366455\n",
      "Saving model\n",
      "Epoch 6\n",
      "Batch :  0\n",
      "Loss: 4.725499153137207\n",
      "Batch :  1\n",
      "Loss: 4.63979959487915\n",
      "Batch :  2\n",
      "Loss: 4.667760372161865\n",
      "Batch :  3\n",
      "Loss: 4.646124839782715\n",
      "Batch :  4\n",
      "Loss: 4.793280601501465\n",
      "Batch :  5\n",
      "Loss: 5.019352436065674\n",
      "Batch :  6\n",
      "Loss: 4.657460689544678\n",
      "Batch :  7\n",
      "Loss: 4.906200408935547\n",
      "Batch :  8\n",
      "Loss: 4.626556873321533\n",
      "Batch :  9\n",
      "Loss: 4.700709342956543\n",
      "Batch :  10\n",
      "Loss: 4.786220550537109\n",
      "Batch :  11\n",
      "Loss: 4.776984691619873\n",
      "Batch :  12\n",
      "Loss: 4.419242858886719\n",
      "Batch :  13\n",
      "Loss: 4.383917808532715\n",
      "Batch :  14\n",
      "Loss: 4.317321300506592\n",
      "Batch :  15\n",
      "Loss: 4.607497692108154\n",
      "Batch :  16\n",
      "Loss: 4.4005818367004395\n",
      "Batch :  17\n",
      "Loss: 4.405843734741211\n",
      "Batch :  18\n",
      "Loss: 4.433296203613281\n",
      "Batch :  19\n",
      "Loss: 4.530012607574463\n",
      "Batch :  20\n",
      "Loss: 4.498225688934326\n",
      "Batch :  21\n",
      "Loss: 5.098546981811523\n",
      "Batch :  22\n",
      "Loss: 4.6360626220703125\n",
      "Batch :  23\n",
      "Loss: 4.525849342346191\n",
      "Batch :  24\n",
      "Loss: 4.433967590332031\n",
      "Batch :  25\n",
      "Loss: 4.625515937805176\n",
      "Batch :  26\n",
      "Loss: 4.330869197845459\n",
      "Batch :  27\n",
      "Loss: 4.320586681365967\n",
      "Batch :  28\n",
      "Loss: 4.429388523101807\n",
      "Batch :  29\n",
      "Loss: 4.317487716674805\n",
      "Batch :  30\n",
      "Loss: 4.490368366241455\n",
      "Batch :  31\n",
      "Loss: 3.6160621643066406\n",
      "Epoch 7\n",
      "Batch :  0\n",
      "Loss: 4.36243200302124\n",
      "Batch :  1\n",
      "Loss: 4.175332069396973\n",
      "Batch :  2\n",
      "Loss: 4.282786846160889\n",
      "Batch :  3\n",
      "Loss: 4.274550437927246\n",
      "Batch :  4\n",
      "Loss: 4.321357727050781\n",
      "Batch :  5\n",
      "Loss: 4.542759418487549\n",
      "Batch :  6\n",
      "Loss: 4.184141635894775\n",
      "Batch :  7\n",
      "Loss: 4.406257152557373\n",
      "Batch :  8\n",
      "Loss: 4.1745147705078125\n",
      "Batch :  9\n",
      "Loss: 4.322195053100586\n",
      "Batch :  10\n",
      "Loss: 4.457062721252441\n",
      "Batch :  11\n",
      "Loss: 4.3461503982543945\n",
      "Batch :  12\n",
      "Loss: 4.115131855010986\n",
      "Batch :  13\n",
      "Loss: 4.104663848876953\n",
      "Batch :  14\n",
      "Loss: 4.034969806671143\n",
      "Batch :  15\n",
      "Loss: 4.379985332489014\n",
      "Batch :  16\n",
      "Loss: 4.155804634094238\n",
      "Batch :  17\n",
      "Loss: 4.140275001525879\n",
      "Batch :  18\n",
      "Loss: 3.971118927001953\n",
      "Batch :  19\n",
      "Loss: 3.9424421787261963\n",
      "Batch :  20\n",
      "Loss: 3.9562389850616455\n",
      "Batch :  21\n",
      "Loss: 4.183396339416504\n",
      "Batch :  22\n",
      "Loss: 3.9686713218688965\n",
      "Batch :  23\n",
      "Loss: 3.981180191040039\n",
      "Batch :  24\n",
      "Loss: 4.072301387786865\n",
      "Batch :  25\n",
      "Loss: 4.164514064788818\n",
      "Batch :  26\n",
      "Loss: 3.8234989643096924\n",
      "Batch :  27\n",
      "Loss: 3.731292247772217\n",
      "Batch :  28\n",
      "Loss: 3.841601848602295\n",
      "Batch :  29\n",
      "Loss: 3.6820921897888184\n",
      "Batch :  30\n",
      "Loss: 3.91666841506958\n",
      "Batch :  31\n",
      "Loss: 2.815737009048462\n",
      "Epoch 8\n",
      "Batch :  0\n",
      "Loss: 3.6837732791900635\n",
      "Batch :  1\n",
      "Loss: 3.6736690998077393\n",
      "Batch :  2\n",
      "Loss: 3.859469413757324\n",
      "Batch :  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.72831392288208\n",
      "Batch :  4\n",
      "Loss: 3.69512939453125\n",
      "Batch :  5\n",
      "Loss: 3.934354066848755\n",
      "Batch :  6\n",
      "Loss: 3.7205159664154053\n",
      "Batch :  7\n",
      "Loss: 4.058916091918945\n",
      "Batch :  8\n",
      "Loss: 3.8494694232940674\n",
      "Batch :  9\n",
      "Loss: 3.8997690677642822\n",
      "Batch :  10\n",
      "Loss: 3.9186880588531494\n",
      "Batch :  11\n",
      "Loss: 3.905054807662964\n",
      "Batch :  12\n",
      "Loss: 3.7119715213775635\n",
      "Batch :  13\n",
      "Loss: 3.7353968620300293\n",
      "Batch :  14\n",
      "Loss: 3.5887691974639893\n",
      "Batch :  15\n",
      "Loss: 3.814040422439575\n",
      "Batch :  16\n",
      "Loss: 3.544013738632202\n",
      "Batch :  17\n",
      "Loss: 3.540208578109741\n",
      "Batch :  18\n",
      "Loss: 3.3428831100463867\n",
      "Batch :  19\n",
      "Loss: 3.3640565872192383\n",
      "Batch :  20\n",
      "Loss: 3.335348129272461\n",
      "Batch :  21\n",
      "Loss: 3.4488658905029297\n",
      "Batch :  22\n",
      "Loss: 3.3032944202423096\n",
      "Batch :  23\n",
      "Loss: 3.4964327812194824\n",
      "Batch :  24\n",
      "Loss: 3.6728591918945312\n",
      "Batch :  25\n",
      "Loss: 3.728938102722168\n",
      "Batch :  26\n",
      "Loss: 3.4259185791015625\n",
      "Batch :  27\n",
      "Loss: 3.3911325931549072\n",
      "Batch :  28\n",
      "Loss: 3.5445642471313477\n",
      "Batch :  29\n",
      "Loss: 3.4456288814544678\n",
      "Batch :  30\n",
      "Loss: 3.6151773929595947\n",
      "Batch :  31\n",
      "Loss: 2.4276232719421387\n",
      "Epoch 9\n",
      "Batch :  0\n",
      "Loss: 3.3847694396972656\n",
      "Batch :  1\n",
      "Loss: 3.3288681507110596\n",
      "Batch :  2\n",
      "Loss: 3.5209672451019287\n",
      "Batch :  3\n",
      "Loss: 3.412100315093994\n",
      "Batch :  4\n",
      "Loss: 3.5041098594665527\n",
      "Batch :  5\n",
      "Loss: 3.6891651153564453\n",
      "Batch :  6\n",
      "Loss: 3.369729518890381\n",
      "Batch :  7\n",
      "Loss: 3.4333019256591797\n",
      "Batch :  8\n",
      "Loss: 3.23262095451355\n",
      "Batch :  9\n",
      "Loss: 3.2934517860412598\n",
      "Batch :  10\n",
      "Loss: 3.2324912548065186\n",
      "Batch :  11\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-3).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.variables_initializer(saved_vars))\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    for batchNum in range(len(batch_input[0])):\n",
    "        print(\"Batch : \",batchNum)\n",
    "        t = session.run([optimizer, loss, question_loss, suppression_loss, expression_loss], {\n",
    "            d_tokens: batch_input[0][batchNum],\n",
    "            d_lengths: batch_input[1][batchNum],\n",
    "            a_labels: batch_input[2][batchNum],\n",
    "            encoder_input_mask: batch_input[3][batchNum],\n",
    "            encoder_lengths: batch_input[4][batchNum],\n",
    "            decoder_inputs: batch_input[5][batchNum],\n",
    "            decoder_labels: batch_input[6][batchNum],\n",
    "            decoder_lengths: batch_input[7][batchNum],\n",
    "            s_answer: batch_input[8][batchNum],\n",
    "            e_context: batch_input[9][batchNum],\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(t[1]))\n",
    "    if t[2] < minQuestionLoss:\n",
    "        SuppresionLossFlag = True\n",
    "        ExpressionLossFlag = True\n",
    "    if(epoch%5 == 0):\n",
    "        print(\"Saving model\")\n",
    "        #saver.save(session, \"qgen-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(session, 'qgen-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = session.run(answer_tags, {\n",
    "    d_tokens: batch_input[0][2],\n",
    "    d_lengths: batch_input[1][2],\n",
    "})\n",
    "answers = np.argmax(answers, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n",
      "Prediction\n",
      " \n",
      "Ground Truth\n",
      "idol gives back is season six featuring $ 185 million in  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(32):\n",
    "    print(\"Prediction\")\n",
    "    printAllAns(answers,2,0)\n",
    "    print(\"Ground Truth\")\n",
    "    printAllAns(batch_input[2][2],2,0)\n",
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDoc(batch,num):\n",
    "    for i in batch_input[0][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "def printQues(batch,num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAnsForQuestion(batch, num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAllAns(answers, batch, num):\n",
    "    for i,word in enumerate(batch_input[0][batch][num]):\n",
    "        if answers[num][i] == 1 :\n",
    "            print(look_up_token_reduced(word),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batchNum = 2\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch_input[0][batchNum].shape[0]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    d_tokens: batch_input[0][batchNum],\n",
    "    d_lengths: batch_input[1][batchNum],\n",
    "    a_labels: batch_input[2][batchNum],\n",
    "    encoder_input_mask: batch_input[3][batchNum],\n",
    "    encoder_lengths: batch_input[4][batchNum],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 333)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[0][batchNum].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[:,:,END_TOKEN] = 0\n",
    "qs = np.argmax(questions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> how much money has american idol raised for charity with its idol gives back specials ? <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['$', '185', 'million']\n",
      "Context:\n",
      "idol gives back is a special charity event started in season six featuring performances by celebrities and various fund-raising initiatives . this event was also held in seasons seven and nine and has raised nearly $ 185 million in total . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> who was the high commissioner of the united nations high commission for human rights ? <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['zeid', \"ra'ad\", 'al', 'hussein']\n",
      "Context:\n",
      "on 17 april 2015 , the sun 's columnist katie hopkins called migrants to britain `` cockroaches '' and `` feral humans '' and said they were `` spreading like the <UNK> '' . her remarks were condemned by the united nations high commission for human rights . in a statement released on 24 april 2015 , high commissioner <UNK> <UNK> al hussein stated that hopkins ' used `` language very similar to that employed by rwanda 's <UNK> newspaper and radio mille <UNK> during the run up to the 1994 genocide '' , and noted that both media organizations were subsequently convicted by an international tribunal of public incitement to commit genocide . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> where does harvard plan to expand to ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['allston']\n",
      "Context:\n",
      "several universities located outside boston have a major presence in the city . harvard university , the nation 's oldest institute of higher education , is centered across the charles river in cambridge but has the majority of its land holdings and a substantial amount of its educational activities in boston . its business , medical , dental , and public health schools are located in boston 's allston and longwood neighborhoods . harvard has plans for additional expansion into allston . the massachusetts institute of technology ( mit ) , which originated in boston and was long known as `` boston tech '' , moved across the river to cambridge in <UNK> tufts university , whose main campus is north of the city in somerville and <UNK> , locates its medical and dental school in boston 's chinatown at tufts medical center , a <UNK> academic medical institution that is home to both a full-service hospital for adults and the floating hospital for children . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> as an example , whose status was downgraded after the civil war ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['traditionally', 'free', 'people', 'of', 'color', 'in', 'louisiana']\n",
      "Context:\n",
      "after the civil war , racial segregation forced african americans to share more of a common lot in society than they might have given widely varying ancestry , educational and economic levels . the binary division altered the separate status of the traditionally free people of color in louisiana , for instance , although they maintained a strong louisiana crÃ©ole culture related to french culture and language , and practice of catholicism . african americans began to create common <UNK> of their multiracial admixture or social and economic stratification . in 20th-century changes , during the rise of the civil rights and black power movements , the african-american community increased its own pressure for people of any portion of african descent to be claimed by the black community to add to its power . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what ku department was rated second in its field ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['special', 'education']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the city management and urban policy program was ranked first in the nation , and the special education program second , by u.s. news & world report 's 2016 rankings . usn & <UNK> also ranked several programs in the top 25 among u.s. universities . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what are the closest relatives of flying theropods ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['deinonychosaurs']\n",
      "Context:\n",
      "the consensus view in contemporary paleontology is that the flying theropods , or <UNK> , are the closest relatives of the <UNK> , which include <UNK> and <UNK> . together , these form a group called <UNK> . some basal members of this group , such as <UNK> , have features which may have enabled them to glide or fly . the most basal <UNK> were very small . this evidence raises the possibility that the ancestor of all <UNK> may have been arboreal , have been able to glide , or both . unlike archaeopteryx and the <UNK> feathered dinosaurs , who primarily ate meat , recent studies suggest that the first <UNK> were omnivores . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what is considered to be the best remaining example of a house from the georgian period and style ? <UNK>  \n",
      "Ground Truth Answer:  ['hammond-harwood', 'house']\n",
      "Context:\n",
      "unlike the baroque style that it replaced , which was mostly used for palaces and churches , and had little representation in the british colonies , simpler georgian styles were widely used by the upper and middle classes . perhaps the best remaining house is the pristine <UNK> house ( 1774 ) in annapolis , maryland , designed by the colonial architect william buckland and modelled on the villa <UNK> at <UNK> , italy as depicted in andrea palladio 's i quattro <UNK> <UNK> ( `` four books of architecture '' ) . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> who rediscovered the laws of inheritance ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['moravian', 'monk', 'gregor', 'mendel']\n",
      "Context:\n",
      "perhaps the most prominent , controversial and far-reaching theory in all of science has been the theory of evolution by natural selection put forward by the british naturalist charles darwin in his book on the origin of species in <UNK> darwin proposed that the features of all living things , including humans , were shaped by natural processes over long periods of time . the theory of evolution in its current form affects almost all areas of biology . implications of evolution on fields outside of pure science have led to both opposition and support from different parts of society , and profoundly influenced the popular understanding of `` man 's place in the universe '' . in the early 20th century , the study of heredity became a major investigation after the rediscovery in 1900 of the laws of inheritance developed by the moravian monk gregor mendel in <UNK> mendel 's laws provided the beginnings of the study of genetics , which became a major field of research for both scientific and industrial research . by 1953 , james d. watson , francis crick and maurice wilkins clarified the basic structure of dna , the genetic material for expressing life in all its forms . in the late 20th century , the possibilities of genetic engineering became practical for the first time , and a massive international effort began in 1990 to map out an entire human genome ( the human genome project ) . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> how often does the assembly of experts meet ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['one', 'week', 'annually']\n",
      "Context:\n",
      "the special clerical court handles crimes allegedly committed by clerics , although it has also taken on cases involving lay people . the special clerical court functions independently of the regular judicial framework and is accountable only to the supreme leader . the court 's rulings are final and can not be appealed . the assembly of experts , which meets for one week annually , comprises 86 `` virtuous and learned '' clerics elected by adult suffrage for eight-year terms . as with the presidential and parliamentary elections , the guardian council determines candidates ' eligibility . the assembly elects the supreme leader and has the constitutional authority to remove the supreme leader from power at any time . it has not challenged any of the supreme leader 's decisions . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what is the differentiation process driven by ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['androgen', 'hormones', ',']\n",
      "Context:\n",
      "as female <UNK> have two x chromosomes and male ones a xy pair , the chromosome y is the responsible for producing male differentiation on the defect female development . the differentiation process is driven by androgen hormones , mainly testosterone and <UNK> ( <UNK> ) . the newly formed testicles in the fetus are responsible for the secretion of androgens , that will cooperate in driving the sexual differentiation of the developing fetus , included its brain . this results in sexual differences between males and females . this fact has led some scientists to test in various ways the result of modifying androgen exposure levels in mammals during fetus and early life . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> how many greeks starved due to nazi occupation ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['over', '100,000']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greece was eventually occupied by the nazis who proceeded to administer athens and thessaloniki , while other regions of the country were given to nazi germany 's partners , fascist italy and bulgaria . the occupation brought about terrible hardships for the greek civilian population . over 100,000 civilians died of starvation during the winter of <UNK> , tens of thousands more died because of reprisals by nazis and collaborators , the country 's economy was ruined and the great majority of greek jews were deported and murdered in nazi concentration camps . the greek resistance , one of the most effective resistance movements in europe fought vehemently against the nazis and their collaborators . the german occupiers committed numerous atrocities , mass executions , and wholesale slaughter of civilians and destruction of towns and villages in reprisals . in the course of the concerted <UNK> campaign , hundreds of villages were systematically torched and almost 1,000,000 greeks left homeless . in total , the germans executed some 21,000 greeks , the bulgarians 40,000 and the italians 9,000 . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what group in the us determines the allowed practices in organic agriculture ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['national', 'organic', 'program']\n",
      "Context:\n",
      "organic cotton is generally understood as cotton from plants not genetically modified and that is certified to be grown without the use of any synthetic agricultural chemicals , such as fertilizers or pesticides . its production also promotes and enhances biodiversity and biological cycles . in the united states , organic cotton plantations are required to enforce the national organic program ( <UNK> ) . this institution determines the allowed practices for pest control , growing , <UNK> , and handling of organic crops . as of 2007 , <UNK> bales of organic cotton were produced in 24 countries , and worldwide production was growing at a rate of more than 50 % per year . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> of what facet of stadium naming do some fans disapprove ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['corporate', 'sponsorship']\n",
      "Context:\n",
      "expansion of highbury was restricted because the east stand had been designated as a grade ii listed building and the other three stands were close to residential properties . these limitations prevented the club from maximising matchday revenue during the 1990s and first decade of the 21st century , putting them in danger of being left behind in the football boom of that time . after considering various options , in 2000 arsenal proposed building a new <UNK> stadium at ashburton grove , since named the emirates stadium , about 500 metres south-west of highbury . the project was initially delayed by red tape and rising costs , and construction was completed in july 2006 , in time for the start of the <UNK> season . the stadium was named after its sponsors , the airline company emirates , with whom the club signed the largest sponsorship deal in english football history , worth around <UNK> million ; some fans referred to the ground as ashburton grove , or the grove , as they did not agree with corporate sponsorship of stadium names . the stadium will be officially known as emirates stadium until at least 2028 , and the airline will be the club 's shirt sponsor until the end of the <UNK> season . from the start of the <UNK> season on , the stands of the stadium have been officially known as north bank , east stand , west stand and clock end . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what is the name of the song that jessica sanchez released first after american idol ? <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['change', 'nothing']\n",
      "Context:\n",
      "phillips released `` home '' as his coronation song , while sanchez released `` change nothing '' . phillips ' `` home '' has since become the best selling of all coronation songs . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what was victoria 's nickname following alberts death ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['widow', 'of', 'windsor']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in march 1861 , victoria 's mother died , with victoria at her side . through reading her mother 's papers , victoria discovered that her mother had loved her deeply ; she was heart-broken , and blamed conroy and lehzen for `` wickedly '' estranging her from her mother . to relieve his wife during her intense and deep grief , albert took on most of her duties , despite being ill himself with chronic stomach trouble . in august , victoria and albert visited their son , the prince of wales , who was attending army manoeuvres near dublin , and spent a few days holidaying in killarney . in november , albert was made aware of gossip that his son had slept with an actress in ireland . appalled , albert travelled to cambridge , where his son was studying , to confront him . by the beginning of december , albert was very unwell . he was diagnosed with typhoid fever by william jenner , and died on 14 december <UNK> victoria was devastated . she blamed her husband 's death on worry over the prince of wales 's philandering . he had been `` killed by that dreadful business '' , she said . she entered a state of mourning and wore black for the remainder of her life . she avoided public appearances , and rarely set foot in london in the following years . her seclusion earned her the nickname `` widow of windsor '' . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> the alps is home to how many people ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['14', 'million', 'people']\n",
      "Context:\n",
      "the alpine region has a strong cultural identity . the traditional culture of farming , cheesemaking , and <UNK> still exists in alpine villages , although the tourist industry began to grow early in the 20th century and expanded greatly after world war ii to become the dominant industry by the end of the century . the winter olympic games have been hosted in the swiss , french , italian , austrian and german alps . at present the region is home to 14 million people and has 120 million annual visitors . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what are terms that are not used outside of switzerland known as ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['helvetisms']\n",
      "Context:\n",
      "the principal official languages ( german , french , and italian ) have terms , not used outside of switzerland , known as <UNK> . german <UNK> are , roughly speaking , a large group of words typical of swiss standard german , which do not appear either in standard german , nor in other german dialects . these include terms from switzerland 's surrounding language cultures ( german <UNK> from french ) , from similar term in another language ( italian <UNK> used not only as act but also as discount from german <UNK> ) . the french spoken in switzerland has similar terms , which are equally known as <UNK> . the most frequent characteristics of <UNK> are in vocabulary , phrases , and pronunciation , but certain <UNK> denote themselves as special in syntax and orthography likewise . <UNK> , one of the prescriptive sources for standard german , is aware of about 3000 <UNK> . current french dictionaries , such as the petit <UNK> , include several hundred <UNK> . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what type of animal was being protected in the area that received the first hcp ? <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['butterflies']\n",
      "Context:\n",
      "the us congress was urged to create the exemption by proponents of a conservation plan on san bruno mountain , california that was drafted in the early 1980s and is the first hcp in the nation . in the conference report on the 1982 amendments , congress specified that it intended the san bruno plan to act `` as a model '' for future conservation plans developed under the incidental take exemption provision and that `` the <UNK> of similar conservation plans should be measured against the san bruno plan '' . congress further noted that the san bruno plan was based on `` an independent exhaustive biological study '' and protected at least 87 % of the habitat of the listed butterflies that led to the development of the hcp . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what types of ice storms sometimes hit richmond ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['freezing', 'rain', 'or', 'glaze']\n",
      "Context:\n",
      "precipitation is rather uniformly distributed throughout the year . however , dry periods lasting several weeks do occur , especially in autumn when long periods of pleasant , mild weather are most common . there is considerable variability in total monthly amounts from year to year so that no one month can be depended upon to be normal . snow has been recorded during seven of the twelve months . falls of 3 inches ( 7.6 cm ) or more within 24 hours occur an average once per year . annual snowfall , however , is usually light , averaging 10.5 inches ( 27 cm ) per season . snow typically remains on the ground only one or two days at a time , but remained for 16 days in 2010 ( january 30 to february 14 ) . ice storms ( freezing rain or glaze ) are not uncommon , but they are seldom severe enough to do any considerable damage . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what does <UNK> mean in english ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['rudder']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the greek-speaking orthodox have collected canons and commentaries upon them in a work known as the <UNK> ( greek : <UNK> , `` rudder '' ) , so named because it is meant to `` steer '' the church . the orthodox christian tradition in general treats its canons more as guidelines than as laws , the bishops adjusting them to cultural and other local circumstances . some orthodox canon scholars point out that , had the ecumenical councils ( which <UNK> in greek ) meant for the canons to be used as laws , they would have called them <UNK> ( laws ) rather than <UNK> ( rules ) , but almost all orthodox conform to them . the dogmatic decisions of the councils , though , are to be obeyed rather than to be treated as guidelines , since they are essential for the church 's unity . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what were the japanese originally going to drop on san diego from their kamikaze planes ? <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['fleas', 'infected', 'with', 'plague', '(', 'yersinia', 'pestis', ')']\n",
      "Context:\n",
      "during world war ii , san diego became a major hub of military and defense activity , due to the presence of so many military installations and defense manufacturers . the city 's population grew rapidly during and after world war ii , more than doubling between 1930 ( <UNK> ) and 1950 ( <UNK> ) . during the final months of the war , the japanese had a plan to target multiple u.s. cities for biological attack , starting with san diego . the plan was called `` operation cherry blossoms at night '' and called for kamikaze planes filled with fleas infected with plague ( <UNK> <UNK> ) to crash into civilian population centers in the city , hoping to spread plague in the city and effectively kill tens of thousands of civilians . the plan was scheduled to launch on september 22 , 1945 , but was not carried out because japan surrendered five weeks earlier . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what was darwin 's original estimate for the amount of time his book would take to write ? <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['five', 'years']\n",
      "Context:\n",
      "a more recent study by science historian john van <UNK> has determined that the idea that darwin delayed publication only dates back to the 1940s , and darwin 's contemporaries thought the time he took was reasonable . darwin always finished one book before starting another . while he was researching , he told many people about his interest in transmutation without causing outrage . he firmly intended to publish , but it was not until september 1854 that he could work on it full-time . his estimate that writing his `` big book '' would take five years was optimistic . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> the <UNK> were under attack by what group ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['pequots']\n",
      "Context:\n",
      "in 1637 a small party of puritans reconnoitered the new haven harbor area and wintered over . in april 1638 , the main party of five hundred puritans who left the massachusetts bay colony under the leadership of the reverend john davenport and the london merchant theophilus eaton sailed into the harbor . these settlers were hoping to establish a ( in their mind ) better theological community , with the government more closely linked to the church than the one they left in massachusetts and sought to take advantage of the excellent port capabilities of the harbor . the <UNK> , who were under attack by neighboring pequots , sold their land to the settlers in return for protection . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> how many legat offices does the fbi operate ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['60']\n",
      "Context:\n",
      "despite its domestic focus , the fbi also maintains a significant international footprint , operating 60 legal <UNK> ( legat ) offices and 15 <UNK> in u.s. embassies and consulates across the globe . these overseas offices exist primarily for the purpose of coordination with foreign security services and do not usually conduct unilateral operations in the host countries . the fbi can and does at times carry out secret activities overseas , just as the cia has a limited domestic function ; these activities generally require coordination across government agencies . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> who was the most famous stagecoach robber in the tucson area ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['william', 'whitney', 'brazelton']\n",
      "Context:\n",
      "from 1877 to 1878 , the area suffered a rash of stagecoach robberies . most notable , however , were the two holdups committed by masked <UNK> william whitney brazelton . brazelton held up two stages in the summer of 1878 near point of mountain station approximately 17 mi ( 27 km ) northwest of tucson . john clum , of tombstone , arizona fame was one of the passengers . brazelton was eventually tracked down and killed on monday august 19 , 1878 , in a mesquite bosque along the santa cruz river 3 miles ( 5 km ) south of tucson by pima county sheriff charles a. <UNK> and his citizen 's posse . brazelton had been suspected of highway robbery not only in the tucson area , but also in the prescott region and silver city , new mexico area as well . brazelton 's crimes prompted john j. valentine , sr. of wells , fargo & co. to send special agent and future pima county sheriff bob paul to investigate . fort lowell , then east of tucson , was established to help protect settlers from apache attacks . in 1882 , frank stilwell was implicated in the murder of morgan earp by cowboy pete spence 's wife , marietta , at the coroner 's inquest on morgan earp 's shooting . the coroner 's jury concluded that spence , stilwell , frederick bode , and florentino `` indian charlie '' cruz were the prime suspects in the assassination of morgan earp . <UNK> deputy u.s. marshal wyatt earp gathered a few trusted friends and accompanied virgil earp and his family as they traveled to benson for a train ride to california . they found stilwell lying in wait for virgil in the tucson station and killed him on the tracks . after killing stilwell , wyatt deputized others and rode on a vendetta , killing three more cowboys over the next few days before leaving the state .  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what museum was formerly called `` museum 25 may '' ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['museum', 'of', 'yugoslav', 'history']\n",
      "Context:\n",
      "tito was interred in a mausoleum in belgrade , which forms part of a memorial complex in the grounds of the museum of yugoslav history ( formerly called `` museum 25 may '' and `` museum of the revolution '' ) . the actual mausoleum is called house of flowers ( <UNK> <UNK> ) and numerous people visit the place as a shrine to `` better times '' . the museum keeps the gifts tito received during his presidency . the collection also includes original prints of los caprichos by francisco goya , and many others . the government of serbia has planned to merge it into the museum of the history of serbia . at the time of his death , speculation began about whether his successors could continue to hold yugoslavia together . ethnic divisions and conflict grew and eventually erupted in a series of yugoslav wars a decade after his death . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> how many terminals do <UNK> have ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['three']\n",
      "Context:\n",
      "<UNK> have three terminals , corresponding to the three layers of <UNK> emitter , a base , and a collector . they are useful in amplifiers because the currents at the emitter and collector are <UNK> by a relatively small base current . in an <UNK> transistor operating in the active region , the <UNK> junction is forward biased ( electrons and holes recombine at the junction ) , and electrons are injected into the base region . because the base is narrow , most of these electrons will diffuse into the <UNK> ( electrons and holes are formed at , and move away from the junction ) <UNK> junction and be swept into the collector ; perhaps one-hundredth of the electrons will recombine in the base , which is the dominant mechanism in the base current . by controlling the number of electrons that can leave the base , the number of electrons entering the collector can be controlled . collector current is approximately Î² ( <UNK> current gain ) times the base current . it is typically greater than 100 for small-signal transistors but can be smaller in transistors designed for high-power applications . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> floyd , fran , and hazel are examples of what that hit the state of north carolina ? <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['hurricanes']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "severe weather occurs regularly in north carolina . on the average , a hurricane hits the state once a decade . destructive hurricanes that have struck the state include hurricane fran , hurricane floyd , and hurricane hazel , the strongest storm to make landfall in the state , as a category 4 in <UNK> hurricane isabel stands out as the most damaging of the 21st century . tropical storms arrive every 3 or 4 years . in addition , many hurricanes and tropical storms graze the state . in some years , several hurricanes or tropical storms can directly strike the state or brush across the coastal areas . only florida and louisiana are hit by hurricanes more often . although many people believe that hurricanes menace only coastal areas , the rare hurricane which moves inland quickly enough can cause severe damage ; for example , in 1989 , hurricane hugo caused heavy damage in charlotte and even as far inland as the blue ridge mountains in the northwestern part of the state . on the average , north carolina has 50 days of thunderstorm activity per year , with some storms becoming severe enough to produce hail , flash floods , and damaging winds . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what type of language is french considered to be ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['romance', 'languages']\n",
      "Context:\n",
      "the native language of the romans was latin . although surviving latin literature consists almost entirely of classical latin , an artificial and highly stylised and polished literary language from the 1st century bc , the actual spoken language was vulgar latin , which significantly differed from classical latin in grammar , vocabulary , and eventually pronunciation . rome 's expansion spread latin throughout europe , and over time vulgar latin evolved and <UNK> in different locations , gradually shifting into a number of distinct romance languages . many of these languages , including french , italian , portuguese , romanian and spanish , flourished , the differences between them growing greater over time . although english is germanic rather than roman in origin , english borrows heavily from latin and latin-derived words . [ citation needed ] <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> how does literature unite members of a society ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['by', 'provoking', 'universal', 'emotions']\n",
      "Context:\n",
      "hogan also explains that the temporal and emotional amount which a person devotes to understanding a <UNK> situation in literature allows literature to be considered <UNK> [ <UNK> ] valid in the study of <UNK> . this can be understood in the sense that literature unites a large community by provoking universal emotions . it also allows readers to access cultural aspects that they are not exposed to thus provoking new emotional experiences . authors choose literary device according to what psychological emotion he or she is attempting to describe , thus certain literary devices are more emotionally effective than others . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> who was the first person to speak on bbc when it was turned back on following world war ii ?  \n",
      "Ground Truth Answer:  ['jasmine', 'bligh']\n",
      "Context:\n",
      "bbc television returned on 7 june 1946 at <UNK> jasmine bligh , one of the original announcers , made the first announcement , saying , <UNK> afternoon everybody . how are you ? do you remember me , jasmine bligh ? ' . the mickey mouse cartoon of 1939 was repeated twenty minutes later . [ unreliable source ? ] alexandra palace was the home base of the channel until the early 1950s when the majority of production moved into the newly acquired lime grove studios . [ original research ? ] <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
      "Ground Truth Question: \n",
      "<START> what academics identified emotions with physiological changes ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['psychophysiologists']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emotions involve different components , such as subjective experience , cognitive processes , expressive behavior , psychophysiological changes , and instrumental behavior . at one time , academics attempted to identify the emotion with one of the components : william james with a subjective experience , behaviorists with instrumental behavior , <UNK> with physiological changes , and so on . more recently , emotion is said to consist of all the components . the different components of emotion are categorized somewhat differently depending on the academic discipline . in psychology and philosophy , emotion typically includes a subjective , conscious experience characterized primarily by psychophysiological expressions , biological reactions , and mental states . a similar <UNK> description of emotion is found in sociology . for example , peggy <UNK> described emotions as involving physiological components , cultural or emotional labels ( e.g. , anger , surprise etc . ) , expressive body actions , and the appraisal of situations and contexts . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# q = set()\n",
    "l = []\n",
    "for i in range(batch_input[0][batchNum].shape[0]):\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, qs[i])\n",
    "    print(\"Generated Question: \" + \" \".join(look_up_token_reduced(token) for token in question))\n",
    "    print(\"Ground Truth Question: \")\n",
    "    printQues(batchNum,i)\n",
    "    print(\"Ground Truth Answer: \", X_train_ans_shuffled[batch_size*batchNum + i])\n",
    "    print(\"Context:\")\n",
    "    printDoc(batchNum,i)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[6][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(batch_input[7][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
