{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 50000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "1510\n",
      "rev\n",
      ".\n",
      "john\n",
      "j.\n",
      "cavanaugh\n",
      ",\n",
      "c.s.c\n",
      ".\n",
      "served\n",
      "more\n",
      "than\n",
      "half\n",
      ",\n",
      "lobund\n",
      "institute\n",
      "for\n",
      "animal\n",
      "studies\n",
      "and\n",
      "medieval\n",
      "institute\n",
      ".\n",
      "hall\n",
      "of\n",
      "liberal\n",
      "arts\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'s\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '``', 'ad', 'me', 'omnes', \"''\", '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in', '1858.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n",
      "['saint', 'bernadette', 'soubirous']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 104)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510\n",
      "['the', 'success', 'of', 'its', 'football', 'team', 'made', 'notre', 'dame', 'a', 'household', 'name', '.', 'the', 'success', 'of', 'note', 'dame', 'reflected', 'rising', 'status', 'of', 'irish', 'americans', 'and', 'catholics', 'in', 'the', '1920s', '.', 'catholics', 'rallied', 'up', 'around', 'the', 'team', 'and', 'listen', 'to', 'the', 'games', 'on', 'the', 'radio', ',', 'especially', 'when', 'it', 'knocked', 'off', 'the', 'schools', 'that', 'symbolized', 'the', 'protestant', 'establishment', 'in', 'america', 'â€”', 'harvard', ',', 'yale', ',', 'princeton', ',', 'and', 'army', '.', 'yet', 'this', 'role', 'as', 'high-profile', 'flagship', 'institution', 'of', 'catholicism', 'made', 'it', 'an', 'easy', 'target', 'of', 'anti-catholicism', '.', 'the', 'most', 'remarkable', 'episode', 'of', 'violence', 'was', 'the', 'clash', 'between', 'notre', 'dame', 'students', 'and', 'the', 'ku', 'klux', 'klan', 'in', '1924.', 'nativism', 'and', 'anti-catholicism', ',', 'especially', 'when', 'directed', 'towards', 'immigrants', ',', 'were', 'cornerstones', 'of', 'the', 'kkk', \"'s\", 'rhetoric', ',', 'and', 'notre', 'dame', 'was', 'seen', 'as', 'a', 'symbol', 'of', 'the', 'threat', 'posed', 'by', 'the', 'catholic', 'church', '.', 'the', 'klan', 'decided', 'to', 'have', 'a', 'week-long', 'klavern', 'in', 'south', 'bend', '.', 'clashes', 'with', 'the', 'student', 'body', 'started', 'on', 'march', '17', ',', 'when', 'students', ',', 'aware', 'of', 'the', 'anti-catholic', 'animosity', ',', 'blocked', 'the', 'klansmen', 'from', 'descending', 'from', 'their', 'trains', 'in', 'the', 'south', 'bend', 'station', 'and', 'ripped', 'the', 'kkk', 'clothes', 'and', 'regalia', '.', 'on', 'may', '19', 'thousands', 'of', 'students', 'massed', 'downtown', 'protesting', 'the', 'klavern', ',', 'and', 'only', 'the', 'arrival', 'of', 'college', 'president', 'fr', '.', 'matthew', 'walsh', 'prevented', 'any', 'further', 'clashes', '.', 'the', 'next', 'day', ',', 'football', 'coach', 'knute', 'rockne', 'spoke', 'at', 'a', 'campus', 'rally', 'and', 'implored', 'the', 'students', 'to', 'obey', 'the', 'college', 'president', 'and', 'refrain', 'from', 'further', 'violence', '.', 'a', 'few', 'days', 'later', 'the', 'klavern', 'broke', 'up', ',', 'but', 'the', 'hostility', 'shown', 'by', 'the', 'students', 'was', 'an', 'omen', 'and', 'a', 'contribution', 'to', 'the', 'downfall', 'of', 'the', 'kkk', 'in', 'indiana', '.']\n",
      "['the', 'ku', 'klux', 'klan']\n",
      "['notre', 'dame', 'students', 'had', 'a', 'showdown', 'in', '1924', 'with', 'which', 'anti-catholic', 'group', '?']\n",
      "['air', 'defence', 'in', 'naval', 'tactics', ',', 'especially', 'within', 'a', 'carrier', 'group', ',', 'is', 'often', 'built', 'around', 'a', 'system', 'of', 'concentric', 'layers', 'with', 'the', 'aircraft', 'carrier', 'at', 'the', 'centre', '.', 'the', 'outer', 'layer', 'will', 'usually', 'be', 'provided', 'by', 'the', 'carrier', \"'s\", 'aircraft', ',', 'specifically', 'its', 'aew', '&', 'c', 'aircraft', 'combined', 'with', 'the', 'cap', '.', 'if', 'an', 'attacker', 'is', 'able', 'to', 'penetrate', 'this', 'layer', ',', 'then', 'the', 'next', 'layers', 'would', 'come', 'from', 'the', 'surface-to-air', 'missiles', 'carried', 'by', 'the', 'carrier', \"'s\", 'escorts', ';', 'the', 'area-defence', 'missiles', ',', 'such', 'as', 'the', 'rim-67', 'standard', ',', 'with', 'a', 'range', 'of', 'up', 'to', '100', 'nmi', ',', 'and', 'the', 'point-defence', 'missiles', ',', 'like', 'the', 'rim-162', 'essm', ',', 'with', 'a', 'range', 'of', 'up', 'to', '30', 'nmi', '.', 'finally', ',', 'virtually', 'every', 'modern', 'warship', 'will', 'be', 'fitted', 'with', 'small-calibre', 'guns', ',', 'including', 'a', 'ciws', ',', 'which', 'is', 'usually', 'a', 'radar-controlled', 'gatling', 'gun', 'of', 'between', '20mm', 'and', '30mm', 'calibre', 'capable', 'of', 'firing', 'several', 'thousand', 'rounds', 'per', 'minute', '.']\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "['aew', '&', 'c', 'aircraft', 'combined', 'with', 'the', 'cap']\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.]\n",
      "['what', 'protects', 'the', 'outer', 'layer', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 1000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 50000, 1)\n"
     ]
    }
   ],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "print(suppression_answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 449)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "18128\n",
      "1439\n",
      "17650\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   22,  1795,   385, 19053,     4,  2629,  4553,  7562, 30963,\n",
       "          35,  3788, 16025,   139,     3,  2592,     5, 24499,    31,\n",
       "           3,  4237,     5,     3,    65,  4329,   104,     4,     3,\n",
       "        2493,   167,     8, 19252,     6,     3, 16025,  6960,     7,\n",
       "        5149,     3,   167,     7,     3,  5646,     4,    51,  2760,\n",
       "         482,     3,  2592,     5, 24499,     7,  7018,  1734,    91,\n",
       "       28738,     4,   601,  4932,     4,     7,     3,   700,     5,\n",
       "           3,   911,     6,    48,    85,   110,  2355,   464,    22,\n",
       "           3,   545,  9146,     9,  2582,   765,     4,     3, 23165,\n",
       "           5,     7,  9146,     9,     3,  4329,     4,     3, 20042,\n",
       "           4,  5304,  8759, 14065,     4, 15324,     4,   338,    22,\n",
       "           3,   700,     5,     3,   911,     4,     3,  5357,   157,\n",
       "           4, 22835,     7, 13628,     4,     3,   393,     5,  2276,\n",
       "        2427,     8,   277,  2642,     4,     3,   855,    73,  1376,\n",
       "           7,     3,   157,     4,   746,   464,     4,     7,     3,\n",
       "       20042,     6,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 449)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "embedding = tf.get_variable(\"embedding\", initializer=reduced_glove)\n",
    "embedding = tf.cast(embedding, dtype=tf.float32)\n",
    "EMBEDDING_DIMENS = glove.shape[1]\n",
    "\n",
    "d_tokens = tf.placeholder(tf.int32, shape=[None, None], name=\"d_tokens\")\n",
    "d_lengths = tf.placeholder(tf.int32, shape=[None], name=\"d_lengths\")\n",
    "\n",
    "\n",
    "document_emb = tf.nn.embedding_lookup(embedding, d_tokens, name=\"document_emb\")\n",
    "document_emb = tf.cast(document_emb, dtype=tf.float64, name=\"casted_document_emb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "forward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "backward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "\n",
    "answer_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, document_emb, d_lengths, dtype=tf.float64,\n",
    "    scope=\"answer_rnn\")\n",
    "\n",
    "answer_outputs = tf.concat(answer_outputs, 2, name=\"answer_output_concat\")\n",
    "\n",
    "answer_outputs = tf.cast(answer_outputs,tf.float32, name=\"answer_output_concat\")\n",
    "\n",
    "answer_tags = tf.layers.dense(inputs=answer_outputs, units=2, name=\"answer_tags\")\n",
    "\n",
    "\n",
    "a_labels = tf.placeholder(tf.int32, shape=[None, None], name=\"a_labels\")\n",
    "\n",
    "answer_mask = tf.sequence_mask(d_lengths, dtype=tf.float32, name=\"answer_mask\")\n",
    "\n",
    "answer_loss = seq2seq.sequence_loss(\n",
    "    logits=answer_tags, targets=a_labels, weights=answer_mask, name=\"answer_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_mask = tf.placeholder(\n",
    "    tf.float32, shape=[None, None, None], name=\"encoder_input_mask\")\n",
    "encoder_inputs = tf.matmul(encoder_input_mask, answer_outputs, name=\"encoder_inputs\")\n",
    "encoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"encoder_lengths\")\n",
    "\n",
    "encoder_cell = tf.contrib.rnn.GRUCell(forward_cell.state_size + backward_cell.state_size)\n",
    "\n",
    "_, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs, encoder_lengths, dtype=tf.float32, scope=\"encoder_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"decoder_inputs\")\n",
    "decoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"decoder_lengths\")\n",
    "\n",
    "decoder_emb = tf.nn.embedding_lookup(embedding, decoder_inputs,name=\"decoder_embedding\")\n",
    "decoder_emb = tf.cast(decoder_emb,tf.float32,name=\"decoder_embedding_cast\")\n",
    "\n",
    "#helper = seq2seq.TrainingHelper(decoder_emb , decoder_lengths2, name=\"helper\")\n",
    "helper = seq2seq.TrainingHelper(decoder_emb , decoder_lengths, name=\"helper\")\n",
    "\n",
    "\n",
    "projection = Dense(embedding.shape[0], use_bias=False, name=\"projection\")\n",
    "\n",
    "decoder_cell = tf.contrib.rnn.GRUCell(encoder_cell.state_size)\n",
    "\n",
    "\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, scope=\"decoder\")\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "decoder_labels = tf.placeholder(tf.int64, shape=[None, None], name=\"decoder_labels\")\n",
    "question_mask = tf.sequence_mask(decoder_lengths ,dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL Loss\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "\n",
    "\n",
    "#Suppression Loss\n",
    "s_answer = tf.placeholder(tf.float32, shape=[None,None,None], name=\"suppression_answer\")\n",
    "lambdaSuppress = 0.5\n",
    "decoder_outputs = tf.Print(decoder_outputs, [decoder_outputs], message=\"This is decoder_outputs: \")\n",
    "suppression_loss = lambdaSuppress * tf.reduce_sum(tf.matmul(tf.exp(decoder_outputs), s_answer))\n",
    "suppression_loss = tf.Print(suppression_loss, [suppression_loss], message=\"This is suppression_loss: \")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Maximize Entropy Loss\n",
    "entropy_loss = tf.matmul(tf.transpose(decoder_outputs),decoder_outputs)\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.add(answer_loss, question_loss, name=\"loss\")\n",
    "loss = tf.add(loss, suppression_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train,max_answer_len),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(*ls):\n",
    "    l =list(zip(*ls))\n",
    "    np.random.shuffle(l)\n",
    "    return zip(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index == 8: # \n",
    "                output.append(inp[start:start+batch_size,:,:]) \n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        maxD = max(inputs[1][start:])\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ]) \n",
    "        elif index ==8:\n",
    "            output.append(inp[start:,:, :]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer]\n",
    "                    ,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('No of batches:', 100)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"No of batches:\",len( batch_input[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saved_vars = []\n",
    "l = len(tf.all_variables())\n",
    "for i,var in enumerate(tf.all_variables()):\n",
    "    print(i,\"/\",l)\n",
    "    saved_vars.append(var)\n",
    "        \n",
    "print(len(saved_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch :  0\n",
      "Loss: 232.2726593017578\n",
      "Batch :  1\n",
      "Loss: 274.4121398925781\n",
      "Batch :  2\n",
      "Loss: 145.31298828125\n",
      "Batch :  3\n",
      "Loss: 73.01394653320312\n",
      "Batch :  4\n",
      "Loss: 479.9945373535156\n",
      "Batch :  5\n",
      "Loss: 21.947721481323242\n",
      "Batch :  6\n",
      "Loss: 36.79529571533203\n",
      "Batch :  7\n",
      "Loss: 30.75307846069336\n",
      "Batch :  8\n",
      "Loss: 12.670621871948242\n",
      "Batch :  9\n",
      "Loss: 111.17853546142578\n",
      "Batch :  10\n",
      "Loss: 83.9248046875\n",
      "Batch :  11\n",
      "Loss: 26.95000457763672\n",
      "Batch :  12\n",
      "Loss: 32.0759391784668\n",
      "Batch :  13\n",
      "Loss: 13.022895812988281\n",
      "Batch :  14\n",
      "Loss: 13.940422058105469\n",
      "Batch :  15\n",
      "Loss: 23.330528259277344\n",
      "Batch :  16\n",
      "Loss: 9.2161865234375\n",
      "Batch :  17\n",
      "Loss: 12.02365493774414\n",
      "Batch :  18\n",
      "Loss: 23.711360931396484\n",
      "Batch :  19\n",
      "Loss: 10.296273231506348\n",
      "Batch :  20\n",
      "Loss: 11.635177612304688\n",
      "Batch :  21\n",
      "Loss: 9.511051177978516\n",
      "Batch :  22\n",
      "Loss: 9.635671615600586\n",
      "Batch :  23\n",
      "Loss: 11.360706329345703\n",
      "Batch :  24\n",
      "Loss: 11.241825103759766\n",
      "Batch :  25\n",
      "Loss: 10.623144149780273\n",
      "Batch :  26\n",
      "Loss: 9.375879287719727\n",
      "Batch :  27\n",
      "Loss: 9.68723201751709\n",
      "Batch :  28\n",
      "Loss: 10.364550590515137\n",
      "Batch :  29\n",
      "Loss: 9.77974796295166\n",
      "Batch :  30\n",
      "Loss: 9.295928955078125\n",
      "Batch :  31\n",
      "Loss: 10.255960464477539\n",
      "Batch :  32\n",
      "Loss: 10.813626289367676\n",
      "Batch :  33\n",
      "Loss: 9.977089881896973\n",
      "Batch :  34\n",
      "Loss: 9.023675918579102\n",
      "Batch :  35\n",
      "Loss: 9.256946563720703\n",
      "Batch :  36\n",
      "Loss: 9.271872520446777\n",
      "Batch :  37\n",
      "Loss: 9.737834930419922\n",
      "Batch :  38\n",
      "Loss: 9.559789657592773\n",
      "Batch :  39\n",
      "Loss: 9.33320140838623\n",
      "Batch :  40\n",
      "Loss: 9.869634628295898\n",
      "Batch :  41\n",
      "Loss: 9.140844345092773\n",
      "Batch :  42\n",
      "Loss: 9.951233863830566\n",
      "Batch :  43\n",
      "Loss: 9.131817817687988\n",
      "Batch :  44\n",
      "Loss: 8.315589904785156\n",
      "Batch :  45\n",
      "Loss: 9.272165298461914\n",
      "Batch :  46\n",
      "Loss: 9.741063117980957\n",
      "Batch :  47\n",
      "Loss: 8.778193473815918\n",
      "Batch :  48\n",
      "Loss: 8.086614608764648\n",
      "Batch :  49\n",
      "Loss: 8.454437255859375\n",
      "Batch :  50\n",
      "Loss: 8.035628318786621\n",
      "Batch :  51\n",
      "Loss: 8.96891975402832\n",
      "Batch :  52\n",
      "Loss: 8.659276008605957\n",
      "Batch :  53\n",
      "Loss: 9.12028980255127\n",
      "Batch :  54\n",
      "Loss: 9.17967414855957\n",
      "Batch :  55\n",
      "Loss: 8.67556095123291\n",
      "Batch :  56\n",
      "Loss: 9.08906364440918\n",
      "Batch :  57\n",
      "Loss: 8.604449272155762\n",
      "Batch :  58\n",
      "Loss: 8.601884841918945\n",
      "Batch :  59\n",
      "Loss: 8.86963939666748\n",
      "Batch :  60\n",
      "Loss: 8.734637260437012\n",
      "Batch :  61\n",
      "Loss: 8.630623817443848\n",
      "Batch :  62\n",
      "Loss: 8.370065689086914\n",
      "Batch :  63\n",
      "Loss: 9.704099655151367\n",
      "Batch :  64\n",
      "Loss: 8.481219291687012\n",
      "Batch :  65\n",
      "Loss: 7.962943077087402\n",
      "Batch :  66\n",
      "Loss: 8.834004402160645\n",
      "Batch :  67\n",
      "Loss: 8.428088188171387\n",
      "Batch :  68\n",
      "Loss: 7.866187572479248\n",
      "Batch :  69\n",
      "Loss: 8.613356590270996\n",
      "Batch :  70\n",
      "Loss: 9.177970886230469\n",
      "Batch :  71\n",
      "Loss: 7.053638935089111\n",
      "Batch :  72\n",
      "Loss: 8.184643745422363\n",
      "Batch :  73\n",
      "Loss: 7.916394233703613\n",
      "Batch :  74\n",
      "Loss: 8.165443420410156\n",
      "Batch :  75\n",
      "Loss: 8.23493766784668\n",
      "Batch :  76\n",
      "Loss: 7.816799163818359\n",
      "Batch :  77\n",
      "Loss: 8.391011238098145\n",
      "Batch :  78\n",
      "Loss: 7.661522388458252\n",
      "Batch :  79\n",
      "Loss: 8.64224910736084\n",
      "Batch :  80\n",
      "Loss: 8.347895622253418\n",
      "Batch :  81\n",
      "Loss: 7.959545612335205\n",
      "Batch :  82\n",
      "Loss: 8.257291793823242\n",
      "Batch :  83\n",
      "Loss: 8.362626075744629\n",
      "Batch :  84\n",
      "Loss: 7.992910385131836\n",
      "Batch :  85\n",
      "Loss: 7.406904697418213\n",
      "Batch :  86\n",
      "Loss: 7.993139266967773\n",
      "Batch :  87\n",
      "Loss: 8.551819801330566\n",
      "Batch :  88\n",
      "Loss: 9.226177215576172\n",
      "Batch :  89\n",
      "Loss: 8.371808052062988\n",
      "Batch :  90\n",
      "Loss: 7.981657028198242\n",
      "Batch :  91\n",
      "Loss: 7.8954668045043945\n",
      "Batch :  92\n",
      "Loss: 7.776066780090332\n",
      "Batch :  93\n",
      "Loss: 8.283602714538574\n",
      "Batch :  94\n",
      "Loss: 7.837263584136963\n",
      "Batch :  95\n",
      "Loss: 7.955399990081787\n",
      "Batch :  96\n",
      "Loss: 7.844387531280518\n",
      "Batch :  97\n",
      "Loss: 8.05117130279541\n",
      "Batch :  98\n",
      "Loss: 7.8514509201049805\n",
      "Batch :  99\n",
      "Loss: 7.912075042724609\n",
      "Epoch 2\n",
      "Batch :  0\n",
      "Loss: 7.129100322723389\n",
      "Batch :  1\n",
      "Loss: 6.782125473022461\n",
      "Batch :  2\n",
      "Loss: 6.7735090255737305\n",
      "Batch :  3\n",
      "Loss: 6.186906814575195\n",
      "Batch :  4\n",
      "Loss: 7.175755023956299\n",
      "Batch :  5\n",
      "Loss: 6.829501152038574\n",
      "Batch :  6\n",
      "Loss: 6.536113739013672\n",
      "Batch :  7\n",
      "Loss: 6.773262023925781\n",
      "Batch :  8\n",
      "Loss: 6.8452935218811035\n",
      "Batch :  9\n",
      "Loss: 6.799002647399902\n",
      "Batch :  10\n",
      "Loss: 7.190500259399414\n",
      "Batch :  11\n",
      "Loss: 6.807187080383301\n",
      "Batch :  12\n",
      "Loss: 7.1719536781311035\n",
      "Batch :  13\n",
      "Loss: 6.9015374183654785\n",
      "Batch :  14\n",
      "Loss: 7.830303192138672\n",
      "Batch :  15\n",
      "Loss: 7.361542224884033\n",
      "Batch :  16\n",
      "Loss: 6.967647075653076\n",
      "Batch :  17\n",
      "Loss: 6.669289588928223\n",
      "Batch :  18\n",
      "Loss: 6.7668962478637695\n",
      "Batch :  19\n",
      "Loss: 6.969638824462891\n",
      "Batch :  20\n",
      "Loss: 7.526083469390869\n",
      "Batch :  21\n",
      "Loss: 6.997363090515137\n",
      "Batch :  22\n",
      "Loss: 6.7612786293029785\n",
      "Batch :  23\n",
      "Loss: 6.9144792556762695\n",
      "Batch :  24\n",
      "Loss: 7.248991966247559\n",
      "Batch :  25\n",
      "Loss: 7.300107479095459\n",
      "Batch :  26\n",
      "Loss: 6.562769889831543\n",
      "Batch :  27\n",
      "Loss: 7.020446300506592\n",
      "Batch :  28\n",
      "Loss: 7.569175720214844\n",
      "Batch :  29\n",
      "Loss: 7.484185218811035\n",
      "Batch :  30\n",
      "Loss: 6.761236190795898\n",
      "Batch :  31\n",
      "Loss: 7.248763084411621\n",
      "Batch :  32\n",
      "Loss: 7.592991828918457\n",
      "Batch :  33\n",
      "Loss: 7.166812419891357\n",
      "Batch :  34\n",
      "Loss: 6.6464948654174805\n",
      "Batch :  35\n",
      "Loss: 6.804686546325684\n",
      "Batch :  36\n",
      "Loss: 6.965339660644531\n",
      "Batch :  37\n",
      "Loss: 7.277218818664551\n",
      "Batch :  38\n",
      "Loss: 7.023250102996826\n",
      "Batch :  39\n",
      "Loss: 6.975137233734131\n",
      "Batch :  40\n",
      "Loss: 7.349024295806885\n",
      "Batch :  41\n",
      "Loss: 6.89453125\n",
      "Batch :  42\n",
      "Loss: 7.212503910064697\n",
      "Batch :  43\n",
      "Loss: 6.563173294067383\n",
      "Batch :  44\n",
      "Loss: 6.274321556091309\n",
      "Batch :  45\n",
      "Loss: 7.1305999755859375\n",
      "Batch :  46\n",
      "Loss: 7.523660182952881\n",
      "Batch :  47\n",
      "Loss: 6.622244834899902\n",
      "Batch :  48\n",
      "Loss: 6.24680757522583\n",
      "Batch :  49\n",
      "Loss: 6.543378829956055\n",
      "Batch :  50\n",
      "Loss: 6.326476097106934\n",
      "Batch :  51\n",
      "Loss: 7.2150797843933105\n",
      "Batch :  52\n",
      "Loss: 6.7728705406188965\n",
      "Batch :  53\n",
      "Loss: 7.2374267578125\n",
      "Batch :  54\n",
      "Loss: 7.283838748931885\n",
      "Batch :  55\n",
      "Loss: 6.870811462402344\n",
      "Batch :  56\n",
      "Loss: 7.233649730682373\n",
      "Batch :  57\n",
      "Loss: 6.846151351928711\n",
      "Batch :  58\n",
      "Loss: 6.835734844207764\n",
      "Batch :  59\n",
      "Loss: 6.8316240310668945\n",
      "Batch :  60\n",
      "Loss: 6.857962608337402\n",
      "Batch :  61\n",
      "Loss: 6.854115962982178\n",
      "Batch :  62\n",
      "Loss: 6.768240928649902\n",
      "Batch :  63\n",
      "Loss: 7.896218776702881\n",
      "Batch :  64\n",
      "Loss: 6.902626037597656\n",
      "Batch :  65\n",
      "Loss: 6.697841644287109\n",
      "Batch :  66\n",
      "Loss: 6.900036811828613\n",
      "Batch :  67\n",
      "Loss: 6.7186455726623535\n",
      "Batch :  68\n",
      "Loss: 6.3664116859436035\n",
      "Batch :  69\n",
      "Loss: 6.818862438201904\n",
      "Batch :  70\n",
      "Loss: 7.204861640930176\n",
      "Batch :  71\n",
      "Loss: 5.889599800109863\n",
      "Batch :  72\n",
      "Loss: 6.753456115722656\n",
      "Batch :  73\n",
      "Loss: 6.608509063720703\n",
      "Batch :  74\n",
      "Loss: 6.745118141174316\n",
      "Batch :  75\n",
      "Loss: 7.006747722625732\n",
      "Batch :  76\n",
      "Loss: 6.586580753326416\n",
      "Batch :  77\n",
      "Loss: 6.923498630523682\n",
      "Batch :  78\n",
      "Loss: 6.401663303375244\n",
      "Batch :  79\n",
      "Loss: 7.1740546226501465\n",
      "Batch :  80\n",
      "Loss: 6.845178604125977\n",
      "Batch :  81\n",
      "Loss: 6.386882305145264\n",
      "Batch :  82\n",
      "Loss: 6.689085960388184\n",
      "Batch :  83\n",
      "Loss: 6.885692596435547\n",
      "Batch :  84\n",
      "Loss: 6.377255439758301\n",
      "Batch :  85\n",
      "Loss: 6.158921718597412\n",
      "Batch :  86\n",
      "Loss: 6.661503791809082\n",
      "Batch :  87\n",
      "Loss: 7.128039836883545\n",
      "Batch :  88\n",
      "Loss: 7.510021686553955\n",
      "Batch :  89\n",
      "Loss: 6.716293811798096\n",
      "Batch :  90\n",
      "Loss: 6.7876434326171875\n",
      "Batch :  91\n",
      "Loss: 6.592529773712158\n",
      "Batch :  92\n",
      "Loss: 6.808562755584717\n",
      "Batch :  93\n",
      "Loss: 6.584743022918701\n",
      "Batch :  94\n",
      "Loss: 6.677488327026367\n",
      "Batch :  95\n",
      "Loss: 6.648343086242676\n",
      "Batch :  96\n",
      "Loss: 6.741127014160156\n",
      "Batch :  97\n",
      "Loss: 6.929437637329102\n",
      "Batch :  98\n",
      "Loss: 6.8097615242004395\n",
      "Batch :  99\n",
      "Loss: 6.827050685882568\n",
      "Epoch 3\n",
      "Batch :  0\n",
      "Loss: 6.655181884765625\n",
      "Batch :  1\n",
      "Loss: 6.661255359649658\n",
      "Batch :  2\n",
      "Loss: 6.761043548583984\n",
      "Batch :  3\n",
      "Loss: 6.342243194580078\n",
      "Batch :  4\n",
      "Loss: 7.13351583480835\n",
      "Batch :  5\n",
      "Loss: 6.8823137283325195\n",
      "Batch :  6\n",
      "Loss: 6.528284549713135\n",
      "Batch :  7\n",
      "Loss: 6.6000752449035645\n",
      "Batch :  8\n",
      "Loss: 6.769037246704102\n",
      "Batch :  9\n",
      "Loss: 6.722726821899414\n",
      "Batch :  10\n",
      "Loss: 6.8368754386901855\n",
      "Batch :  11\n",
      "Loss: 6.467215061187744\n",
      "Batch :  12\n",
      "Loss: 6.884393215179443\n",
      "Batch :  13\n",
      "Loss: 6.610775470733643\n",
      "Batch :  14\n",
      "Loss: 7.408811092376709\n",
      "Batch :  15\n",
      "Loss: 6.996457576751709\n",
      "Batch :  16\n",
      "Loss: 6.572139739990234\n",
      "Batch :  17\n",
      "Loss: 6.2634077072143555\n",
      "Batch :  18\n",
      "Loss: 6.459613800048828\n",
      "Batch :  19\n",
      "Loss: 6.5613203048706055\n",
      "Batch :  20\n",
      "Loss: 7.121649265289307\n",
      "Batch :  21\n",
      "Loss: 6.68679666519165\n",
      "Batch :  22\n",
      "Loss: 6.423131465911865\n",
      "Batch :  23\n",
      "Loss: 6.652928352355957\n",
      "Batch :  24\n",
      "Loss: 7.049229145050049\n",
      "Batch :  25\n",
      "Loss: 6.997120380401611\n",
      "Batch :  26\n",
      "Loss: 6.339169979095459\n",
      "Batch :  27\n",
      "Loss: 6.85019063949585\n",
      "Batch :  28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.423376083374023\n",
      "Batch :  29\n",
      "Loss: 7.185211658477783\n",
      "Batch :  30\n",
      "Loss: 6.504748344421387\n",
      "Batch :  31\n",
      "Loss: 7.012627601623535\n",
      "Batch :  32\n",
      "Loss: 7.206829071044922\n",
      "Batch :  33\n",
      "Loss: 6.905035018920898\n",
      "Batch :  34\n",
      "Loss: 6.394003868103027\n",
      "Batch :  35\n",
      "Loss: 6.274222373962402\n",
      "Batch :  36\n",
      "Loss: 6.606213569641113\n",
      "Batch :  37\n",
      "Loss: 6.856496810913086\n",
      "Batch :  38\n",
      "Loss: 6.688869953155518\n",
      "Batch :  39\n",
      "Loss: 6.5604705810546875\n",
      "Batch :  40\n",
      "Loss: 6.983190536499023\n",
      "Batch :  41\n",
      "Loss: 6.574142932891846\n",
      "Batch :  42\n",
      "Loss: 6.862995624542236\n",
      "Batch :  43\n",
      "Loss: 6.281550407409668\n",
      "Batch :  44\n",
      "Loss: 5.911989212036133\n",
      "Batch :  45\n",
      "Loss: 6.7705817222595215\n",
      "Batch :  46\n",
      "Loss: 7.308733940124512\n",
      "Batch :  47\n",
      "Loss: 6.354945182800293\n",
      "Batch :  48\n",
      "Loss: 5.946995258331299\n",
      "Batch :  49\n",
      "Loss: 6.197289943695068\n",
      "Batch :  50\n",
      "Loss: 6.007554531097412\n",
      "Batch :  51\n",
      "Loss: 6.88509464263916\n",
      "Batch :  52\n",
      "Loss: 6.438209056854248\n",
      "Batch :  53\n",
      "Loss: 6.752099514007568\n",
      "Batch :  54\n",
      "Loss: 6.8063225746154785\n",
      "Batch :  55\n",
      "Loss: 6.3842291831970215\n",
      "Batch :  56\n",
      "Loss: 6.692368030548096\n",
      "Batch :  57\n",
      "Loss: 6.4071364402771\n",
      "Batch :  58\n",
      "Loss: 6.392612934112549\n",
      "Batch :  59\n",
      "Loss: 6.292265892028809\n",
      "Batch :  60\n",
      "Loss: 6.381744384765625\n",
      "Batch :  61\n",
      "Loss: 6.4127583503723145\n",
      "Batch :  62\n",
      "Loss: 6.317294597625732\n",
      "Batch :  63\n",
      "Loss: 7.361962795257568\n",
      "Batch :  64\n",
      "Loss: 6.551675796508789\n",
      "Batch :  65\n",
      "Loss: 6.355296611785889\n",
      "Batch :  66\n",
      "Loss: 6.521302223205566\n",
      "Batch :  67\n",
      "Loss: 6.370680809020996\n",
      "Batch :  68\n",
      "Loss: 6.197622299194336\n",
      "Batch :  69\n",
      "Loss: 6.732568740844727\n",
      "Batch :  70\n",
      "Loss: 6.735085487365723\n",
      "Batch :  71\n",
      "Loss: 5.623098850250244\n",
      "Batch :  72\n",
      "Loss: 6.41133975982666\n",
      "Batch :  73\n",
      "Loss: 6.34412956237793\n",
      "Batch :  74\n",
      "Loss: 6.344516754150391\n",
      "Batch :  75\n",
      "Loss: 6.767674922943115\n",
      "Batch :  76\n",
      "Loss: 6.32695198059082\n",
      "Batch :  77\n",
      "Loss: 6.560360431671143\n",
      "Batch :  78\n",
      "Loss: 6.134200572967529\n",
      "Batch :  79\n",
      "Loss: 6.837085247039795\n",
      "Batch :  80\n",
      "Loss: 6.547855854034424\n",
      "Batch :  81\n",
      "Loss: 5.970581531524658\n",
      "Batch :  82\n",
      "Loss: 6.444952011108398\n",
      "Batch :  83\n",
      "Loss: 6.554951190948486\n",
      "Batch :  84\n",
      "Loss: 6.920167922973633\n",
      "Batch :  85\n",
      "Loss: 5.87636661529541\n",
      "Batch :  86\n",
      "Loss: 6.432694911956787\n",
      "Batch :  87\n",
      "Loss: 6.955077648162842\n",
      "Batch :  88\n",
      "Loss: 7.126300811767578\n",
      "Batch :  89\n",
      "Loss: 6.661579608917236\n",
      "Batch :  90\n",
      "Loss: 6.439834117889404\n",
      "Batch :  91\n",
      "Loss: 6.374147415161133\n",
      "Batch :  92\n",
      "Loss: 6.405665874481201\n",
      "Batch :  93\n",
      "Loss: 6.344784736633301\n",
      "Batch :  94\n",
      "Loss: 6.251388072967529\n",
      "Batch :  95\n",
      "Loss: 6.404877662658691\n",
      "Batch :  96\n",
      "Loss: 6.580718040466309\n",
      "Batch :  97\n",
      "Loss: 6.824346542358398\n",
      "Batch :  98\n",
      "Loss: 6.507887840270996\n",
      "Batch :  99\n",
      "Loss: 6.472424030303955\n",
      "Epoch 4\n",
      "Batch :  0\n",
      "Loss: 6.301507472991943\n",
      "Batch :  1\n",
      "Loss: 6.272082805633545\n",
      "Batch :  2\n",
      "Loss: 6.3260908126831055\n",
      "Batch :  3\n",
      "Loss: 5.983345985412598\n",
      "Batch :  4\n",
      "Loss: 6.6899495124816895\n",
      "Batch :  5\n",
      "Loss: 6.467689037322998\n",
      "Batch :  6\n",
      "Loss: 6.135175704956055\n",
      "Batch :  7\n",
      "Loss: 6.312686443328857\n",
      "Batch :  8\n",
      "Loss: 6.376755714416504\n",
      "Batch :  9\n",
      "Loss: 6.282834529876709\n",
      "Batch :  10\n",
      "Loss: 6.4395647048950195\n",
      "Batch :  11\n",
      "Loss: 6.219225883483887\n",
      "Batch :  12\n",
      "Loss: 6.571906566619873\n",
      "Batch :  13\n",
      "Loss: 6.3281426429748535\n",
      "Batch :  14\n",
      "Loss: 7.011979103088379\n",
      "Batch :  15\n",
      "Loss: 6.591087341308594\n",
      "Batch :  16\n",
      "Loss: 6.209094524383545\n",
      "Batch :  17\n",
      "Loss: 5.900580883026123\n",
      "Batch :  18\n",
      "Loss: 6.05136775970459\n",
      "Batch :  19\n",
      "Loss: 6.241818904876709\n",
      "Batch :  20\n",
      "Loss: 6.826451301574707\n",
      "Batch :  21\n",
      "Loss: 6.373388290405273\n",
      "Batch :  22\n",
      "Loss: 6.0606689453125\n",
      "Batch :  23\n",
      "Loss: 6.283478260040283\n",
      "Batch :  24\n",
      "Loss: 6.586778163909912\n",
      "Batch :  25\n",
      "Loss: 6.545671463012695\n",
      "Batch :  26\n",
      "Loss: 5.9556965827941895\n",
      "Batch :  27\n",
      "Loss: 6.342605113983154\n",
      "Batch :  28\n",
      "Loss: 6.913417816162109\n",
      "Batch :  29\n",
      "Loss: 6.643887996673584\n",
      "Batch :  30\n",
      "Loss: 6.080857276916504\n",
      "Batch :  31\n",
      "Loss: 6.603701114654541\n",
      "Batch :  32\n",
      "Loss: 6.626351356506348\n",
      "Batch :  33\n",
      "Loss: 6.514187812805176\n",
      "Batch :  34\n",
      "Loss: 6.027289390563965\n",
      "Batch :  35\n",
      "Loss: 5.928860664367676\n",
      "Batch :  36\n",
      "Loss: 6.240345001220703\n",
      "Batch :  37\n",
      "Loss: 6.527201175689697\n",
      "Batch :  38\n",
      "Loss: 6.220199108123779\n",
      "Batch :  39\n",
      "Loss: 6.004311561584473\n",
      "Batch :  40\n",
      "Loss: 6.34049654006958\n",
      "Batch :  41\n",
      "Loss: 6.103687763214111\n",
      "Batch :  42\n",
      "Loss: 6.243205547332764\n",
      "Batch :  43\n",
      "Loss: 5.767648696899414\n",
      "Batch :  44\n",
      "Loss: 5.397273063659668\n",
      "Batch :  45\n",
      "Loss: 6.038577079772949\n",
      "Batch :  46\n",
      "Loss: 6.616699695587158\n",
      "Batch :  47\n",
      "Loss: 5.82204532623291\n",
      "Batch :  48\n",
      "Loss: 5.404825687408447\n",
      "Batch :  49\n",
      "Loss: 5.7928080558776855\n",
      "Batch :  50\n",
      "Loss: 5.570786952972412\n",
      "Batch :  51\n",
      "Loss: 6.40941858291626\n",
      "Batch :  52\n",
      "Loss: 6.028369903564453\n",
      "Batch :  53\n",
      "Loss: 6.256179332733154\n",
      "Batch :  54\n",
      "Loss: 6.537028789520264\n",
      "Batch :  55\n",
      "Loss: 6.137068748474121\n",
      "Batch :  56\n",
      "Loss: 6.3612141609191895\n",
      "Batch :  57\n",
      "Loss: 6.02640438079834\n",
      "Batch :  58\n",
      "Loss: 6.113578796386719\n",
      "Batch :  59\n",
      "Loss: 5.853487014770508\n",
      "Batch :  60\n",
      "Loss: 6.264005184173584\n",
      "Batch :  61\n",
      "Loss: 6.218731880187988\n",
      "Batch :  62\n",
      "Loss: 5.983060836791992\n",
      "Batch :  63\n",
      "Loss: 6.834757328033447\n",
      "Batch :  64\n",
      "Loss: 6.0551838874816895\n",
      "Batch :  65\n",
      "Loss: 5.944175720214844\n",
      "Batch :  66\n",
      "Loss: 6.081905841827393\n",
      "Batch :  67\n",
      "Loss: 5.874929904937744\n",
      "Batch :  68\n",
      "Loss: 5.774646282196045\n",
      "Batch :  69\n",
      "Loss: 6.254256248474121\n",
      "Batch :  70\n",
      "Loss: 6.180791854858398\n",
      "Batch :  71\n",
      "Loss: 5.241730213165283\n",
      "Batch :  72\n",
      "Loss: 5.971649646759033\n",
      "Batch :  73\n",
      "Loss: 5.841676235198975\n",
      "Batch :  74\n",
      "Loss: 6.035817623138428\n",
      "Batch :  75\n",
      "Loss: 6.356278896331787\n",
      "Batch :  76\n",
      "Loss: 5.863860607147217\n",
      "Batch :  77\n",
      "Loss: 6.207350730895996\n",
      "Batch :  78\n",
      "Loss: 5.779851913452148\n",
      "Batch :  79\n",
      "Loss: 6.6400017738342285\n",
      "Batch :  80\n",
      "Loss: 6.4212775230407715\n",
      "Batch :  81\n",
      "Loss: 5.864056587219238\n",
      "Batch :  82\n",
      "Loss: 6.210371494293213\n",
      "Batch :  83\n",
      "Loss: 6.2446794509887695\n",
      "Batch :  84\n",
      "Loss: 5.754605770111084\n",
      "Batch :  85\n",
      "Loss: 5.531683921813965\n",
      "Batch :  86\n",
      "Loss: 6.261166572570801\n",
      "Batch :  87\n",
      "Loss: 6.550361156463623\n",
      "Batch :  88\n",
      "Loss: 6.562417030334473\n",
      "Batch :  89\n",
      "Loss: 6.2197699546813965\n",
      "Batch :  90\n",
      "Loss: 6.022047996520996\n",
      "Batch :  91\n",
      "Loss: 6.072547912597656\n",
      "Batch :  92\n",
      "Loss: 5.931793212890625\n",
      "Batch :  93\n",
      "Loss: 5.715800762176514\n",
      "Batch :  94\n",
      "Loss: 5.825791835784912\n",
      "Batch :  95\n",
      "Loss: 5.778731346130371\n",
      "Batch :  96\n",
      "Loss: 5.915650367736816\n",
      "Batch :  97\n",
      "Loss: 6.128933429718018\n",
      "Batch :  98\n",
      "Loss: 6.037578582763672\n",
      "Batch :  99\n",
      "Loss: 5.9018073081970215\n",
      "Epoch 5\n",
      "Batch :  0\n",
      "Loss: 5.855992317199707\n",
      "Batch :  1\n",
      "Loss: 5.980204105377197\n",
      "Batch :  2\n",
      "Loss: 6.055727958679199\n",
      "Batch :  3\n",
      "Loss: 5.835606575012207\n",
      "Batch :  4\n",
      "Loss: 6.60706901550293\n",
      "Batch :  5\n",
      "Loss: 6.172614097595215\n",
      "Batch :  6\n",
      "Loss: 6.037182331085205\n",
      "Batch :  7\n",
      "Loss: 6.041616916656494\n",
      "Batch :  8\n",
      "Loss: 6.129177093505859\n",
      "Batch :  9\n",
      "Loss: 6.0608673095703125\n",
      "Batch :  10\n",
      "Loss: 6.075821399688721\n",
      "Batch :  11\n",
      "Loss: 6.000100612640381\n",
      "Batch :  12\n",
      "Loss: 6.246374607086182\n",
      "Batch :  13\n",
      "Loss: 5.989589691162109\n",
      "Batch :  14\n",
      "Loss: 6.458518981933594\n",
      "Batch :  15\n",
      "Loss: 6.291605472564697\n",
      "Batch :  16\n",
      "Loss: 5.823253631591797\n",
      "Batch :  17\n",
      "Loss: 5.6465229988098145\n",
      "Batch :  18\n",
      "Loss: 5.7111921310424805\n",
      "Batch :  19\n",
      "Loss: 5.924427032470703\n",
      "Batch :  20\n",
      "Loss: 6.425292491912842\n",
      "Batch :  21\n",
      "Loss: 5.842363357543945\n",
      "Batch :  22\n",
      "Loss: 5.7431721687316895\n",
      "Batch :  23\n",
      "Loss: 5.986660480499268\n",
      "Batch :  24\n",
      "Loss: 6.368145942687988\n",
      "Batch :  25\n",
      "Loss: 6.267817497253418\n",
      "Batch :  26\n",
      "Loss: 5.618324279785156\n",
      "Batch :  27\n",
      "Loss: 6.063157558441162\n",
      "Batch :  28\n",
      "Loss: 6.715451717376709\n",
      "Batch :  29\n",
      "Loss: 6.405186176300049\n",
      "Batch :  30\n",
      "Loss: 5.8724775314331055\n",
      "Batch :  31\n",
      "Loss: 6.381052494049072\n",
      "Batch :  32\n",
      "Loss: 6.304964542388916\n",
      "Batch :  33\n",
      "Loss: 6.123061656951904\n",
      "Batch :  34\n",
      "Loss: 5.675947666168213\n",
      "Batch :  35\n",
      "Loss: 5.437273025512695\n",
      "Batch :  36\n",
      "Loss: 5.717762470245361\n",
      "Batch :  37\n",
      "Loss: 6.106723785400391\n",
      "Batch :  38\n",
      "Loss: 5.849415302276611\n",
      "Batch :  39\n",
      "Loss: 5.626439571380615\n",
      "Batch :  40\n",
      "Loss: 5.958846569061279\n",
      "Batch :  41\n",
      "Loss: 5.679774284362793\n",
      "Batch :  42\n",
      "Loss: 5.876291751861572\n",
      "Batch :  43\n",
      "Loss: 5.3770599365234375\n",
      "Batch :  44\n",
      "Loss: 5.090031147003174\n",
      "Batch :  45\n",
      "Loss: 5.6040120124816895\n",
      "Batch :  46\n",
      "Loss: 6.102693557739258\n",
      "Batch :  47\n",
      "Loss: 5.213066577911377\n",
      "Batch :  48\n",
      "Loss: 4.861171722412109\n",
      "Batch :  49\n",
      "Loss: 5.260540008544922\n",
      "Batch :  50\n",
      "Loss: 5.036510467529297\n",
      "Batch :  51\n",
      "Loss: 5.977449417114258\n",
      "Batch :  52\n",
      "Loss: 5.588881015777588\n",
      "Batch :  53\n",
      "Loss: 5.76363468170166\n",
      "Batch :  54\n",
      "Loss: 6.0492377281188965\n",
      "Batch :  55\n",
      "Loss: 5.680758953094482\n",
      "Batch :  56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.873495101928711\n",
      "Batch :  57\n",
      "Loss: 5.597692966461182\n",
      "Batch :  58\n",
      "Loss: 5.599781036376953\n",
      "Batch :  59\n",
      "Loss: 5.553738117218018\n",
      "Batch :  60\n",
      "Loss: 5.633862495422363\n",
      "Batch :  61\n",
      "Loss: 5.625927448272705\n",
      "Batch :  62\n",
      "Loss: 5.440574645996094\n",
      "Batch :  63\n",
      "Loss: 6.284501552581787\n",
      "Batch :  64\n",
      "Loss: 5.662989616394043\n",
      "Batch :  65\n",
      "Loss: 5.711974620819092\n",
      "Batch :  66\n",
      "Loss: 5.616353988647461\n",
      "Batch :  67\n",
      "Loss: 5.230132102966309\n",
      "Batch :  68\n",
      "Loss: 5.094156742095947\n",
      "Batch :  69\n",
      "Loss: 5.764741897583008\n",
      "Batch :  70\n",
      "Loss: 5.6909050941467285\n",
      "Batch :  71\n",
      "Loss: 4.718756198883057\n",
      "Batch :  72\n",
      "Loss: 5.493353366851807\n",
      "Batch :  73\n",
      "Loss: 5.365789890289307\n",
      "Batch :  74\n",
      "Loss: 5.458958625793457\n",
      "Batch :  75\n",
      "Loss: 5.940831184387207\n",
      "Batch :  76\n",
      "Loss: 5.541176795959473\n",
      "Batch :  77\n",
      "Loss: 5.817055702209473\n",
      "Batch :  78\n",
      "Loss: 5.306770324707031\n",
      "Batch :  79\n",
      "Loss: 5.824134349822998\n",
      "Batch :  80\n",
      "Loss: 5.983602046966553\n",
      "Batch :  81\n",
      "Loss: 5.190749645233154\n",
      "Batch :  82\n",
      "Loss: 5.402558326721191\n",
      "Batch :  83\n",
      "Loss: 5.52564001083374\n",
      "Batch :  84\n",
      "Loss: 5.210931301116943\n",
      "Batch :  85\n",
      "Loss: 4.966945648193359\n",
      "Batch :  86\n",
      "Loss: 5.487835884094238\n",
      "Batch :  87\n",
      "Loss: 5.90844202041626\n",
      "Batch :  88\n",
      "Loss: 6.026634693145752\n",
      "Batch :  89\n",
      "Loss: 5.741581916809082\n",
      "Batch :  90\n",
      "Loss: 5.627580165863037\n",
      "Batch :  91\n",
      "Loss: 5.464529037475586\n",
      "Batch :  92\n",
      "Loss: 5.446494102478027\n",
      "Batch :  93\n",
      "Loss: 5.259658336639404\n",
      "Batch :  94\n",
      "Loss: 5.542869567871094\n",
      "Batch :  95\n",
      "Loss: 5.507288455963135\n",
      "Batch :  96\n",
      "Loss: 5.652656555175781\n",
      "Batch :  97\n",
      "Loss: 5.684826374053955\n",
      "Batch :  98\n",
      "Loss: 5.588118553161621\n",
      "Batch :  99\n",
      "Loss: 5.4779791831970215\n",
      "Saving model\n",
      "Epoch 6\n",
      "Batch :  0\n",
      "Loss: 5.497361660003662\n",
      "Batch :  1\n",
      "Loss: 5.526996612548828\n",
      "Batch :  2\n",
      "Loss: 5.652742385864258\n",
      "Batch :  3\n",
      "Loss: 5.573423862457275\n",
      "Batch :  4\n",
      "Loss: 6.256585121154785\n",
      "Batch :  5\n",
      "Loss: 5.756318092346191\n",
      "Batch :  6\n",
      "Loss: 5.560953140258789\n",
      "Batch :  7\n",
      "Loss: 5.608044624328613\n",
      "Batch :  8\n",
      "Loss: 5.805448532104492\n",
      "Batch :  9\n",
      "Loss: 5.728001117706299\n",
      "Batch :  10\n",
      "Loss: 5.926690101623535\n",
      "Batch :  11\n",
      "Loss: 5.707120895385742\n",
      "Batch :  12\n",
      "Loss: 6.016249179840088\n",
      "Batch :  13\n",
      "Loss: 5.754140853881836\n",
      "Batch :  14\n",
      "Loss: 6.204416751861572\n",
      "Batch :  15\n",
      "Loss: 5.831908702850342\n",
      "Batch :  16\n",
      "Loss: 5.383680820465088\n",
      "Batch :  17\n",
      "Loss: 5.4065961837768555\n",
      "Batch :  18\n",
      "Loss: 5.284539222717285\n",
      "Batch :  19\n",
      "Loss: 5.60529088973999\n",
      "Batch :  20\n",
      "Loss: 5.953726768493652\n",
      "Batch :  21\n",
      "Loss: 5.550920486450195\n",
      "Batch :  22\n",
      "Loss: 5.419912815093994\n",
      "Batch :  23\n",
      "Loss: 5.390408039093018\n",
      "Batch :  24\n",
      "Loss: 5.891736030578613\n",
      "Batch :  25\n",
      "Loss: 5.691743850708008\n",
      "Batch :  26\n",
      "Loss: 5.193851947784424\n",
      "Batch :  27\n",
      "Loss: 5.661628246307373\n",
      "Batch :  28\n",
      "Loss: 6.277170658111572\n",
      "Batch :  29\n",
      "Loss: 5.809678077697754\n",
      "Batch :  30\n",
      "Loss: 5.329586029052734\n",
      "Batch :  31\n",
      "Loss: 5.5891876220703125\n",
      "Batch :  32\n",
      "Loss: 5.8678107261657715\n",
      "Batch :  33\n",
      "Loss: 5.498596668243408\n",
      "Batch :  34\n",
      "Loss: 5.242127895355225\n",
      "Batch :  35\n",
      "Loss: 4.873598098754883\n",
      "Batch :  36\n",
      "Loss: 5.195225238800049\n",
      "Batch :  37\n",
      "Loss: 5.634713172912598\n",
      "Batch :  38\n",
      "Loss: 5.297713756561279\n",
      "Batch :  39\n",
      "Loss: 5.167479038238525\n",
      "Batch :  40\n",
      "Loss: 5.521401882171631\n",
      "Batch :  41\n",
      "Loss: 5.252809524536133\n",
      "Batch :  42\n",
      "Loss: 5.283796787261963\n",
      "Batch :  43\n",
      "Loss: 4.799598217010498\n",
      "Batch :  44\n",
      "Loss: 4.507805824279785\n",
      "Batch :  45\n",
      "Loss: 5.243863105773926\n",
      "Batch :  46\n",
      "Loss: 5.801074981689453\n",
      "Batch :  47\n",
      "Loss: 4.943663597106934\n",
      "Batch :  48\n",
      "Loss: 4.451723098754883\n",
      "Batch :  49\n",
      "Loss: 5.048332691192627\n",
      "Batch :  50\n",
      "Loss: 4.648108959197998\n",
      "Batch :  51\n",
      "Loss: 5.604332447052002\n",
      "Batch :  52\n",
      "Loss: 5.13944149017334\n",
      "Batch :  53\n",
      "Loss: 5.589785099029541\n",
      "Batch :  54\n",
      "Loss: 5.7006425857543945\n",
      "Batch :  55\n",
      "Loss: 5.226287364959717\n",
      "Batch :  56\n",
      "Loss: 5.462687015533447\n",
      "Batch :  57\n",
      "Loss: 5.133247375488281\n",
      "Batch :  58\n",
      "Loss: 5.052403926849365\n",
      "Batch :  59\n",
      "Loss: 5.083339691162109\n",
      "Batch :  60\n",
      "Loss: 5.337510108947754\n",
      "Batch :  61\n",
      "Loss: 5.223177909851074\n",
      "Batch :  62\n",
      "Loss: 5.080814361572266\n",
      "Batch :  63\n",
      "Loss: 5.767918586730957\n",
      "Batch :  64\n",
      "Loss: 5.156620502471924\n",
      "Batch :  65\n",
      "Loss: 5.143334865570068\n",
      "Batch :  66\n",
      "Loss: 5.0168280601501465\n",
      "Batch :  67\n",
      "Loss: 4.655506610870361\n",
      "Batch :  68\n",
      "Loss: 4.785473346710205\n",
      "Batch :  69\n",
      "Loss: 5.280604839324951\n",
      "Batch :  70\n",
      "Loss: 5.030195236206055\n",
      "Batch :  71\n",
      "Loss: 4.102208137512207\n",
      "Batch :  72\n",
      "Loss: 4.999395847320557\n",
      "Batch :  73\n",
      "Loss: 4.852256774902344\n",
      "Batch :  74\n",
      "Loss: 5.164378643035889\n",
      "Batch :  75\n",
      "Loss: 5.325650691986084\n",
      "Batch :  76\n",
      "Loss: 4.828083038330078\n",
      "Batch :  77\n",
      "Loss: 5.31533670425415\n",
      "Batch :  78\n",
      "Loss: 4.736013889312744\n",
      "Batch :  79\n",
      "Loss: 5.376685619354248\n",
      "Batch :  80\n",
      "Loss: 5.348954677581787\n",
      "Batch :  81\n",
      "Loss: 4.70482873916626\n",
      "Batch :  82\n",
      "Loss: 5.076232433319092\n",
      "Batch :  83\n",
      "Loss: 5.215132236480713\n",
      "Batch :  84\n",
      "Loss: 4.849010467529297\n",
      "Batch :  85\n",
      "Loss: 4.498575687408447\n",
      "Batch :  86\n",
      "Loss: 5.1419477462768555\n",
      "Batch :  87\n",
      "Loss: 5.517487049102783\n",
      "Batch :  88\n",
      "Loss: 5.502575397491455\n",
      "Batch :  89\n",
      "Loss: 5.272578716278076\n",
      "Batch :  90\n",
      "Loss: 5.020669460296631\n",
      "Batch :  91\n",
      "Loss: 5.056947231292725\n",
      "Batch :  92\n",
      "Loss: 5.082205295562744\n",
      "Batch :  93\n",
      "Loss: 4.563945293426514\n",
      "Batch :  94\n",
      "Loss: 4.843417167663574\n",
      "Batch :  95\n",
      "Loss: 4.893477439880371\n",
      "Batch :  96\n",
      "Loss: 4.791046142578125\n",
      "Batch :  97\n",
      "Loss: 5.216063976287842\n",
      "Batch :  98\n",
      "Loss: 5.116299629211426\n",
      "Batch :  99\n",
      "Loss: 5.119259834289551\n",
      "Epoch 7\n",
      "Batch :  0\n",
      "Loss: 4.878652095794678\n",
      "Batch :  1\n",
      "Loss: 5.14153528213501\n",
      "Batch :  2\n",
      "Loss: 5.089557647705078\n",
      "Batch :  3\n",
      "Loss: 4.965182781219482\n",
      "Batch :  4\n",
      "Loss: 5.900123119354248\n",
      "Batch :  5\n",
      "Loss: 5.271505355834961\n",
      "Batch :  6\n",
      "Loss: 5.165140628814697\n",
      "Batch :  7\n",
      "Loss: 5.226236343383789\n",
      "Batch :  8\n",
      "Loss: 5.16915225982666\n",
      "Batch :  9\n",
      "Loss: 5.19931697845459\n",
      "Batch :  10\n",
      "Loss: 5.289941787719727\n",
      "Batch :  11\n",
      "Loss: 5.171708583831787\n",
      "Batch :  12\n",
      "Loss: 5.47636604309082\n",
      "Batch :  13\n",
      "Loss: 5.145980358123779\n",
      "Batch :  14\n",
      "Loss: 5.552524089813232\n",
      "Batch :  15\n",
      "Loss: 5.329544544219971\n",
      "Batch :  16\n",
      "Loss: 4.865383148193359\n",
      "Batch :  17\n",
      "Loss: 4.79865026473999\n",
      "Batch :  18\n",
      "Loss: 4.521393775939941\n",
      "Batch :  19\n",
      "Loss: 4.944552898406982\n",
      "Batch :  20\n",
      "Loss: 5.419844627380371\n",
      "Batch :  21\n",
      "Loss: 4.930662631988525\n",
      "Batch :  22\n",
      "Loss: 4.932783603668213\n",
      "Batch :  23\n",
      "Loss: 4.9951252937316895\n",
      "Batch :  24\n",
      "Loss: 5.619796276092529\n",
      "Batch :  25\n",
      "Loss: 5.297506332397461\n",
      "Batch :  26\n",
      "Loss: 4.6621856689453125\n",
      "Batch :  27\n",
      "Loss: 5.0676774978637695\n",
      "Batch :  28\n",
      "Loss: 5.795384883880615\n",
      "Batch :  29\n",
      "Loss: 5.371433258056641\n",
      "Batch :  30\n",
      "Loss: 5.055395603179932\n",
      "Batch :  31\n",
      "Loss: 5.5287652015686035\n",
      "Batch :  32\n",
      "Loss: 5.40448522567749\n",
      "Batch :  33\n",
      "Loss: 5.089218616485596\n",
      "Batch :  34\n",
      "Loss: 4.855163097381592\n",
      "Batch :  35\n",
      "Loss: 4.642482757568359\n",
      "Batch :  36\n",
      "Loss: 4.796557426452637\n",
      "Batch :  37\n",
      "Loss: 5.143101692199707\n",
      "Batch :  38\n",
      "Loss: 4.931490898132324\n",
      "Batch :  39\n",
      "Loss: 4.559047222137451\n",
      "Batch :  40\n",
      "Loss: 4.942896366119385\n",
      "Batch :  41\n",
      "Loss: 4.812996864318848\n",
      "Batch :  42\n",
      "Loss: 4.633910655975342\n",
      "Batch :  43\n",
      "Loss: 4.124268054962158\n",
      "Batch :  44\n",
      "Loss: 4.129335403442383\n",
      "Batch :  45\n",
      "Loss: 4.611417770385742\n",
      "Batch :  46\n",
      "Loss: 5.110378265380859\n",
      "Batch :  47\n",
      "Loss: 4.221704959869385\n",
      "Batch :  48\n",
      "Loss: 4.022439002990723\n",
      "Batch :  49\n",
      "Loss: 4.508713245391846\n",
      "Batch :  50\n",
      "Loss: 4.299981117248535\n",
      "Batch :  51\n",
      "Loss: 5.138815402984619\n",
      "Batch :  52\n",
      "Loss: 4.493215084075928\n",
      "Batch :  53\n",
      "Loss: 4.8055925369262695\n",
      "Batch :  54\n",
      "Loss: 5.1980061531066895\n",
      "Batch :  55\n",
      "Loss: 4.634194850921631\n",
      "Batch :  56\n",
      "Loss: 5.028231143951416\n",
      "Batch :  57\n",
      "Loss: 4.7279181480407715\n",
      "Batch :  58\n",
      "Loss: 4.511901378631592\n",
      "Batch :  59\n",
      "Loss: 4.260810375213623\n",
      "Batch :  60\n",
      "Loss: 4.706098556518555\n",
      "Batch :  61\n",
      "Loss: 4.583443641662598\n",
      "Batch :  62\n",
      "Loss: 4.498877048492432\n",
      "Batch :  63\n",
      "Loss: 5.157619476318359\n",
      "Batch :  64\n",
      "Loss: 4.7408127784729\n",
      "Batch :  65\n",
      "Loss: 4.857112407684326\n",
      "Batch :  66\n",
      "Loss: 4.6691765785217285\n",
      "Batch :  67\n",
      "Loss: 4.094289302825928\n",
      "Batch :  68\n",
      "Loss: 4.06758975982666\n",
      "Batch :  69\n",
      "Loss: 4.983116149902344\n",
      "Batch :  70\n",
      "Loss: 4.530613422393799\n",
      "Batch :  71\n",
      "Loss: 3.6747138500213623\n",
      "Batch :  72\n",
      "Loss: 4.698564052581787\n",
      "Batch :  73\n",
      "Loss: 4.544225215911865\n",
      "Batch :  74\n",
      "Loss: 4.415489673614502\n",
      "Batch :  75\n",
      "Loss: 4.9165472984313965\n",
      "Batch :  76\n",
      "Loss: 4.390181064605713\n",
      "Batch :  77\n",
      "Loss: 4.9066996574401855\n",
      "Batch :  78\n",
      "Loss: 4.315472602844238\n",
      "Batch :  79\n",
      "Loss: 4.88615608215332\n",
      "Batch :  80\n",
      "Loss: 4.979025363922119\n",
      "Batch :  81\n",
      "Loss: 4.445515155792236\n",
      "Batch :  82\n",
      "Loss: 4.93298864364624\n",
      "Batch :  83\n",
      "Loss: 4.614602088928223\n",
      "Batch :  84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.225068092346191\n",
      "Batch :  85\n",
      "Loss: 3.885964870452881\n",
      "Batch :  86\n",
      "Loss: 4.398972034454346\n",
      "Batch :  87\n",
      "Loss: 4.881364822387695\n",
      "Batch :  88\n",
      "Loss: 4.773468494415283\n",
      "Batch :  89\n",
      "Loss: 4.432913780212402\n",
      "Batch :  90\n",
      "Loss: 4.490651607513428\n",
      "Batch :  91\n",
      "Loss: 4.623348236083984\n",
      "Batch :  92\n",
      "Loss: 8.954522132873535\n",
      "Batch :  93\n",
      "Loss: 4.313284397125244\n",
      "Batch :  94\n",
      "Loss: 4.738758087158203\n",
      "Batch :  95\n",
      "Loss: 5.049170017242432\n",
      "Batch :  96\n",
      "Loss: 5.5059967041015625\n",
      "Batch :  97\n",
      "Loss: 5.430203914642334\n",
      "Batch :  98\n",
      "Loss: 5.878780364990234\n",
      "Batch :  99\n",
      "Loss: 5.942455291748047\n",
      "Epoch 8\n",
      "Batch :  0\n",
      "Loss: 5.498902320861816\n",
      "Batch :  1\n",
      "Loss: 5.6360650062561035\n",
      "Batch :  2\n",
      "Loss: 5.261592388153076\n",
      "Batch :  3\n",
      "Loss: 5.426651954650879\n",
      "Batch :  4\n",
      "Loss: 6.171736717224121\n",
      "Batch :  5\n",
      "Loss: 5.332431793212891\n",
      "Batch :  6\n",
      "Loss: 5.427584648132324\n",
      "Batch :  7\n",
      "Loss: 5.349945545196533\n",
      "Batch :  8\n",
      "Loss: 5.402608394622803\n",
      "Batch :  9\n",
      "Loss: 5.4499406814575195\n",
      "Batch :  10\n",
      "Loss: 5.6967902183532715\n",
      "Batch :  11\n",
      "Loss: 5.305965900421143\n",
      "Batch :  12\n",
      "Loss: 5.435425758361816\n",
      "Batch :  13\n",
      "Loss: 4.905322551727295\n",
      "Batch :  14\n",
      "Loss: 5.46302604675293\n",
      "Batch :  15\n",
      "Loss: 5.594897747039795\n",
      "Batch :  16\n",
      "Loss: 4.910062789916992\n",
      "Batch :  17\n",
      "Loss: 4.938797950744629\n",
      "Batch :  18\n",
      "Loss: 4.57658576965332\n",
      "Batch :  19\n",
      "Loss: 4.965737819671631\n",
      "Batch :  20\n",
      "Loss: 5.3691301345825195\n",
      "Batch :  21\n",
      "Loss: 4.921250820159912\n",
      "Batch :  22\n",
      "Loss: 4.972176551818848\n",
      "Batch :  23\n",
      "Loss: 4.96034049987793\n",
      "Batch :  24\n",
      "Loss: 5.50662088394165\n",
      "Batch :  25\n",
      "Loss: 5.15665864944458\n",
      "Batch :  26\n",
      "Loss: 4.440144062042236\n",
      "Batch :  27\n",
      "Loss: 5.067458629608154\n",
      "Batch :  28\n",
      "Loss: 5.849303245544434\n",
      "Batch :  29\n",
      "Loss: 5.619234085083008\n",
      "Batch :  30\n",
      "Loss: 4.853612899780273\n",
      "Batch :  31\n",
      "Loss: 5.195826053619385\n",
      "Batch :  32\n",
      "Loss: 5.628483295440674\n",
      "Batch :  33\n",
      "Loss: 5.267268657684326\n",
      "Batch :  34\n",
      "Loss: 4.667577266693115\n",
      "Batch :  35\n",
      "Loss: 4.560391902923584\n",
      "Batch :  36\n",
      "Loss: 4.791468620300293\n",
      "Batch :  37\n",
      "Loss: 5.090245723724365\n",
      "Batch :  38\n",
      "Loss: 4.883457660675049\n",
      "Batch :  39\n",
      "Loss: 4.773303031921387\n",
      "Batch :  40\n",
      "Loss: 5.123723030090332\n",
      "Batch :  41\n",
      "Loss: 4.886626243591309\n",
      "Batch :  42\n",
      "Loss: 4.9797797203063965\n",
      "Batch :  43\n",
      "Loss: 4.3655619621276855\n",
      "Batch :  44\n",
      "Loss: 4.31362247467041\n",
      "Batch :  45\n",
      "Loss: 4.7630534172058105\n",
      "Batch :  46\n",
      "Loss: 5.48363733291626\n",
      "Batch :  47\n",
      "Loss: 4.058370113372803\n",
      "Batch :  48\n",
      "Loss: 4.249226093292236\n",
      "Batch :  49\n",
      "Loss: 4.307577133178711\n",
      "Batch :  50\n",
      "Loss: 4.190551280975342\n",
      "Batch :  51\n",
      "Loss: 5.343761444091797\n",
      "Batch :  52\n",
      "Loss: 4.643704891204834\n",
      "Batch :  53\n",
      "Loss: 4.843710422515869\n",
      "Batch :  54\n",
      "Loss: 5.201038837432861\n",
      "Batch :  55\n",
      "Loss: 4.614823341369629\n",
      "Batch :  56\n",
      "Loss: 5.0490617752075195\n",
      "Batch :  57\n",
      "Loss: 4.679656982421875\n",
      "Batch :  58\n",
      "Loss: 4.562435626983643\n",
      "Batch :  59\n",
      "Loss: 4.207583427429199\n",
      "Batch :  60\n",
      "Loss: 4.571357250213623\n",
      "Batch :  61\n",
      "Loss: 4.539325714111328\n",
      "Batch :  62\n",
      "Loss: 4.605393409729004\n",
      "Batch :  63\n",
      "Loss: 4.870487689971924\n",
      "Batch :  64\n",
      "Loss: 4.72471809387207\n",
      "Batch :  65\n",
      "Loss: 4.785116195678711\n",
      "Batch :  66\n",
      "Loss: 4.583096981048584\n",
      "Batch :  67\n",
      "Loss: 4.071369171142578\n",
      "Batch :  68\n",
      "Loss: 4.111112594604492\n",
      "Batch :  69\n",
      "Loss: 4.555534839630127\n",
      "Batch :  70\n",
      "Loss: 4.454525947570801\n",
      "Batch :  71\n",
      "Loss: 3.5441055297851562\n",
      "Batch :  72\n",
      "Loss: 4.570964336395264\n",
      "Batch :  73\n",
      "Loss: 4.413527965545654\n",
      "Batch :  74\n",
      "Loss: 4.462416172027588\n",
      "Batch :  75\n",
      "Loss: 4.813228607177734\n",
      "Batch :  76\n",
      "Loss: 4.083346843719482\n",
      "Batch :  77\n",
      "Loss: 4.8744049072265625\n",
      "Batch :  78\n",
      "Loss: 4.181779384613037\n",
      "Batch :  79\n",
      "Loss: 4.8860907554626465\n",
      "Batch :  80\n",
      "Loss: 4.6559367179870605\n",
      "Batch :  81\n",
      "Loss: 4.289191722869873\n",
      "Batch :  82\n",
      "Loss: 4.536304950714111\n",
      "Batch :  83\n",
      "Loss: 4.5508599281311035\n",
      "Batch :  84\n",
      "Loss: 4.198072910308838\n",
      "Batch :  85\n",
      "Loss: 3.870676040649414\n",
      "Batch :  86\n",
      "Loss: 4.200067043304443\n",
      "Batch :  87\n",
      "Loss: 4.837810516357422\n",
      "Batch :  88\n",
      "Loss: 4.5988450050354\n",
      "Batch :  89\n",
      "Loss: 4.101578712463379\n",
      "Batch :  90\n",
      "Loss: 4.2319416999816895\n",
      "Batch :  91\n",
      "Loss: 4.146660327911377\n",
      "Batch :  92\n",
      "Loss: 4.546537399291992\n",
      "Batch :  93\n",
      "Loss: 3.6444201469421387\n",
      "Batch :  94\n",
      "Loss: 4.207777500152588\n",
      "Batch :  95\n",
      "Loss: 4.30557107925415\n",
      "Batch :  96\n",
      "Loss: 4.476926803588867\n",
      "Batch :  97\n",
      "Loss: 4.31926965713501\n",
      "Batch :  98\n",
      "Loss: 4.446789741516113\n",
      "Batch :  99\n",
      "Loss: 4.5363030433654785\n",
      "Epoch 9\n",
      "Batch :  0\n",
      "Loss: 4.188954830169678\n",
      "Batch :  1\n",
      "Loss: 4.438643932342529\n",
      "Batch :  2\n",
      "Loss: 4.4457926750183105\n",
      "Batch :  3\n",
      "Loss: 4.450394153594971\n",
      "Batch :  4\n",
      "Loss: 5.257446765899658\n",
      "Batch :  5\n",
      "Loss: 4.451090335845947\n",
      "Batch :  6\n",
      "Loss: 4.342460632324219\n",
      "Batch :  7\n",
      "Loss: 4.466419219970703\n",
      "Batch :  8\n",
      "Loss: 4.591067314147949\n",
      "Batch :  9\n",
      "Loss: 4.532876014709473\n",
      "Batch :  10\n",
      "Loss: 4.888940811157227\n",
      "Batch :  11\n",
      "Loss: 4.704813480377197\n",
      "Batch :  12\n",
      "Loss: 4.642748832702637\n",
      "Batch :  13\n",
      "Loss: 4.250611305236816\n",
      "Batch :  14\n",
      "Loss: 4.585635662078857\n",
      "Batch :  15\n",
      "Loss: 4.431739330291748\n",
      "Batch :  16\n",
      "Loss: 3.9826500415802\n",
      "Batch :  17\n",
      "Loss: 4.385810852050781\n",
      "Batch :  18\n",
      "Loss: 3.853830099105835\n",
      "Batch :  19\n",
      "Loss: 4.439817428588867\n",
      "Batch :  20\n",
      "Loss: 4.856282711029053\n",
      "Batch :  21\n",
      "Loss: 4.37277364730835\n",
      "Batch :  22\n",
      "Loss: 4.483170032501221\n",
      "Batch :  23\n",
      "Loss: 4.108644008636475\n",
      "Batch :  24\n",
      "Loss: 4.694799423217773\n",
      "Batch :  25\n",
      "Loss: 4.6161789894104\n",
      "Batch :  26\n",
      "Loss: 3.6710805892944336\n",
      "Batch :  27\n",
      "Loss: 4.554605484008789\n",
      "Batch :  28\n",
      "Loss: 5.2153191566467285\n",
      "Batch :  29\n",
      "Loss: 4.73123025894165\n",
      "Batch :  30\n",
      "Loss: 4.4686503410339355\n",
      "Batch :  31\n",
      "Loss: 4.687067031860352\n",
      "Batch :  32\n",
      "Loss: 4.839532375335693\n",
      "Batch :  33\n",
      "Loss: 4.4556884765625\n",
      "Batch :  34\n",
      "Loss: 4.0595927238464355\n",
      "Batch :  35\n",
      "Loss: 3.8044567108154297\n",
      "Batch :  36\n",
      "Loss: 4.072622299194336\n",
      "Batch :  37\n",
      "Loss: 4.560007572174072\n",
      "Batch :  38\n",
      "Loss: 4.4973320960998535\n",
      "Batch :  39\n",
      "Loss: 4.319624423980713\n",
      "Batch :  40\n",
      "Loss: 4.831068992614746\n",
      "Batch :  41\n",
      "Loss: 4.1714653968811035\n",
      "Batch :  42\n",
      "Loss: 4.360795497894287\n",
      "Batch :  43\n",
      "Loss: 3.628903388977051\n",
      "Batch :  44\n",
      "Loss: 3.7848353385925293\n",
      "Batch :  45\n",
      "Loss: 3.8944478034973145\n",
      "Batch :  46\n",
      "Loss: 4.844456195831299\n",
      "Batch :  47\n",
      "Loss: 3.2941946983337402\n",
      "Batch :  48\n",
      "Loss: 3.1806414127349854\n",
      "Batch :  49\n",
      "Loss: 4.422882556915283\n",
      "Batch :  50\n",
      "Loss: 3.7792468070983887\n",
      "Batch :  51\n",
      "Loss: 5.0686211585998535\n",
      "Batch :  52\n",
      "Loss: 4.200072288513184\n",
      "Batch :  53\n",
      "Loss: 4.489614963531494\n",
      "Batch :  54\n",
      "Loss: 4.758549213409424\n",
      "Batch :  55\n",
      "Loss: 4.184278964996338\n",
      "Batch :  56\n",
      "Loss: 4.405264377593994\n",
      "Batch :  57\n",
      "Loss: 4.21423864364624\n",
      "Batch :  58\n",
      "Loss: 3.9228456020355225\n",
      "Batch :  59\n",
      "Loss: 3.4933409690856934\n",
      "Batch :  60\n",
      "Loss: 3.9737823009490967\n",
      "Batch :  61\n",
      "Loss: 3.8872311115264893\n",
      "Batch :  62\n",
      "Loss: 3.7218637466430664\n",
      "Batch :  63\n",
      "Loss: 4.475630760192871\n",
      "Batch :  64\n",
      "Loss: 4.012969017028809\n",
      "Batch :  65\n",
      "Loss: 4.150134563446045\n",
      "Batch :  66\n",
      "Loss: 3.7514164447784424\n",
      "Batch :  67\n",
      "Loss: 3.2724289894104004\n",
      "Batch :  68\n",
      "Loss: 3.333404541015625\n",
      "Batch :  69\n",
      "Loss: 3.882568836212158\n",
      "Batch :  70\n",
      "Loss: 3.408186197280884\n",
      "Batch :  71\n",
      "Loss: 2.9966204166412354\n",
      "Batch :  72\n",
      "Loss: 3.7441930770874023\n",
      "Batch :  73\n",
      "Loss: 3.7571542263031006\n",
      "Batch :  74\n",
      "Loss: 3.7401866912841797\n",
      "Batch :  75\n",
      "Loss: 3.947922468185425\n",
      "Batch :  76\n",
      "Loss: 3.4056057929992676\n",
      "Batch :  77\n",
      "Loss: 4.09481143951416\n",
      "Batch :  78\n",
      "Loss: 3.534681797027588\n",
      "Batch :  79\n",
      "Loss: 4.079013824462891\n",
      "Batch :  80\n",
      "Loss: 3.810971975326538\n",
      "Batch :  81\n",
      "Loss: 3.559802770614624\n",
      "Batch :  82\n",
      "Loss: 3.9839999675750732\n",
      "Batch :  83\n",
      "Loss: 3.6521782875061035\n",
      "Batch :  84\n",
      "Loss: 4.027878761291504\n",
      "Batch :  85\n",
      "Loss: 3.163377285003662\n",
      "Batch :  86\n",
      "Loss: 3.484133243560791\n",
      "Batch :  87\n",
      "Loss: 3.9422085285186768\n",
      "Batch :  88\n",
      "Loss: 3.887627124786377\n",
      "Batch :  89\n",
      "Loss: 3.639554977416992\n",
      "Batch :  90\n",
      "Loss: 3.9718058109283447\n",
      "Batch :  91\n",
      "Loss: 3.792771339416504\n",
      "Batch :  92\n",
      "Loss: 3.943516492843628\n",
      "Batch :  93\n",
      "Loss: 3.158698558807373\n",
      "Batch :  94\n",
      "Loss: 3.316364288330078\n",
      "Batch :  95\n",
      "Loss: 3.379055976867676\n",
      "Batch :  96\n",
      "Loss: 3.526611804962158\n",
      "Batch :  97\n",
      "Loss: 3.795699119567871\n",
      "Batch :  98\n",
      "Loss: 3.986569404602051\n",
      "Batch :  99\n",
      "Loss: 3.947690486907959\n",
      "Epoch 10\n",
      "Batch :  0\n",
      "Loss: 3.633396863937378\n",
      "Batch :  1\n",
      "Loss: 4.005917072296143\n",
      "Batch :  2\n",
      "Loss: 3.853781223297119\n",
      "Batch :  3\n",
      "Loss: 3.777949571609497\n",
      "Batch :  4\n",
      "Loss: 4.7086501121521\n",
      "Batch :  5\n",
      "Loss: 3.6461620330810547\n",
      "Batch :  6\n",
      "Loss: 3.815627336502075\n",
      "Batch :  7\n",
      "Loss: 3.8801586627960205\n",
      "Batch :  8\n",
      "Loss: 3.9191157817840576\n",
      "Batch :  9\n",
      "Loss: 3.770294189453125\n",
      "Batch :  10\n",
      "Loss: 4.077450752258301\n",
      "Batch :  11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.909174680709839\n",
      "Batch :  12\n",
      "Loss: 4.063060283660889\n",
      "Batch :  13\n",
      "Loss: 3.8217849731445312\n",
      "Batch :  14\n",
      "Loss: 4.181344509124756\n",
      "Batch :  15\n",
      "Loss: 4.047612190246582\n",
      "Batch :  16\n",
      "Loss: 3.543832540512085\n",
      "Batch :  17\n",
      "Loss: 3.7757606506347656\n",
      "Batch :  18\n",
      "Loss: 3.181278705596924\n",
      "Batch :  19\n",
      "Loss: 3.601701498031616\n",
      "Batch :  20\n",
      "Loss: 3.8277432918548584\n",
      "Batch :  21\n",
      "Loss: 3.334277629852295\n",
      "Batch :  22\n",
      "Loss: 3.519456386566162\n",
      "Batch :  23\n",
      "Loss: 3.3259007930755615\n",
      "Batch :  24\n",
      "Loss: 3.99045729637146\n",
      "Batch :  25\n",
      "Loss: 3.683467388153076\n",
      "Batch :  26\n",
      "Loss: 3.0562965869903564\n",
      "Batch :  27\n",
      "Loss: 3.90948748588562\n",
      "Batch :  28\n",
      "Loss: 4.3975958824157715\n",
      "Batch :  29\n",
      "Loss: 3.7338435649871826\n",
      "Batch :  30\n",
      "Loss: 3.547039747238159\n",
      "Batch :  31\n",
      "Loss: 3.5046987533569336\n",
      "Batch :  32\n",
      "Loss: 4.036134243011475\n",
      "Batch :  33\n",
      "Loss: 3.650454521179199\n",
      "Batch :  34\n",
      "Loss: 3.4224140644073486\n",
      "Batch :  35\n",
      "Loss: 3.2632601261138916\n",
      "Batch :  36\n",
      "Loss: 3.5608792304992676\n",
      "Batch :  37\n",
      "Loss: 3.802885055541992\n",
      "Batch :  38\n",
      "Loss: 3.6224467754364014\n",
      "Batch :  39\n",
      "Loss: 3.4532418251037598\n",
      "Batch :  40\n",
      "Loss: 3.820798635482788\n",
      "Batch :  41\n",
      "Loss: 3.5032153129577637\n",
      "Batch :  42\n",
      "Loss: 3.6457507610321045\n",
      "Batch :  43\n",
      "Loss: 2.8803670406341553\n",
      "Batch :  44\n",
      "Loss: 3.0113439559936523\n",
      "Batch :  45\n",
      "Loss: 3.3514885902404785\n",
      "Batch :  46\n",
      "Loss: 4.178995609283447\n",
      "Batch :  47\n",
      "Loss: 2.9294586181640625\n",
      "Batch :  48\n",
      "Loss: 2.8624253273010254\n",
      "Batch :  49\n",
      "Loss: 3.2816333770751953\n",
      "Batch :  50\n",
      "Loss: 2.843322515487671\n",
      "Batch :  51\n",
      "Loss: 4.11175012588501\n",
      "Batch :  52\n",
      "Loss: 3.035047769546509\n",
      "Batch :  53\n",
      "Loss: 3.551886796951294\n",
      "Batch :  54\n",
      "Loss: 3.841533660888672\n",
      "Batch :  55\n",
      "Loss: 3.157118082046509\n",
      "Batch :  56\n",
      "Loss: 3.6840872764587402\n",
      "Batch :  57\n",
      "Loss: 3.304335117340088\n",
      "Batch :  58\n",
      "Loss: 3.262477159500122\n",
      "Batch :  59\n",
      "Loss: 2.9435222148895264\n",
      "Batch :  60\n",
      "Loss: 3.257619857788086\n",
      "Batch :  61\n",
      "Loss: 3.1570653915405273\n",
      "Batch :  62\n",
      "Loss: 3.1050446033477783\n",
      "Batch :  63\n",
      "Loss: 3.2702741622924805\n",
      "Batch :  64\n",
      "Loss: 3.083845615386963\n",
      "Batch :  65\n",
      "Loss: 3.577489137649536\n",
      "Batch :  66\n",
      "Loss: 2.963444709777832\n",
      "Batch :  67\n",
      "Loss: 2.6320321559906006\n",
      "Batch :  68\n",
      "Loss: 2.4814183712005615\n",
      "Batch :  69\n",
      "Loss: 3.078800678253174\n",
      "Batch :  70\n",
      "Loss: 2.6379146575927734\n",
      "Batch :  71\n",
      "Loss: 2.523602247238159\n",
      "Batch :  72\n",
      "Loss: 3.2364468574523926\n",
      "Batch :  73\n",
      "Loss: 3.39530611038208\n",
      "Batch :  74\n",
      "Loss: 2.8654448986053467\n",
      "Batch :  75\n",
      "Loss: 3.3870131969451904\n",
      "Batch :  76\n",
      "Loss: 2.845065116882324\n",
      "Batch :  77\n",
      "Loss: 3.3827672004699707\n",
      "Batch :  78\n",
      "Loss: 2.885603189468384\n",
      "Batch :  79\n",
      "Loss: 3.3409969806671143\n",
      "Batch :  80\n",
      "Loss: 3.2658209800720215\n",
      "Batch :  81\n",
      "Loss: 2.747974157333374\n",
      "Batch :  82\n",
      "Loss: 3.3788630962371826\n",
      "Batch :  83\n",
      "Loss: 2.935453176498413\n",
      "Batch :  84\n",
      "Loss: 2.9543681144714355\n",
      "Batch :  85\n",
      "Loss: 2.6043787002563477\n",
      "Batch :  86\n",
      "Loss: 2.761730670928955\n",
      "Batch :  87\n",
      "Loss: 3.355259895324707\n",
      "Batch :  88\n",
      "Loss: 3.2553586959838867\n",
      "Batch :  89\n",
      "Loss: 2.642883062362671\n",
      "Batch :  90\n",
      "Loss: 2.9292337894439697\n",
      "Batch :  91\n",
      "Loss: 2.9043593406677246\n",
      "Batch :  92\n",
      "Loss: 3.215087413787842\n",
      "Batch :  93\n",
      "Loss: 2.334773302078247\n",
      "Batch :  94\n",
      "Loss: 2.5467207431793213\n",
      "Batch :  95\n",
      "Loss: 2.7877981662750244\n",
      "Batch :  96\n",
      "Loss: 2.7599754333496094\n",
      "Batch :  97\n",
      "Loss: 3.039982557296753\n",
      "Batch :  98\n",
      "Loss: 3.0568389892578125\n",
      "Batch :  99\n",
      "Loss: 3.16494083404541\n",
      "Saving model\n",
      "Epoch 11\n",
      "Batch :  0\n",
      "Loss: 2.8041059970855713\n",
      "Batch :  1\n",
      "Loss: 3.3442862033843994\n",
      "Batch :  2\n",
      "Loss: 3.248445510864258\n",
      "Batch :  3\n",
      "Loss: 3.2075207233428955\n",
      "Batch :  4\n",
      "Loss: 3.9255006313323975\n",
      "Batch :  5\n",
      "Loss: 2.9772188663482666\n",
      "Batch :  6\n",
      "Loss: 2.9710800647735596\n",
      "Batch :  7\n",
      "Loss: 3.212693214416504\n",
      "Batch :  8\n",
      "Loss: 3.058540105819702\n",
      "Batch :  9\n",
      "Loss: 2.930492877960205\n",
      "Batch :  10\n",
      "Loss: 3.4733643531799316\n",
      "Batch :  11\n",
      "Loss: 3.205397844314575\n",
      "Batch :  12\n",
      "Loss: 3.322774887084961\n",
      "Batch :  13\n",
      "Loss: 2.742562770843506\n",
      "Batch :  14\n",
      "Loss: 3.205437421798706\n",
      "Batch :  15\n",
      "Loss: 3.329024314880371\n",
      "Batch :  16\n",
      "Loss: 2.6432220935821533\n",
      "Batch :  17\n",
      "Loss: 3.3144783973693848\n",
      "Batch :  18\n",
      "Loss: 2.4226808547973633\n",
      "Batch :  19\n",
      "Loss: 2.9542596340179443\n",
      "Batch :  20\n",
      "Loss: 3.0669748783111572\n",
      "Batch :  21\n",
      "Loss: 2.7385730743408203\n",
      "Batch :  22\n",
      "Loss: 3.085134267807007\n",
      "Batch :  23\n",
      "Loss: 2.7489371299743652\n",
      "Batch :  24\n",
      "Loss: 3.358456611633301\n",
      "Batch :  25\n",
      "Loss: 3.0004842281341553\n",
      "Batch :  26\n",
      "Loss: 2.4111528396606445\n",
      "Batch :  27\n",
      "Loss: 3.1842668056488037\n",
      "Batch :  28\n",
      "Loss: 3.4861512184143066\n",
      "Batch :  29\n",
      "Loss: 2.945624828338623\n",
      "Batch :  30\n",
      "Loss: 2.910888910293579\n",
      "Batch :  31\n",
      "Loss: 3.4173691272735596\n",
      "Batch :  32\n",
      "Loss: 3.200190544128418\n",
      "Batch :  33\n",
      "Loss: 2.826406717300415\n",
      "Batch :  34\n",
      "Loss: 2.6230220794677734\n",
      "Batch :  35\n",
      "Loss: 2.5521984100341797\n",
      "Batch :  36\n",
      "Loss: 2.580333948135376\n",
      "Batch :  37\n",
      "Loss: 2.839109420776367\n",
      "Batch :  38\n",
      "Loss: 2.9222376346588135\n",
      "Batch :  39\n",
      "Loss: 2.7842869758605957\n",
      "Batch :  40\n",
      "Loss: 3.12557315826416\n",
      "Batch :  41\n",
      "Loss: 2.852640151977539\n",
      "Batch :  42\n",
      "Loss: 2.78139591217041\n",
      "Batch :  43\n",
      "Loss: 2.0240938663482666\n",
      "Batch :  44\n",
      "Loss: 2.54350209236145\n",
      "Batch :  45\n",
      "Loss: 2.4849436283111572\n",
      "Batch :  46\n",
      "Loss: 3.0284552574157715\n",
      "Batch :  47\n",
      "Loss: 2.238062620162964\n",
      "Batch :  48\n",
      "Loss: 2.265687942504883\n",
      "Batch :  49\n",
      "Loss: 2.6472814083099365\n",
      "Batch :  50\n",
      "Loss: 2.364625930786133\n",
      "Batch :  51\n",
      "Loss: 3.257885217666626\n",
      "Batch :  52\n",
      "Loss: 2.6961452960968018\n",
      "Batch :  53\n",
      "Loss: 2.8396129608154297\n",
      "Batch :  54\n",
      "Loss: 3.429644823074341\n",
      "Batch :  55\n",
      "Loss: 2.5225706100463867\n",
      "Batch :  56\n",
      "Loss: 3.0906102657318115\n",
      "Batch :  57\n",
      "Loss: 2.767606258392334\n",
      "Batch :  58\n",
      "Loss: 2.3789918422698975\n",
      "Batch :  59\n",
      "Loss: 2.5254158973693848\n",
      "Batch :  60\n",
      "Loss: 2.831507682800293\n",
      "Batch :  61\n",
      "Loss: 2.493818759918213\n",
      "Batch :  62\n",
      "Loss: 2.503570079803467\n",
      "Batch :  63\n",
      "Loss: 2.6267709732055664\n",
      "Batch :  64\n",
      "Loss: 2.468255043029785\n",
      "Batch :  65\n",
      "Loss: 3.0907680988311768\n",
      "Batch :  66\n",
      "Loss: 2.449252128601074\n",
      "Batch :  67\n",
      "Loss: 1.932052493095398\n",
      "Batch :  68\n",
      "Loss: 1.9475570917129517\n",
      "Batch :  69\n",
      "Loss: 2.570674180984497\n",
      "Batch :  70\n",
      "Loss: 2.070561408996582\n",
      "Batch :  71\n",
      "Loss: 1.9340626001358032\n",
      "Batch :  72\n",
      "Loss: 2.3553104400634766\n",
      "Batch :  73\n",
      "Loss: 2.3695480823516846\n",
      "Batch :  74\n",
      "Loss: 2.166743755340576\n",
      "Batch :  75\n",
      "Loss: 2.617403507232666\n",
      "Batch :  76\n",
      "Loss: 2.1235969066619873\n",
      "Batch :  77\n",
      "Loss: 2.675374984741211\n",
      "Batch :  78\n",
      "Loss: 2.2137231826782227\n",
      "Batch :  79\n",
      "Loss: 2.8370425701141357\n",
      "Batch :  80\n",
      "Loss: 2.541140556335449\n",
      "Batch :  81\n",
      "Loss: 2.481614112854004\n",
      "Batch :  82\n",
      "Loss: 2.7374281883239746\n",
      "Batch :  83\n",
      "Loss: 2.4202277660369873\n",
      "Batch :  84\n",
      "Loss: 2.372509717941284\n",
      "Batch :  85\n",
      "Loss: 2.153329372406006\n",
      "Batch :  86\n",
      "Loss: 2.214850425720215\n",
      "Batch :  87\n",
      "Loss: 2.7444615364074707\n",
      "Batch :  88\n",
      "Loss: 2.5836076736450195\n",
      "Batch :  89\n",
      "Loss: 2.1871016025543213\n",
      "Batch :  90\n",
      "Loss: 2.439554214477539\n",
      "Batch :  91\n",
      "Loss: 2.3668484687805176\n",
      "Batch :  92\n",
      "Loss: 2.520321846008301\n",
      "Batch :  93\n",
      "Loss: 1.6919918060302734\n",
      "Batch :  94\n",
      "Loss: 1.9885194301605225\n",
      "Batch :  95\n",
      "Loss: 2.091078758239746\n",
      "Batch :  96\n",
      "Loss: 2.1471214294433594\n",
      "Batch :  97\n",
      "Loss: 2.6061224937438965\n",
      "Batch :  98\n",
      "Loss: 2.3517444133758545\n",
      "Batch :  99\n",
      "Loss: 2.4641454219818115\n",
      "Epoch 12\n",
      "Batch :  0\n",
      "Loss: 2.1011128425598145\n",
      "Batch :  1\n",
      "Loss: 2.736488103866577\n",
      "Batch :  2\n",
      "Loss: 2.4629387855529785\n",
      "Batch :  3\n",
      "Loss: 2.5588269233703613\n",
      "Batch :  4\n",
      "Loss: 3.4038169384002686\n",
      "Batch :  5\n",
      "Loss: 2.3417646884918213\n",
      "Batch :  6\n",
      "Loss: 2.250805616378784\n",
      "Batch :  7\n",
      "Loss: 2.6358156204223633\n",
      "Batch :  8\n",
      "Loss: 2.338740587234497\n",
      "Batch :  9\n",
      "Loss: 2.2730236053466797\n",
      "Batch :  10\n",
      "Loss: 2.8696751594543457\n",
      "Batch :  11\n",
      "Loss: 2.540703058242798\n",
      "Batch :  12\n",
      "Loss: 2.7696571350097656\n",
      "Batch :  13\n",
      "Loss: 2.1067564487457275\n",
      "Batch :  14\n",
      "Loss: 2.5644094944000244\n",
      "Batch :  15\n",
      "Loss: 2.616837739944458\n",
      "Batch :  16\n",
      "Loss: 1.962179183959961\n",
      "Batch :  17\n",
      "Loss: 2.577789545059204\n",
      "Batch :  18\n",
      "Loss: 1.773658275604248\n",
      "Batch :  19\n",
      "Loss: 2.2203001976013184\n",
      "Batch :  20\n",
      "Loss: 2.499727487564087\n",
      "Batch :  21\n",
      "Loss: 1.9440321922302246\n",
      "Batch :  22\n",
      "Loss: 2.203852891921997\n",
      "Batch :  23\n",
      "Loss: 2.055177688598633\n",
      "Batch :  24\n",
      "Loss: 2.552062511444092\n",
      "Batch :  25\n",
      "Loss: 2.314244031906128\n",
      "Batch :  26\n",
      "Loss: 1.8467062711715698\n",
      "Batch :  27\n",
      "Loss: 2.7178971767425537\n",
      "Batch :  28\n",
      "Loss: 2.875096321105957\n",
      "Batch :  29\n",
      "Loss: 2.4748237133026123\n",
      "Batch :  30\n",
      "Loss: 2.328063726425171\n",
      "Batch :  31\n",
      "Loss: 1.9963901042938232\n",
      "Batch :  32\n",
      "Loss: 2.5484185218811035\n",
      "Batch :  33\n",
      "Loss: 2.1317598819732666\n",
      "Batch :  34\n",
      "Loss: 2.1322407722473145\n",
      "Batch :  35\n",
      "Loss: 1.938818335533142\n",
      "Batch :  36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.090219736099243\n",
      "Batch :  37\n",
      "Loss: 2.2811057567596436\n",
      "Batch :  38\n",
      "Loss: 2.320932388305664\n",
      "Batch :  39\n",
      "Loss: 2.3607563972473145\n",
      "Batch :  40\n",
      "Loss: 2.5861690044403076\n",
      "Batch :  41\n",
      "Loss: 2.4403178691864014\n",
      "Batch :  42\n",
      "Loss: 2.4532744884490967\n",
      "Batch :  43\n",
      "Loss: 1.7282763719558716\n",
      "Batch :  44\n",
      "Loss: 2.0423033237457275\n",
      "Batch :  45\n",
      "Loss: 1.9753785133361816\n",
      "Batch :  46\n",
      "Loss: 2.5732269287109375\n",
      "Batch :  47\n",
      "Loss: 1.8082501888275146\n",
      "Batch :  48\n",
      "Loss: 1.7415486574172974\n",
      "Batch :  49\n",
      "Loss: 2.0778896808624268\n",
      "Batch :  50\n",
      "Loss: 1.843035340309143\n",
      "Batch :  51\n",
      "Loss: 2.5965123176574707\n",
      "Batch :  52\n",
      "Loss: 2.030379056930542\n",
      "Batch :  53\n",
      "Loss: 2.2865946292877197\n",
      "Batch :  54\n",
      "Loss: 2.6772994995117188\n",
      "Batch :  55\n",
      "Loss: 1.9923254251480103\n",
      "Batch :  56\n",
      "Loss: 2.573425054550171\n",
      "Batch :  57\n",
      "Loss: 2.361879825592041\n",
      "Batch :  58\n",
      "Loss: 1.8917326927185059\n",
      "Batch :  59\n",
      "Loss: 2.0885701179504395\n",
      "Batch :  60\n",
      "Loss: 2.267735719680786\n",
      "Batch :  61\n",
      "Loss: 2.0327205657958984\n",
      "Batch :  62\n",
      "Loss: 2.081230640411377\n",
      "Batch :  63\n",
      "Loss: 2.0582127571105957\n",
      "Batch :  64\n",
      "Loss: 2.072605609893799\n",
      "Batch :  65\n",
      "Loss: 2.527160882949829\n",
      "Batch :  66\n",
      "Loss: 2.0735156536102295\n",
      "Batch :  67\n",
      "Loss: 1.6552534103393555\n",
      "Batch :  68\n",
      "Loss: 1.6349482536315918\n",
      "Batch :  69\n",
      "Loss: 2.0812582969665527\n",
      "Batch :  70\n",
      "Loss: 1.6022223234176636\n",
      "Batch :  71\n",
      "Loss: 1.5309051275253296\n",
      "Batch :  72\n",
      "Loss: 2.0278854370117188\n",
      "Batch :  73\n",
      "Loss: 2.106071710586548\n",
      "Batch :  74\n",
      "Loss: 1.7794899940490723\n",
      "Batch :  75\n",
      "Loss: 2.118931531906128\n",
      "Batch :  76\n",
      "Loss: 1.705002784729004\n",
      "Batch :  77\n",
      "Loss: 2.288527727127075\n",
      "Batch :  78\n",
      "Loss: 1.7070647478103638\n",
      "Batch :  79\n",
      "Loss: 2.2537074089050293\n",
      "Batch :  80\n",
      "Loss: 2.082869052886963\n",
      "Batch :  81\n",
      "Loss: 1.789599061012268\n",
      "Batch :  82\n",
      "Loss: 2.1431045532226562\n",
      "Batch :  83\n",
      "Loss: 1.7944403886795044\n",
      "Batch :  84\n",
      "Loss: 1.758825421333313\n",
      "Batch :  85\n",
      "Loss: 1.6305989027023315\n",
      "Batch :  86\n",
      "Loss: 1.701866865158081\n",
      "Batch :  87\n",
      "Loss: 2.1118507385253906\n",
      "Batch :  88\n",
      "Loss: 1.848706841468811\n",
      "Batch :  89\n",
      "Loss: 1.7565650939941406\n",
      "Batch :  90\n",
      "Loss: 1.8278067111968994\n",
      "Batch :  91\n",
      "Loss: 1.9365580081939697\n",
      "Batch :  92\n",
      "Loss: 2.1092026233673096\n",
      "Batch :  93\n",
      "Loss: 1.3709254264831543\n",
      "Batch :  94\n",
      "Loss: 1.6006155014038086\n",
      "Batch :  95\n",
      "Loss: 1.7463115453720093\n",
      "Batch :  96\n",
      "Loss: 1.748996615409851\n",
      "Batch :  97\n",
      "Loss: 2.2779030799865723\n",
      "Batch :  98\n",
      "Loss: 1.8898909091949463\n",
      "Batch :  99\n",
      "Loss: 2.0814735889434814\n",
      "Epoch 13\n",
      "Batch :  0\n",
      "Loss: 1.7138385772705078\n",
      "Batch :  1\n",
      "Loss: 2.166001319885254\n",
      "Batch :  2\n",
      "Loss: 2.044710159301758\n",
      "Batch :  3\n",
      "Loss: 2.1549432277679443\n",
      "Batch :  4\n",
      "Loss: 2.717402219772339\n",
      "Batch :  5\n",
      "Loss: 1.9530094861984253\n",
      "Batch :  6\n",
      "Loss: 1.6674422025680542\n",
      "Batch :  7\n",
      "Loss: 2.107034683227539\n",
      "Batch :  8\n",
      "Loss: 1.8885339498519897\n",
      "Batch :  9\n",
      "Loss: 1.8243937492370605\n",
      "Batch :  10\n",
      "Loss: 2.6186363697052\n",
      "Batch :  11\n",
      "Loss: 2.1363580226898193\n",
      "Batch :  12\n",
      "Loss: 2.2654621601104736\n",
      "Batch :  13\n",
      "Loss: 1.577987790107727\n",
      "Batch :  14\n",
      "Loss: 2.081225872039795\n",
      "Batch :  15\n",
      "Loss: 2.1168487071990967\n",
      "Batch :  16\n",
      "Loss: 1.569342851638794\n",
      "Batch :  17\n",
      "Loss: 2.0553371906280518\n",
      "Batch :  18\n",
      "Loss: 1.489612102508545\n",
      "Batch :  19\n",
      "Loss: 1.8350046873092651\n",
      "Batch :  20\n",
      "Loss: 1.9894284009933472\n",
      "Batch :  21\n",
      "Loss: 1.6637520790100098\n",
      "Batch :  22\n",
      "Loss: 1.8416752815246582\n",
      "Batch :  23\n",
      "Loss: 1.5669329166412354\n",
      "Batch :  24\n",
      "Loss: 1.8778806924819946\n",
      "Batch :  25\n",
      "Loss: 1.806559443473816\n",
      "Batch :  26\n",
      "Loss: 1.4746648073196411\n",
      "Batch :  27\n",
      "Loss: 2.247284173965454\n",
      "Batch :  28\n",
      "Loss: 2.227707862854004\n",
      "Batch :  29\n",
      "Loss: 1.937505841255188\n",
      "Batch :  30\n",
      "Loss: 1.884975552558899\n",
      "Batch :  31\n",
      "Loss: 2.1037912368774414\n",
      "Batch :  32\n",
      "Loss: 2.101311206817627\n",
      "Batch :  33\n",
      "Loss: 1.7879949808120728\n",
      "Batch :  34\n",
      "Loss: 1.7073700428009033\n",
      "Batch :  35\n",
      "Loss: 1.5093134641647339\n",
      "Batch :  36\n",
      "Loss: 1.5733466148376465\n",
      "Batch :  37\n",
      "Loss: 1.6513128280639648\n",
      "Batch :  38\n",
      "Loss: 1.8306328058242798\n",
      "Batch :  39\n",
      "Loss: 1.8417490720748901\n",
      "Batch :  40\n",
      "Loss: 1.9117907285690308\n",
      "Batch :  41\n",
      "Loss: 1.8464674949645996\n",
      "Batch :  42\n",
      "Loss: 1.860184669494629\n",
      "Batch :  43\n",
      "Loss: 1.0735716819763184\n",
      "Batch :  44\n",
      "Loss: 1.8781107664108276\n",
      "Batch :  45\n",
      "Loss: 1.4112353324890137\n",
      "Batch :  46\n",
      "Loss: 2.1276485919952393\n",
      "Batch :  47\n",
      "Loss: 1.3691691160202026\n",
      "Batch :  48\n",
      "Loss: 1.281951904296875\n",
      "Batch :  49\n",
      "Loss: 1.6471521854400635\n",
      "Batch :  50\n",
      "Loss: 1.5214135646820068\n",
      "Batch :  51\n",
      "Loss: 1.8783543109893799\n",
      "Batch :  52\n",
      "Loss: 1.597881555557251\n",
      "Batch :  53\n",
      "Loss: 1.8820463418960571\n",
      "Batch :  54\n",
      "Loss: 2.0957722663879395\n",
      "Batch :  55\n",
      "Loss: 1.4763119220733643\n",
      "Batch :  56\n",
      "Loss: 1.9199872016906738\n",
      "Batch :  57\n",
      "Loss: 1.7462389469146729\n",
      "Batch :  58\n",
      "Loss: 1.4164371490478516\n",
      "Batch :  59\n",
      "Loss: 1.4133274555206299\n",
      "Batch :  60\n",
      "Loss: 1.6637182235717773\n",
      "Batch :  61\n",
      "Loss: 1.6476855278015137\n",
      "Batch :  62\n",
      "Loss: 1.566798210144043\n",
      "Batch :  63\n",
      "Loss: 1.6689679622650146\n",
      "Batch :  64\n",
      "Loss: 1.672389268875122\n",
      "Batch :  65\n",
      "Loss: 1.9764037132263184\n",
      "Batch :  66\n",
      "Loss: 1.6092456579208374\n",
      "Batch :  67\n",
      "Loss: 1.3282389640808105\n",
      "Batch :  68\n",
      "Loss: 1.2627809047698975\n",
      "Batch :  69\n",
      "Loss: 1.6014569997787476\n",
      "Batch :  70\n",
      "Loss: 1.387313723564148\n",
      "Batch :  71\n",
      "Loss: 1.3430876731872559\n",
      "Batch :  72\n",
      "Loss: 1.8348819017410278\n",
      "Batch :  73\n",
      "Loss: 1.7252815961837769\n",
      "Batch :  74\n",
      "Loss: 1.4323937892913818\n",
      "Batch :  75\n",
      "Loss: 1.6200017929077148\n",
      "Batch :  76\n",
      "Loss: 1.3468587398529053\n",
      "Batch :  77\n",
      "Loss: 1.8232841491699219\n",
      "Batch :  78\n",
      "Loss: 1.479183554649353\n",
      "Batch :  79\n",
      "Loss: 1.8213410377502441\n",
      "Batch :  80\n",
      "Loss: 1.5952492952346802\n",
      "Batch :  81\n",
      "Loss: 1.5573978424072266\n",
      "Batch :  82\n",
      "Loss: 1.791466236114502\n",
      "Batch :  83\n",
      "Loss: 1.6087929010391235\n",
      "Batch :  84\n",
      "Loss: 1.5329339504241943\n",
      "Batch :  85\n",
      "Loss: 1.2770583629608154\n",
      "Batch :  86\n",
      "Loss: 1.3882945775985718\n",
      "Batch :  87\n",
      "Loss: 1.7306444644927979\n",
      "Batch :  88\n",
      "Loss: 1.422730565071106\n",
      "Batch :  89\n",
      "Loss: 1.332561731338501\n",
      "Batch :  90\n",
      "Loss: 1.5242754220962524\n",
      "Batch :  91\n",
      "Loss: 1.543839693069458\n",
      "Batch :  92\n",
      "Loss: 1.5659127235412598\n",
      "Batch :  93\n",
      "Loss: 1.0330647230148315\n",
      "Batch :  94\n",
      "Loss: 1.2762513160705566\n",
      "Batch :  95\n",
      "Loss: 1.3149176836013794\n",
      "Batch :  96\n",
      "Loss: 1.5412520170211792\n",
      "Batch :  97\n",
      "Loss: 1.8032536506652832\n",
      "Batch :  98\n",
      "Loss: 1.4544274806976318\n",
      "Batch :  99\n",
      "Loss: 1.7325685024261475\n",
      "Epoch 14\n",
      "Batch :  0\n",
      "Loss: 1.396938443183899\n",
      "Batch :  1\n",
      "Loss: 1.7641774415969849\n",
      "Batch :  2\n",
      "Loss: 1.5292208194732666\n",
      "Batch :  3\n",
      "Loss: 1.800290584564209\n",
      "Batch :  4\n",
      "Loss: 2.283553123474121\n",
      "Batch :  5\n",
      "Loss: 1.382899284362793\n",
      "Batch :  6\n",
      "Loss: 1.3347086906433105\n",
      "Batch :  7\n",
      "Loss: 1.58820641040802\n",
      "Batch :  8\n",
      "Loss: 1.4303022623062134\n",
      "Batch :  9\n",
      "Loss: 1.447214126586914\n",
      "Batch :  10\n",
      "Loss: 1.985742449760437\n",
      "Batch :  11\n",
      "Loss: 1.7352288961410522\n",
      "Batch :  12\n",
      "Loss: 1.8629683256149292\n",
      "Batch :  13\n",
      "Loss: 1.266398310661316\n",
      "Batch :  14\n",
      "Loss: 1.790385127067566\n",
      "Batch :  15\n",
      "Loss: 1.8439383506774902\n",
      "Batch :  16\n",
      "Loss: 1.3588365316390991\n",
      "Batch :  17\n",
      "Loss: 1.6607602834701538\n",
      "Batch :  18\n",
      "Loss: 1.1566089391708374\n",
      "Batch :  19\n",
      "Loss: 1.4564597606658936\n",
      "Batch :  20\n",
      "Loss: 1.629947543144226\n",
      "Batch :  21\n",
      "Loss: 1.241335391998291\n",
      "Batch :  22\n",
      "Loss: 1.4437743425369263\n",
      "Batch :  23\n",
      "Loss: 1.252315878868103\n",
      "Batch :  24\n",
      "Loss: 1.5124001502990723\n",
      "Batch :  25\n",
      "Loss: 1.403939962387085\n",
      "Batch :  26\n",
      "Loss: 1.1677225828170776\n",
      "Batch :  27\n",
      "Loss: 1.923223614692688\n",
      "Batch :  28\n",
      "Loss: 1.8094770908355713\n",
      "Batch :  29\n",
      "Loss: 1.6329936981201172\n",
      "Batch :  30\n",
      "Loss: 1.3244054317474365\n",
      "Batch :  31\n",
      "Loss: 1.4770749807357788\n",
      "Batch :  32\n",
      "Loss: 1.7158153057098389\n",
      "Batch :  33\n",
      "Loss: 1.40884530544281\n",
      "Batch :  34\n",
      "Loss: 1.487748622894287\n",
      "Batch :  35\n",
      "Loss: 1.2590093612670898\n",
      "Batch :  36\n",
      "Loss: 1.3566102981567383\n",
      "Batch :  37\n",
      "Loss: 1.5064857006072998\n",
      "Batch :  38\n",
      "Loss: 1.7157490253448486\n",
      "Batch :  39\n",
      "Loss: 1.6644436120986938\n",
      "Batch :  40\n",
      "Loss: 1.6314356327056885\n",
      "Batch :  41\n",
      "Loss: 1.4885984659194946\n",
      "Batch :  42\n",
      "Loss: 1.5649863481521606\n",
      "Batch :  43\n",
      "Loss: 0.9520823359489441\n",
      "Batch :  44\n",
      "Loss: 1.451812982559204\n",
      "Batch :  45\n",
      "Loss: 1.1417021751403809\n",
      "Batch :  46\n",
      "Loss: 1.6309372186660767\n",
      "Batch :  47\n",
      "Loss: 1.0242376327514648\n",
      "Batch :  48\n",
      "Loss: 1.2226550579071045\n",
      "Batch :  49\n",
      "Loss: 1.4438527822494507\n",
      "Batch :  50\n",
      "Loss: 1.1449154615402222\n",
      "Batch :  51\n",
      "Loss: 1.6915477514266968\n",
      "Batch :  52\n",
      "Loss: 1.3391218185424805\n",
      "Batch :  53\n",
      "Loss: 1.537224292755127\n",
      "Batch :  54\n",
      "Loss: 1.7591629028320312\n",
      "Batch :  55\n",
      "Loss: 1.131868600845337\n",
      "Batch :  56\n",
      "Loss: 1.6823540925979614\n",
      "Batch :  57\n",
      "Loss: 1.4668312072753906\n",
      "Batch :  58\n",
      "Loss: 1.0728503465652466\n",
      "Batch :  59\n",
      "Loss: 1.12834894657135\n",
      "Batch :  60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.441646933555603\n",
      "Batch :  61\n",
      "Loss: 1.3191155195236206\n",
      "Batch :  62\n",
      "Loss: 1.2419353723526\n",
      "Batch :  63\n",
      "Loss: 1.2778549194335938\n",
      "Batch :  64\n",
      "Loss: 1.3462048768997192\n",
      "Batch :  65\n",
      "Loss: 1.7126545906066895\n",
      "Batch :  66\n",
      "Loss: 1.3625879287719727\n",
      "Batch :  67\n",
      "Loss: 0.928442120552063\n",
      "Batch :  68\n",
      "Loss: 1.0959209203720093\n",
      "Batch :  69\n",
      "Loss: 1.3328137397766113\n",
      "Batch :  70\n",
      "Loss: 1.190093994140625\n",
      "Batch :  71\n",
      "Loss: 0.9441039562225342\n",
      "Batch :  72\n",
      "Loss: 1.4681552648544312\n",
      "Batch :  73\n",
      "Loss: 1.4860478639602661\n",
      "Batch :  74\n",
      "Loss: 1.2522250413894653\n",
      "Batch :  75\n",
      "Loss: 1.3187129497528076\n",
      "Batch :  76\n",
      "Loss: 1.2567378282546997\n",
      "Batch :  77\n",
      "Loss: 1.7259953022003174\n",
      "Batch :  78\n",
      "Loss: 1.228967308998108\n",
      "Batch :  79\n",
      "Loss: 1.657352328300476\n",
      "Batch :  80\n",
      "Loss: 1.3634533882141113\n",
      "Batch :  81\n",
      "Loss: 1.219016194343567\n",
      "Batch :  82\n",
      "Loss: 1.4933913946151733\n",
      "Batch :  83\n",
      "Loss: 1.3038687705993652\n",
      "Batch :  84\n",
      "Loss: 1.3020403385162354\n",
      "Batch :  85\n",
      "Loss: 1.1719930171966553\n",
      "Batch :  86\n",
      "Loss: 1.2488116025924683\n",
      "Batch :  87\n",
      "Loss: 1.3932204246520996\n",
      "Batch :  88\n",
      "Loss: 1.2470582723617554\n",
      "Batch :  89\n",
      "Loss: 1.1989588737487793\n",
      "Batch :  90\n",
      "Loss: 1.3461997509002686\n",
      "Batch :  91\n",
      "Loss: 1.3233819007873535\n",
      "Batch :  92\n",
      "Loss: 1.2827177047729492\n",
      "Batch :  93\n",
      "Loss: 0.8428812026977539\n",
      "Batch :  94\n",
      "Loss: 1.152600646018982\n",
      "Batch :  95\n",
      "Loss: 1.0730319023132324\n",
      "Batch :  96\n",
      "Loss: 1.3250846862792969\n",
      "Batch :  97\n",
      "Loss: 1.47260582447052\n",
      "Batch :  98\n",
      "Loss: 1.3022733926773071\n",
      "Batch :  99\n",
      "Loss: 1.563393235206604\n",
      "Epoch 15\n",
      "Batch :  0\n",
      "Loss: 1.1886029243469238\n",
      "Batch :  1\n",
      "Loss: 1.4345698356628418\n",
      "Batch :  2\n",
      "Loss: 1.2225905656814575\n",
      "Batch :  3\n",
      "Loss: 1.4778552055358887\n",
      "Batch :  4\n",
      "Loss: 1.6722311973571777\n",
      "Batch :  5\n",
      "Loss: 1.1074172258377075\n",
      "Batch :  6\n",
      "Loss: 1.0633658170700073\n",
      "Batch :  7\n",
      "Loss: 1.3807053565979004\n",
      "Batch :  8\n",
      "Loss: 1.1793898344039917\n",
      "Batch :  9\n",
      "Loss: 1.2768793106079102\n",
      "Batch :  10\n",
      "Loss: 1.6646240949630737\n",
      "Batch :  11\n",
      "Loss: 1.469483733177185\n",
      "Batch :  12\n",
      "Loss: 1.612510085105896\n",
      "Batch :  13\n",
      "Loss: 0.9309089183807373\n",
      "Batch :  14\n",
      "Loss: 1.4590644836425781\n",
      "Batch :  15\n",
      "Loss: 1.4363374710083008\n",
      "Batch :  16\n",
      "Loss: 0.9933430552482605\n",
      "Batch :  17\n",
      "Loss: 1.2586451768875122\n",
      "Batch :  18\n",
      "Loss: 0.9504831433296204\n",
      "Batch :  19\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-3).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.variables_initializer(saved_vars))\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    for batchNum in range(len(batch_input[0])):\n",
    "        print(\"Batch : \",batchNum)\n",
    "        t = session.run([optimizer, loss, decoder_outputs, suppression_loss], {\n",
    "            d_tokens: batch_input[0][batchNum],\n",
    "            d_lengths: batch_input[1][batchNum],\n",
    "            a_labels: batch_input[2][batchNum],\n",
    "            encoder_input_mask: batch_input[3][batchNum],\n",
    "            encoder_lengths: batch_input[4][batchNum],\n",
    "            decoder_inputs: batch_input[5][batchNum],\n",
    "            decoder_labels: batch_input[6][batchNum],\n",
    "            decoder_lengths: batch_input[7][batchNum],\n",
    "            s_answer: batch_input[8][batchNum],\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(t[1]))\n",
    "    if(epoch%5 == 0):\n",
    "        print(\"Saving model\")\n",
    "        #saver.save(session, \"qgen-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-1.50505602 -3.07428956  2.56434155 ..., -3.07349539 -3.09046841\n",
      "   -3.03399873]\n",
      "  [-1.76248872 -3.63838267  3.2439065  ..., -3.60971045 -3.58260107\n",
      "   -3.6037097 ]\n",
      "  [-1.89708757 -3.88434219  3.52075863 ..., -3.82470012 -3.79168701\n",
      "   -3.84030294]\n",
      "  ..., \n",
      "  [-2.04667497 -4.1406498   3.82709146 ..., -4.07375765 -4.03911209\n",
      "   -4.09230137]\n",
      "  [-2.04751468 -4.14365482  3.82962894 ..., -4.07715559 -4.04227924\n",
      "   -4.09470797]\n",
      "  [-2.04816628 -4.14639616  3.83192492 ..., -4.08027935 -4.04514122\n",
      "   -4.09690571]]\n",
      "\n",
      " [[-1.8915031  -3.94711781  3.36775589 ..., -3.78215742 -3.83540106\n",
      "   -3.81771779]\n",
      "  [-1.97481418 -4.14009571  3.65373945 ..., -3.9985826  -4.01781034\n",
      "   -4.04910374]\n",
      "  [-2.0104506  -4.19613743  3.75386572 ..., -4.06355572 -4.07419825\n",
      "   -4.12057781]\n",
      "  ..., \n",
      "  [-2.06712484 -4.23035145  3.86246133 ..., -4.13321733 -4.09855413\n",
      "   -4.1683917 ]\n",
      "  [-2.06826019 -4.22900057  3.86204839 ..., -4.13337708 -4.09799767\n",
      "   -4.16759014]\n",
      "  [-2.06893539 -4.2278266   3.86186838 ..., -4.13366032 -4.09758949\n",
      "   -4.16688824]]\n",
      "\n",
      " [[-1.88013005 -3.87152386  3.29846358 ..., -3.7286849  -3.79381537\n",
      "   -3.76498032]\n",
      "  [-1.98060858 -4.10726023  3.63085699 ..., -3.97976255 -3.99573946\n",
      "   -4.02115154]\n",
      "  [-2.02381229 -4.17123318  3.72794366 ..., -4.05074883 -4.04597282\n",
      "   -4.09477329]\n",
      "  ..., \n",
      "  [-2.04314899 -4.23872757  3.88732529 ..., -4.15039635 -4.10966778\n",
      "   -4.19590759]\n",
      "  [-2.03739262 -4.24235249  3.89061403 ..., -4.15428448 -4.11463356\n",
      "   -4.19712591]\n",
      "  [-2.04232621 -4.2427063   3.8987844  ..., -4.14458084 -4.11690187\n",
      "   -4.18986034]]\n",
      "\n",
      " ..., \n",
      " [[-1.80674505 -3.72418189  3.17803812 ..., -3.63802314 -3.68150878\n",
      "   -3.65071917]\n",
      "  [-1.94555628 -4.02657795  3.57415676 ..., -3.92764163 -3.93769097\n",
      "   -3.96361446]\n",
      "  [-1.98931932 -4.13345003  3.72063494 ..., -4.02601862 -4.02158308\n",
      "   -4.076231  ]\n",
      "  ..., \n",
      "  [-2.06761646 -4.21443319  3.86576056 ..., -4.12213993 -4.09062386\n",
      "   -4.15673542]\n",
      "  [-2.06774044 -4.21416664  3.86636877 ..., -4.12335682 -4.09114599\n",
      "   -4.15676785]\n",
      "  [-2.06776881 -4.21388626  3.86692023 ..., -4.1244669  -4.09160852\n",
      "   -4.15675783]]\n",
      "\n",
      " [[-1.45651042 -2.98926783  2.4816246  ..., -2.98181272 -3.00183153\n",
      "   -2.93712568]\n",
      "  [-1.73354244 -3.58235312  3.19267368 ..., -3.55408144 -3.52669811\n",
      "   -3.54210782]\n",
      "  [-1.87774062 -3.83721447  3.49003148 ..., -3.78221631 -3.74793839\n",
      "   -3.79815292]\n",
      "  ..., \n",
      "  [-2.04293489 -4.13442612  3.82597113 ..., -4.06618547 -4.03122997\n",
      "   -4.08688831]\n",
      "  [-2.04410028 -4.13705921  3.82751584 ..., -4.06963253 -4.03432941\n",
      "   -4.08889294]\n",
      "  [-2.04494405 -4.13959599  3.82915258 ..., -4.07291746 -4.03723049\n",
      "   -4.09087276]]\n",
      "\n",
      " [[-1.88575017 -3.90033817  3.32405329 ..., -3.74974608 -3.80613875\n",
      "   -3.78445721]\n",
      "  [-1.98022258 -4.11250019  3.63569832 ..., -3.97673178 -3.9996891\n",
      "   -4.03226852]\n",
      "  [-2.0159421  -4.18427658  3.7487793  ..., -4.05271816 -4.04609728\n",
      "   -4.11215305]\n",
      "  ..., \n",
      "  [-2.04251242 -4.24353266  3.89233303 ..., -4.1518631  -4.11892128\n",
      "   -4.18477774]\n",
      "  [-2.03731251 -4.24551487  3.8961122  ..., -4.15568066 -4.11831427\n",
      "   -4.19307995]\n",
      "  [-2.04285526 -4.24580288  3.9030323  ..., -4.14593935 -4.11881495\n",
      "   -4.1878624 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(t[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(session, 'qgen-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = session.run(answer_tags, {\n",
    "    d_tokens: batch_input[0][48],\n",
    "    d_lengths: batch_input[1][48],\n",
    "})\n",
    "answers = np.argmax(answers, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(28):\n",
    "    print(\"Prediction\")\n",
    "    printAllAns(answers,48,0)\n",
    "    print(\"Ground Truth\")\n",
    "    printAllAns(batch_input[2][48],48,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDoc(batch,num):\n",
    "    for i in batch_input[0][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "def printQues(batch,num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAnsForQuestion(batch, num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAllAns(answers, batch, num):\n",
    "    for i,word in enumerate(batch_input[0][batch][num]):\n",
    "        if answers[num][i] == 1 :\n",
    "            print(look_up_token_reduced(word),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "printQues(47,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batchNum = 80\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch_input[0][batchNum].shape[0]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    d_tokens: batch_input[0][batchNum],\n",
    "    d_lengths: batch_input[1][batchNum],\n",
    "    a_labels: batch_input[2][batchNum],\n",
    "    encoder_input_mask: batch_input[3][batchNum],\n",
    "    encoder_lengths: batch_input[4][batchNum],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[0][batchNum].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[:,:,END_TOKEN] = 0\n",
    "qs = np.argmax(questions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = set()\n",
    "l = []\n",
    "for i in range(batch_input[0][batchNum].shape[0]):\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, qs[i])\n",
    "    print(\"Generated Question: \" + \" \".join(look_up_token_reduced(token) for token in question))\n",
    "    print(\"Ground Truth Question: \")\n",
    "    printQues(batchNum,i)\n",
    "    print(\"Ground Truth Answer: \", X_train_ans_shuffled[batch_size*batchNum + i])\n",
    "    print(\"Context:\")\n",
    "    printDoc(batchNum,i)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[6][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(batch_input[7][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
