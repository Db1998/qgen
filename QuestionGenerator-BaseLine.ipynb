{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "from embedding import *\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')\n",
    "nltkStopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 50000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "1510\n",
      "rev\n",
      ".\n",
      "john\n",
      "j.\n",
      "cavanaugh\n",
      ",\n",
      "c.s.c\n",
      ".\n",
      "served\n",
      "more\n",
      "than\n",
      "half\n",
      ",\n",
      "lobund\n",
      "institute\n",
      "for\n",
      "animal\n",
      "studies\n",
      "and\n",
      "medieval\n",
      "institute\n",
      ".\n",
      "hall\n",
      "of\n",
      "liberal\n",
      "arts\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'s\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '``', 'ad', 'me', 'omnes', \"''\", '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in', '1858.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n",
      "['saint', 'bernadette', 'soubirous']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 104)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510\n",
      "['the', 'success', 'of', 'its', 'football', 'team', 'made', 'notre', 'dame', 'a', 'household', 'name', '.', 'the', 'success', 'of', 'note', 'dame', 'reflected', 'rising', 'status', 'of', 'irish', 'americans', 'and', 'catholics', 'in', 'the', '1920s', '.', 'catholics', 'rallied', 'up', 'around', 'the', 'team', 'and', 'listen', 'to', 'the', 'games', 'on', 'the', 'radio', ',', 'especially', 'when', 'it', 'knocked', 'off', 'the', 'schools', 'that', 'symbolized', 'the', 'protestant', 'establishment', 'in', 'america', 'â€”', 'harvard', ',', 'yale', ',', 'princeton', ',', 'and', 'army', '.', 'yet', 'this', 'role', 'as', 'high-profile', 'flagship', 'institution', 'of', 'catholicism', 'made', 'it', 'an', 'easy', 'target', 'of', 'anti-catholicism', '.', 'the', 'most', 'remarkable', 'episode', 'of', 'violence', 'was', 'the', 'clash', 'between', 'notre', 'dame', 'students', 'and', 'the', 'ku', 'klux', 'klan', 'in', '1924.', 'nativism', 'and', 'anti-catholicism', ',', 'especially', 'when', 'directed', 'towards', 'immigrants', ',', 'were', 'cornerstones', 'of', 'the', 'kkk', \"'s\", 'rhetoric', ',', 'and', 'notre', 'dame', 'was', 'seen', 'as', 'a', 'symbol', 'of', 'the', 'threat', 'posed', 'by', 'the', 'catholic', 'church', '.', 'the', 'klan', 'decided', 'to', 'have', 'a', 'week-long', 'klavern', 'in', 'south', 'bend', '.', 'clashes', 'with', 'the', 'student', 'body', 'started', 'on', 'march', '17', ',', 'when', 'students', ',', 'aware', 'of', 'the', 'anti-catholic', 'animosity', ',', 'blocked', 'the', 'klansmen', 'from', 'descending', 'from', 'their', 'trains', 'in', 'the', 'south', 'bend', 'station', 'and', 'ripped', 'the', 'kkk', 'clothes', 'and', 'regalia', '.', 'on', 'may', '19', 'thousands', 'of', 'students', 'massed', 'downtown', 'protesting', 'the', 'klavern', ',', 'and', 'only', 'the', 'arrival', 'of', 'college', 'president', 'fr', '.', 'matthew', 'walsh', 'prevented', 'any', 'further', 'clashes', '.', 'the', 'next', 'day', ',', 'football', 'coach', 'knute', 'rockne', 'spoke', 'at', 'a', 'campus', 'rally', 'and', 'implored', 'the', 'students', 'to', 'obey', 'the', 'college', 'president', 'and', 'refrain', 'from', 'further', 'violence', '.', 'a', 'few', 'days', 'later', 'the', 'klavern', 'broke', 'up', ',', 'but', 'the', 'hostility', 'shown', 'by', 'the', 'students', 'was', 'an', 'omen', 'and', 'a', 'contribution', 'to', 'the', 'downfall', 'of', 'the', 'kkk', 'in', 'indiana', '.']\n",
      "['the', 'ku', 'klux', 'klan']\n",
      "['notre', 'dame', 'students', 'had', 'a', 'showdown', 'in', '1924', 'with', 'which', 'anti-catholic', 'group', '?']\n",
      "['bern', 'was', 'the', 'site', 'of', 'the', '1954', 'football', '(', 'soccer', ')', 'world', 'cup', 'final', ',', 'a', 'huge', 'upset', 'for', 'the', 'hungarian', 'golden', 'team', ',', 'who', 'were', 'beaten', '3â€“2', 'by', 'west', 'germany', '.', 'the', 'football', 'team', 'bsc', 'young', 'boys', 'is', 'based', 'in', 'bern', 'at', 'the', 'stade', 'de', 'suisse', 'wankdorf', ',', 'which', 'also', 'was', 'one', 'of', 'the', 'venues', 'for', 'the', 'european', 'football', 'championship', '2008', 'in', 'which', 'it', 'hosted', '3', 'matches', '.']\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "['bsc', 'young', 'boys']\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  0.  0.  0.  1.\n",
      "  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "['what', 'football', 'team', 'is', 'based', 'in', 'bern', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 10000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "expression_contexts = np.ones((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 766)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1\n",
    "\n",
    "    for j,word in enumerate(X_train_comp[i]):\n",
    "        expression_contexts[i,look_up_word_reduced(word),:] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "61672\n",
      "5881\n",
      "59463\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   76,    27,   132,  1771,     9,     3,     0,   308,     8,\n",
       "        1325,   558,     6,   190,   460,    11,   438,     6,     3,\n",
       "        1125,     5,   438,   172,   138,   558,     4,   265,     7,\n",
       "        1325,   558,  1043,     3,   818,     5,     3,   604,    13,\n",
       "         939,  4205,    23,   320,  8234,    15,     0,    14,   532,\n",
       "        1847,     9,     3,   231,    20,    39,   144,  2261,     5,\n",
       "         438,     6,    53,  1575,     8,  3965,     4,  4202,     4,\n",
       "           7,  5223,     6,    36,    11,  5827,     8,     3,     0,\n",
       "           4,     0,     4,     7,     0,  1068,    44,    29,    31,\n",
       "          50,    78,   256,  1110,     8,   149,     5,    39,   209,\n",
       "           6,     3,   155,   460,    11,     3,   393,     5,  4392,\n",
       "          32, 25436,     6,   101,   294,   214,   100,     3, 11651,\n",
       "        4392,   432,    10,   367,   393,     8,  5635,     3,  1430,\n",
       "          18,  5180,   804,   975,     9,     3,   231,     6,  1103,\n",
       "           4,     3,  2387,  1710,  6671,  1847,     7,    19,   339,\n",
       "        4371,     3,     0,     5,     3,   604,   340,     3,   231,\n",
       "           6,     3,  2337,     5,     3,   231,    21,    56,   218,\n",
       "         704,     4, 21734,  5218,     4,  6671,  1847,     8,     3,\n",
       "        2691,    83,     4, 26060,    10,   133,     5,  6923,  3027,\n",
       "           5,  1847,   340,     3,   231,    13,  1847,   661,    10,\n",
       "        1772,   290,   172,     3,  2387,     7,  1774,  1710,     6,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 766)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellFlag = 'GRU'\n",
    "minQuestionLoss = 1.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "embedding = tf.get_variable(\"embedding\", initializer=reduced_glove)\n",
    "embedding = tf.cast(embedding, dtype=tf.float32)\n",
    "EMBEDDING_DIMENS = glove.shape[1]\n",
    "\n",
    "d_tokens = tf.placeholder(tf.int32, shape=[None, None], name=\"d_tokens\")\n",
    "d_lengths = tf.placeholder(tf.int32, shape=[None], name=\"d_lengths\")\n",
    "\n",
    "\n",
    "document_emb = tf.nn.embedding_lookup(embedding, d_tokens, name=\"document_emb\")\n",
    "document_emb = tf.cast(document_emb, dtype=tf.float64, name=\"casted_document_emb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cellFlag == 'LSTM':\n",
    "    forward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "elif cellFlag == 'GRU':\n",
    "    forward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "\n",
    "answer_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, document_emb, d_lengths, dtype=tf.float64,\n",
    "    scope=\"answer_rnn\")\n",
    "\n",
    "answer_outputs = tf.concat(answer_outputs, 2, name=\"answer_output_concat\")\n",
    "\n",
    "answer_outputs = tf.cast(answer_outputs,tf.float32, name=\"answer_output_concat\")\n",
    "\n",
    "answer_tags = tf.layers.dense(inputs=answer_outputs, units=2, name=\"answer_tags\")\n",
    "\n",
    "\n",
    "a_labels = tf.placeholder(tf.int32, shape=[None, None], name=\"a_labels\")\n",
    "\n",
    "answer_mask = tf.sequence_mask(d_lengths, dtype=tf.float32, name=\"answer_mask\")\n",
    "\n",
    "answer_loss = seq2seq.sequence_loss(\n",
    "    logits=answer_tags, targets=a_labels, weights=answer_mask, name=\"answer_loss\")\n",
    "\n",
    "answer_loss = tf.Print(answer_loss, [answer_loss], message=\"This is answer_loss: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_mask = tf.placeholder(\n",
    "    tf.float32, shape=[None, None, None], name=\"encoder_input_mask\")\n",
    "encoder_inputs = tf.matmul(encoder_input_mask, answer_outputs, name=\"encoder_inputs\")\n",
    "encoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"encoder_lengths\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    encoder_cell = tf.contrib.rnn.GRUCell(forward_cell.state_size + backward_cell.state_size)\n",
    "elif cellFlag == 'LSTM':\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs, encoder_lengths, dtype=tf.float32, scope=\"encoder_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"decoder_inputs\")\n",
    "decoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"decoder_lengths\")\n",
    "\n",
    "decoder_emb = tf.nn.embedding_lookup(embedding, decoder_inputs,name=\"decoder_embedding\")\n",
    "decoder_emb = tf.cast(decoder_emb,tf.float32,name=\"decoder_embedding_cast\")\n",
    "\n",
    "helper = seq2seq.TrainingHelper(decoder_emb , decoder_lengths, name=\"helper\")\n",
    "\n",
    "\n",
    "projection = Dense(embedding.shape[0], use_bias=False, name=\"projection\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    decoder_cell = tf.contrib.rnn.GRUCell(encoder_cell.state_size)\n",
    "elif cellFlag == \"LSTM\":\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, scope=\"decoder\")\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "decoder_labels = tf.placeholder(tf.int64, shape=[None, None], name=\"decoder_labels\")\n",
    "question_mask = tf.sequence_mask(decoder_lengths ,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL Loss\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "question_loss = tf.Print(question_loss, [question_loss], message=\"This is question_loss: \")\n",
    "\n",
    "#Suppression Loss\n",
    "s_answer = tf.placeholder(tf.float32, shape=[None,None,None], name=\"suppression_answer\")\n",
    "lambdaSuppress = 0.5\n",
    "\n",
    "suppression_loss = lambdaSuppress * tf.reduce_sum(tf.matmul(tf.exp(decoder_outputs), s_answer))\n",
    "suppression_loss = tf.Print(suppression_loss, [suppression_loss], message=\"This is suppression_loss: \")\n",
    "\n",
    "\n",
    "#Expression Loss\n",
    "e_context = tf.placeholder(tf.float32, shape=[None,None,None], name=\"expression_answer\")\n",
    "lambdaExpress = 0.1\n",
    "\n",
    "expression_loss = lambdaExpress * tf.reduce_sum(tf.matmul(tf.exp(decoder_outputs), e_context))\n",
    "expression_loss = tf.Print(expression_loss, [expression_loss], message=\"This is expression loss: \")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Maximize Entropy Loss\n",
    "entropy_loss = tf.matmul(tf.transpose(decoder_outputs),decoder_outputs)\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lossFlags = tf.placeholder(shape = (4), dtype=tf.float32)\n",
    "x = tf.stack([question_loss,answer_loss,suppression_loss,expression_loss])\n",
    "loss = tf.reduce_sum(tf.multiply(x, lossFlags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train,max_answer_len),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(*ls):\n",
    "    l =list(zip(*ls))\n",
    "    np.random.shuffle(l)\n",
    "    return zip(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index == 8: # \n",
    "                output.append(inp[start:start+batch_size,:,:]) \n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        maxD = max(inputs[1][start:])\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ]) \n",
    "        elif index ==8:\n",
    "            output.append(inp[start:,:, :]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer,expression_contexts]\n",
    "                    ,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of features: 10\n",
      "No of batches: 313\n"
     ]
    }
   ],
   "source": [
    "print(\"No of features:\",len( batch_input))\n",
    "print(\"No of batches:\",len( batch_input[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saved_vars = []\n",
    "l = len(tf.all_variables())\n",
    "for i,var in enumerate(tf.all_variables()):\n",
    "    print(i,\"/\",l)\n",
    "    saved_vars.append(var)\n",
    "        \n",
    "print(len(saved_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-3).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.variables_initializer(saved_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch :  0\n",
      "Loss: 11.697443962097168\n",
      "Batch :  1\n",
      "Loss: 11.497557640075684\n",
      "Batch :  2\n",
      "Loss: 10.212152481079102\n",
      "Batch :  3\n",
      "Loss: 8.543424606323242\n",
      "Batch :  4\n",
      "Loss: 8.203423500061035\n",
      "Batch :  5\n",
      "Loss: 7.71380615234375\n",
      "Batch :  6\n",
      "Loss: 8.082677841186523\n",
      "Batch :  7\n",
      "Loss: 8.182862281799316\n",
      "Batch :  8\n",
      "Loss: 8.126458168029785\n",
      "Batch :  9\n",
      "Loss: 8.502995491027832\n",
      "Batch :  10\n",
      "Loss: 8.176431655883789\n",
      "Batch :  11\n",
      "Loss: 8.149927139282227\n",
      "Batch :  12\n",
      "Loss: 8.13116455078125\n",
      "Batch :  13\n",
      "Loss: 7.777310848236084\n",
      "Batch :  14\n",
      "Loss: 7.7111406326293945\n",
      "Batch :  15\n",
      "Loss: 7.822638988494873\n",
      "Batch :  16\n",
      "Loss: 7.539064884185791\n",
      "Batch :  17\n",
      "Loss: 7.27225923538208\n",
      "Batch :  18\n",
      "Loss: 7.47455358505249\n",
      "Batch :  19\n",
      "Loss: 7.253452777862549\n",
      "Batch :  20\n",
      "Loss: 7.369457721710205\n",
      "Batch :  21\n",
      "Loss: 7.182992458343506\n",
      "Batch :  22\n",
      "Loss: 7.137919902801514\n",
      "Batch :  23\n",
      "Loss: 7.269352436065674\n",
      "Batch :  24\n",
      "Loss: 7.018163204193115\n",
      "Batch :  25\n",
      "Loss: 7.210416316986084\n",
      "Batch :  26\n",
      "Loss: 7.168123245239258\n",
      "Batch :  27\n",
      "Loss: 7.356608867645264\n",
      "Batch :  28\n",
      "Loss: 7.305673599243164\n",
      "Batch :  29\n",
      "Loss: 7.254888534545898\n",
      "Batch :  30\n",
      "Loss: 7.606753349304199\n",
      "Batch :  31\n",
      "Loss: 7.027230739593506\n",
      "Batch :  32\n",
      "Loss: 7.093895435333252\n",
      "Batch :  33\n",
      "Loss: 6.880685806274414\n",
      "Batch :  34\n",
      "Loss: 7.031980991363525\n",
      "Batch :  35\n",
      "Loss: 7.357849597930908\n",
      "Batch :  36\n",
      "Loss: 7.281151294708252\n",
      "Batch :  37\n",
      "Loss: 7.376643180847168\n",
      "Batch :  38\n",
      "Loss: 7.160771369934082\n",
      "Batch :  39\n",
      "Loss: 6.996489524841309\n",
      "Batch :  40\n",
      "Loss: 6.774441719055176\n",
      "Batch :  41\n",
      "Loss: 7.249486446380615\n",
      "Batch :  42\n",
      "Loss: 7.325042247772217\n",
      "Batch :  43\n",
      "Loss: 6.728730201721191\n",
      "Batch :  44\n",
      "Loss: 6.581830024719238\n",
      "Batch :  45\n",
      "Loss: 6.81895112991333\n",
      "Batch :  46\n",
      "Loss: 7.054879188537598\n",
      "Batch :  47\n",
      "Loss: 6.886054039001465\n",
      "Batch :  48\n",
      "Loss: 6.878079891204834\n",
      "Batch :  49\n",
      "Loss: 6.959400177001953\n",
      "Batch :  50\n",
      "Loss: 7.1035590171813965\n",
      "Batch :  51\n",
      "Loss: 6.678219795227051\n",
      "Batch :  52\n",
      "Loss: 6.50669002532959\n",
      "Batch :  53\n",
      "Loss: 6.764137268066406\n",
      "Batch :  54\n",
      "Loss: 6.573122978210449\n",
      "Batch :  55\n",
      "Loss: 6.532604217529297\n",
      "Batch :  56\n",
      "Loss: 6.586906909942627\n",
      "Batch :  57\n",
      "Loss: 6.895935535430908\n",
      "Batch :  58\n",
      "Loss: 6.681914806365967\n",
      "Batch :  59\n",
      "Loss: 6.411530017852783\n",
      "Batch :  60\n",
      "Loss: 6.802533149719238\n",
      "Batch :  61\n",
      "Loss: 6.856774806976318\n",
      "Batch :  62\n",
      "Loss: 6.824245452880859\n",
      "Batch :  63\n",
      "Loss: 6.941099643707275\n",
      "Batch :  64\n",
      "Loss: 6.488059997558594\n",
      "Batch :  65\n",
      "Loss: 6.688518524169922\n",
      "Batch :  66\n",
      "Loss: 6.738852024078369\n",
      "Batch :  67\n",
      "Loss: 6.345511436462402\n",
      "Batch :  68\n",
      "Loss: 6.566586017608643\n",
      "Batch :  69\n",
      "Loss: 6.379972457885742\n",
      "Batch :  70\n",
      "Loss: 6.412423133850098\n",
      "Batch :  71\n",
      "Loss: 6.405540943145752\n",
      "Batch :  72\n",
      "Loss: 6.503867149353027\n",
      "Batch :  73\n",
      "Loss: 6.478460311889648\n",
      "Batch :  74\n",
      "Loss: 6.40750789642334\n",
      "Batch :  75\n",
      "Loss: 6.870804786682129\n",
      "Batch :  76\n",
      "Loss: 5.950205326080322\n",
      "Batch :  77\n",
      "Loss: 6.461529731750488\n",
      "Batch :  78\n",
      "Loss: 6.671370029449463\n",
      "Batch :  79\n",
      "Loss: 6.713233947753906\n",
      "Batch :  80\n",
      "Loss: 6.494988441467285\n",
      "Batch :  81\n",
      "Loss: 6.6982269287109375\n",
      "Batch :  82\n",
      "Loss: 6.507674694061279\n",
      "Batch :  83\n",
      "Loss: 6.485589981079102\n",
      "Batch :  84\n",
      "Loss: 6.645163536071777\n",
      "Batch :  85\n",
      "Loss: 6.452378273010254\n",
      "Batch :  86\n",
      "Loss: 6.347179412841797\n",
      "Batch :  87\n",
      "Loss: 6.227421283721924\n",
      "Batch :  88\n",
      "Loss: 6.225680351257324\n",
      "Batch :  89\n",
      "Loss: 6.432119846343994\n",
      "Batch :  90\n",
      "Loss: 6.548434734344482\n",
      "Batch :  91\n",
      "Loss: 6.467222213745117\n",
      "Batch :  92\n",
      "Loss: 6.208591938018799\n",
      "Batch :  93\n",
      "Loss: 6.41934061050415\n",
      "Batch :  94\n",
      "Loss: 6.499780178070068\n",
      "Batch :  95\n",
      "Loss: 6.437028408050537\n",
      "Batch :  96\n",
      "Loss: 6.105442523956299\n",
      "Batch :  97\n",
      "Loss: 6.388454437255859\n",
      "Batch :  98\n",
      "Loss: 6.358791828155518\n",
      "Batch :  99\n",
      "Loss: 6.514045715332031\n",
      "Batch :  100\n",
      "Loss: 6.653159141540527\n",
      "Batch :  101\n",
      "Loss: 6.584384441375732\n",
      "Batch :  102\n",
      "Loss: 6.304673194885254\n",
      "Batch :  103\n",
      "Loss: 6.436124801635742\n",
      "Batch :  104\n",
      "Loss: 6.333746433258057\n",
      "Batch :  105\n",
      "Loss: 6.216919898986816\n",
      "Batch :  106\n",
      "Loss: 6.355562210083008\n",
      "Batch :  107\n",
      "Loss: 6.210361957550049\n",
      "Batch :  108\n",
      "Loss: 6.136919021606445\n",
      "Batch :  109\n",
      "Loss: 6.102630138397217\n",
      "Batch :  110\n",
      "Loss: 6.2388596534729\n",
      "Batch :  111\n",
      "Loss: 5.847072124481201\n",
      "Batch :  112\n",
      "Loss: 6.25417423248291\n",
      "Batch :  113\n",
      "Loss: 5.87287712097168\n",
      "Batch :  114\n",
      "Loss: 6.317566394805908\n",
      "Batch :  115\n",
      "Loss: 6.5292277336120605\n",
      "Batch :  116\n",
      "Loss: 6.392446517944336\n",
      "Batch :  117\n",
      "Loss: 6.12333869934082\n",
      "Batch :  118\n",
      "Loss: 6.187873840332031\n",
      "Batch :  119\n",
      "Loss: 6.16103982925415\n",
      "Batch :  120\n",
      "Loss: 6.16923713684082\n",
      "Batch :  121\n",
      "Loss: 6.103126049041748\n",
      "Batch :  122\n",
      "Loss: 6.1855621337890625\n",
      "Batch :  123\n",
      "Loss: 5.942284107208252\n",
      "Batch :  124\n",
      "Loss: 6.225017547607422\n",
      "Batch :  125\n",
      "Loss: 6.255126953125\n",
      "Batch :  126\n",
      "Loss: 5.776859283447266\n",
      "Batch :  127\n",
      "Loss: 6.283380031585693\n",
      "Batch :  128\n",
      "Loss: 5.849086284637451\n",
      "Batch :  129\n",
      "Loss: 6.055947303771973\n",
      "Batch :  130\n",
      "Loss: 5.96449089050293\n",
      "Batch :  131\n",
      "Loss: 6.076654434204102\n",
      "Batch :  132\n",
      "Loss: 6.121949195861816\n",
      "Batch :  133\n",
      "Loss: 6.209202766418457\n",
      "Batch :  134\n",
      "Loss: 6.1324896812438965\n",
      "Batch :  135\n",
      "Loss: 6.134446144104004\n",
      "Batch :  136\n",
      "Loss: 5.929673194885254\n",
      "Batch :  137\n",
      "Loss: 6.206762313842773\n",
      "Batch :  138\n",
      "Loss: 6.097513198852539\n",
      "Batch :  139\n",
      "Loss: 5.600403785705566\n",
      "Batch :  140\n",
      "Loss: 5.79265832901001\n",
      "Batch :  141\n",
      "Loss: 6.099441051483154\n",
      "Batch :  142\n",
      "Loss: 6.1187663078308105\n",
      "Batch :  143\n",
      "Loss: 5.97612190246582\n",
      "Batch :  144\n",
      "Loss: 6.086668014526367\n",
      "Batch :  145\n",
      "Loss: 6.007387161254883\n",
      "Batch :  146\n",
      "Loss: 6.0813212394714355\n",
      "Batch :  147\n",
      "Loss: 6.046258926391602\n",
      "Batch :  148\n",
      "Loss: 6.069808006286621\n",
      "Batch :  149\n",
      "Loss: 6.18306303024292\n",
      "Batch :  150\n",
      "Loss: 5.757754325866699\n",
      "Batch :  151\n",
      "Loss: 6.059474945068359\n",
      "Batch :  152\n",
      "Loss: 5.8544745445251465\n",
      "Batch :  153\n",
      "Loss: 6.021271705627441\n",
      "Batch :  154\n",
      "Loss: 5.99172306060791\n",
      "Batch :  155\n",
      "Loss: 6.1289544105529785\n",
      "Batch :  156\n",
      "Loss: 6.048989295959473\n",
      "Batch :  157\n",
      "Loss: 6.174924850463867\n",
      "Batch :  158\n",
      "Loss: 5.673252105712891\n",
      "Batch :  159\n",
      "Loss: 6.238860607147217\n",
      "Batch :  160\n",
      "Loss: 5.886926651000977\n",
      "Batch :  161\n",
      "Loss: 6.132221221923828\n",
      "Batch :  162\n",
      "Loss: 5.853135108947754\n",
      "Batch :  163\n",
      "Loss: 5.504827976226807\n",
      "Batch :  164\n",
      "Loss: 5.761923313140869\n",
      "Batch :  165\n",
      "Loss: 5.495330333709717\n",
      "Batch :  166\n",
      "Loss: 5.993386268615723\n",
      "Batch :  167\n",
      "Loss: 5.83849573135376\n",
      "Batch :  168\n",
      "Loss: 6.235774517059326\n",
      "Batch :  169\n",
      "Loss: 5.84537935256958\n",
      "Batch :  170\n",
      "Loss: 5.7946600914001465\n",
      "Batch :  171\n",
      "Loss: 5.967813491821289\n",
      "Batch :  172\n",
      "Loss: 6.020828723907471\n",
      "Batch :  173\n",
      "Loss: 6.096433639526367\n",
      "Batch :  174\n",
      "Loss: 5.9284892082214355\n",
      "Batch :  175\n",
      "Loss: 5.8130903244018555\n",
      "Batch :  176\n",
      "Loss: 5.835381507873535\n",
      "Batch :  177\n",
      "Loss: 5.5790486335754395\n",
      "Batch :  178\n",
      "Loss: 5.688179969787598\n",
      "Batch :  179\n",
      "Loss: 5.821721076965332\n",
      "Batch :  180\n",
      "Loss: 5.662158012390137\n",
      "Batch :  181\n",
      "Loss: 5.535000324249268\n",
      "Batch :  182\n",
      "Loss: 5.988818168640137\n",
      "Batch :  183\n",
      "Loss: 5.868565082550049\n",
      "Batch :  184\n",
      "Loss: 5.805323600769043\n",
      "Batch :  185\n",
      "Loss: 5.9242730140686035\n",
      "Batch :  186\n",
      "Loss: 5.265069961547852\n",
      "Batch :  187\n",
      "Loss: 5.7899675369262695\n",
      "Batch :  188\n",
      "Loss: 5.506190776824951\n",
      "Batch :  189\n",
      "Loss: 5.571870803833008\n",
      "Batch :  190\n",
      "Loss: 5.501938819885254\n",
      "Batch :  191\n",
      "Loss: 5.879548072814941\n",
      "Batch :  192\n",
      "Loss: 5.820401668548584\n",
      "Batch :  193\n",
      "Loss: 5.666835784912109\n",
      "Batch :  194\n",
      "Loss: 5.967108249664307\n",
      "Batch :  195\n",
      "Loss: 5.693342208862305\n",
      "Batch :  196\n",
      "Loss: 5.924465179443359\n",
      "Batch :  197\n",
      "Loss: 5.5923261642456055\n",
      "Batch :  198\n",
      "Loss: 5.470516681671143\n",
      "Batch :  199\n",
      "Loss: 5.52775764465332\n",
      "Batch :  200\n",
      "Loss: 5.844700336456299\n",
      "Batch :  201\n",
      "Loss: 5.505141258239746\n",
      "Batch :  202\n",
      "Loss: 5.701628684997559\n",
      "Batch :  203\n",
      "Loss: 5.488828659057617\n",
      "Batch :  204\n",
      "Loss: 5.58370304107666\n",
      "Batch :  205\n",
      "Loss: 5.539983749389648\n",
      "Batch :  206\n",
      "Loss: 5.474514484405518\n",
      "Batch :  207\n",
      "Loss: 5.734378814697266\n",
      "Batch :  208\n",
      "Loss: 5.684732437133789\n",
      "Batch :  209\n",
      "Loss: 5.687015056610107\n",
      "Batch :  210\n",
      "Loss: 5.945353984832764\n",
      "Batch :  211\n",
      "Loss: 5.901103973388672\n",
      "Batch :  212\n",
      "Loss: 5.738202095031738\n",
      "Batch :  213\n",
      "Loss: 5.720957279205322\n",
      "Batch :  214\n",
      "Loss: 5.8992743492126465\n",
      "Batch :  215\n",
      "Loss: 5.587348937988281\n",
      "Batch :  216\n",
      "Loss: 5.782278060913086\n",
      "Batch :  217\n",
      "Loss: 5.3521037101745605\n",
      "Batch :  218\n",
      "Loss: 5.484219551086426\n",
      "Batch :  219\n",
      "Loss: 5.72473669052124\n",
      "Batch :  220\n",
      "Loss: 5.685472011566162\n",
      "Batch :  221\n",
      "Loss: 5.484428882598877\n",
      "Batch :  222\n",
      "Loss: 5.409432888031006\n",
      "Batch :  223\n",
      "Loss: 5.491106986999512\n",
      "Batch :  224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 5.897654056549072\n",
      "Batch :  225\n",
      "Loss: 5.496918201446533\n",
      "Batch :  226\n",
      "Loss: 5.707451343536377\n",
      "Batch :  227\n",
      "Loss: 5.405379295349121\n",
      "Batch :  228\n",
      "Loss: 6.04636812210083\n",
      "Batch :  229\n",
      "Loss: 5.571234703063965\n",
      "Batch :  230\n",
      "Loss: 5.180836200714111\n",
      "Batch :  231\n",
      "Loss: 5.627233982086182\n",
      "Batch :  232\n",
      "Loss: 5.380681991577148\n",
      "Batch :  233\n",
      "Loss: 5.498132705688477\n",
      "Batch :  234\n",
      "Loss: 5.387778282165527\n",
      "Batch :  235\n",
      "Loss: 5.546021938323975\n",
      "Batch :  236\n",
      "Loss: 5.409597873687744\n",
      "Batch :  237\n",
      "Loss: 5.592176914215088\n",
      "Batch :  238\n",
      "Loss: 5.788273811340332\n",
      "Batch :  239\n",
      "Loss: 5.996137619018555\n",
      "Batch :  240\n",
      "Loss: 5.498562335968018\n",
      "Batch :  241\n",
      "Loss: 5.577475070953369\n",
      "Batch :  242\n",
      "Loss: 5.909254550933838\n",
      "Batch :  243\n",
      "Loss: 5.778346538543701\n",
      "Batch :  244\n",
      "Loss: 5.520327568054199\n",
      "Batch :  245\n",
      "Loss: 5.594327926635742\n",
      "Batch :  246\n",
      "Loss: 5.549367427825928\n",
      "Batch :  247\n",
      "Loss: 5.706071376800537\n",
      "Batch :  248\n",
      "Loss: 5.3867597579956055\n",
      "Batch :  249\n",
      "Loss: 5.561187267303467\n",
      "Batch :  250\n",
      "Loss: 5.372147560119629\n",
      "Batch :  251\n",
      "Loss: 5.466678142547607\n",
      "Batch :  252\n",
      "Loss: 5.622200012207031\n",
      "Batch :  253\n",
      "Loss: 5.383878231048584\n",
      "Batch :  254\n",
      "Loss: 5.50624418258667\n",
      "Batch :  255\n",
      "Loss: 5.304708480834961\n",
      "Batch :  256\n",
      "Loss: 5.430380821228027\n",
      "Batch :  257\n",
      "Loss: 5.435140132904053\n",
      "Batch :  258\n",
      "Loss: 5.52447509765625\n",
      "Batch :  259\n",
      "Loss: 5.372094631195068\n",
      "Batch :  260\n",
      "Loss: 5.434965133666992\n",
      "Batch :  261\n",
      "Loss: 5.258844375610352\n",
      "Batch :  262\n",
      "Loss: 5.502110481262207\n",
      "Batch :  263\n",
      "Loss: 5.70362663269043\n",
      "Batch :  264\n",
      "Loss: 5.61159610748291\n",
      "Batch :  265\n",
      "Loss: 5.377020359039307\n",
      "Batch :  266\n",
      "Loss: 5.807090759277344\n",
      "Batch :  267\n",
      "Loss: 5.552223205566406\n",
      "Batch :  268\n",
      "Loss: 5.46099853515625\n",
      "Batch :  269\n",
      "Loss: 5.748490810394287\n",
      "Batch :  270\n",
      "Loss: 5.58260440826416\n",
      "Batch :  271\n",
      "Loss: 5.426915168762207\n",
      "Batch :  272\n",
      "Loss: 5.405901908874512\n",
      "Batch :  273\n",
      "Loss: 5.230839252471924\n",
      "Batch :  274\n",
      "Loss: 5.5559468269348145\n",
      "Batch :  275\n",
      "Loss: 5.686217308044434\n",
      "Batch :  276\n",
      "Loss: 5.26760721206665\n",
      "Batch :  277\n",
      "Loss: 5.378333568572998\n",
      "Batch :  278\n",
      "Loss: 5.503663539886475\n",
      "Batch :  279\n",
      "Loss: 5.280968189239502\n",
      "Batch :  280\n",
      "Loss: 5.6389079093933105\n",
      "Batch :  281\n",
      "Loss: 5.649935722351074\n",
      "Batch :  282\n",
      "Loss: 5.441517353057861\n",
      "Batch :  283\n",
      "Loss: 5.611281394958496\n",
      "Batch :  284\n",
      "Loss: 5.582343578338623\n",
      "Batch :  285\n",
      "Loss: 5.915818214416504\n",
      "Batch :  286\n",
      "Loss: 5.627340793609619\n",
      "Batch :  287\n",
      "Loss: 5.369387149810791\n",
      "Batch :  288\n",
      "Loss: 5.501585483551025\n",
      "Batch :  289\n",
      "Loss: 5.41303014755249\n",
      "Batch :  290\n",
      "Loss: 5.19090461730957\n",
      "Batch :  291\n",
      "Loss: 5.511533260345459\n",
      "Batch :  292\n",
      "Loss: 5.551576614379883\n",
      "Batch :  293\n",
      "Loss: 5.276124000549316\n",
      "Batch :  294\n",
      "Loss: 5.6383819580078125\n",
      "Batch :  295\n",
      "Loss: 5.547906875610352\n",
      "Batch :  296\n",
      "Loss: 5.1446614265441895\n",
      "Batch :  297\n",
      "Loss: 5.242484092712402\n",
      "Batch :  298\n",
      "Loss: 5.278341770172119\n",
      "Batch :  299\n",
      "Loss: 5.283188343048096\n",
      "Batch :  300\n",
      "Loss: 5.530418872833252\n",
      "Batch :  301\n",
      "Loss: 5.591508865356445\n",
      "Batch :  302\n",
      "Loss: 5.301133155822754\n",
      "Batch :  303\n",
      "Loss: 5.2967023849487305\n",
      "Batch :  304\n",
      "Loss: 5.049077987670898\n",
      "Batch :  305\n",
      "Loss: 5.590543270111084\n",
      "Batch :  306\n",
      "Loss: 5.371366500854492\n",
      "Batch :  307\n",
      "Loss: 5.1361565589904785\n",
      "Batch :  308\n",
      "Loss: 5.094342231750488\n",
      "Batch :  309\n",
      "Loss: 5.547950267791748\n",
      "Batch :  310\n",
      "Loss: 5.769814968109131\n",
      "Batch :  311\n",
      "Loss: 5.2610859870910645\n",
      "Batch :  312\n",
      "Loss: 5.516608238220215\n",
      "Batch - Loss: 5.783329780871114\n",
      "Epoch 2\n",
      "Batch :  0\n",
      "Loss: 5.1022491455078125\n",
      "Batch :  1\n",
      "Loss: 5.773752212524414\n",
      "Batch :  2\n",
      "Loss: 5.27094841003418\n",
      "Batch :  3\n",
      "Loss: 4.77316427230835\n",
      "Batch :  4\n",
      "Loss: 5.054718971252441\n",
      "Batch :  5\n",
      "Loss: 4.861392498016357\n",
      "Batch :  6\n",
      "Loss: 4.822479724884033\n",
      "Batch :  7\n",
      "Loss: 4.691077709197998\n",
      "Batch :  8\n",
      "Loss: 4.933795928955078\n",
      "Batch :  9\n",
      "Loss: 5.278229713439941\n",
      "Batch :  10\n",
      "Loss: 5.112819194793701\n",
      "Batch :  11\n",
      "Loss: 5.107740879058838\n",
      "Batch :  12\n",
      "Loss: 5.378488540649414\n",
      "Batch :  13\n",
      "Loss: 5.158445835113525\n",
      "Batch :  14\n",
      "Loss: 5.247450351715088\n",
      "Batch :  15\n",
      "Loss: 5.368375301361084\n",
      "Batch :  16\n",
      "Loss: 5.29521369934082\n",
      "Batch :  17\n",
      "Loss: 5.108199119567871\n",
      "Batch :  18\n",
      "Loss: 5.215022563934326\n",
      "Batch :  19\n",
      "Loss: 5.183362007141113\n",
      "Batch :  20\n",
      "Loss: 5.175822734832764\n",
      "Batch :  21\n",
      "Loss: 4.7974748611450195\n",
      "Batch :  22\n",
      "Loss: 5.227512359619141\n",
      "Batch :  23\n",
      "Loss: 5.282137393951416\n",
      "Batch :  24\n",
      "Loss: 4.938675880432129\n",
      "Batch :  25\n",
      "Loss: 4.867824554443359\n",
      "Batch :  26\n",
      "Loss: 4.9508466720581055\n",
      "Batch :  27\n",
      "Loss: 5.035403728485107\n",
      "Batch :  28\n",
      "Loss: 5.05207633972168\n",
      "Batch :  29\n",
      "Loss: 5.062480926513672\n",
      "Batch :  30\n",
      "Loss: 5.321966648101807\n",
      "Batch :  31\n",
      "Loss: 5.046665191650391\n",
      "Batch :  32\n",
      "Loss: 4.868659019470215\n",
      "Batch :  33\n",
      "Loss: 4.754397869110107\n",
      "Batch :  34\n",
      "Loss: 4.935139179229736\n",
      "Batch :  35\n",
      "Loss: 5.0447845458984375\n",
      "Batch :  36\n",
      "Loss: 5.098332405090332\n",
      "Batch :  37\n",
      "Loss: 5.148855686187744\n",
      "Batch :  38\n",
      "Loss: 5.06960391998291\n",
      "Batch :  39\n",
      "Loss: 4.959419250488281\n",
      "Batch :  40\n",
      "Loss: 4.726032733917236\n",
      "Batch :  41\n",
      "Loss: 5.054042339324951\n",
      "Batch :  42\n",
      "Loss: 5.209566116333008\n",
      "Batch :  43\n",
      "Loss: 4.745242595672607\n",
      "Batch :  44\n",
      "Loss: 4.699244022369385\n",
      "Batch :  45\n",
      "Loss: 4.847895622253418\n",
      "Batch :  46\n",
      "Loss: 5.060946464538574\n",
      "Batch :  47\n",
      "Loss: 4.837436199188232\n",
      "Batch :  48\n",
      "Loss: 4.965691566467285\n",
      "Batch :  49\n",
      "Loss: 4.967141151428223\n",
      "Batch :  50\n",
      "Loss: 5.094111919403076\n",
      "Batch :  51\n",
      "Loss: 4.923089504241943\n",
      "Batch :  52\n",
      "Loss: 4.81891393661499\n",
      "Batch :  53\n",
      "Loss: 4.962721347808838\n",
      "Batch :  54\n",
      "Loss: 4.963449478149414\n",
      "Batch :  55\n",
      "Loss: 4.818146228790283\n",
      "Batch :  56\n",
      "Loss: 5.031348705291748\n",
      "Batch :  57\n",
      "Loss: 5.024905204772949\n",
      "Batch :  58\n",
      "Loss: 5.07761287689209\n",
      "Batch :  59\n",
      "Loss: 4.793374538421631\n",
      "Batch :  60\n",
      "Loss: 4.904872894287109\n",
      "Batch :  61\n",
      "Loss: 4.914411544799805\n",
      "Batch :  62\n",
      "Loss: 4.985890865325928\n",
      "Batch :  63\n",
      "Loss: 5.217416286468506\n",
      "Batch :  64\n",
      "Loss: 4.903913497924805\n",
      "Batch :  65\n",
      "Loss: 4.881156921386719\n",
      "Batch :  66\n",
      "Loss: 4.97055721282959\n",
      "Batch :  67\n",
      "Loss: 4.657893657684326\n",
      "Batch :  68\n",
      "Loss: 4.714026927947998\n",
      "Batch :  69\n",
      "Loss: 4.7239990234375\n",
      "Batch :  70\n",
      "Loss: 4.787700653076172\n",
      "Batch :  71\n",
      "Loss: 4.625875949859619\n",
      "Batch :  72\n",
      "Loss: 4.616231918334961\n",
      "Batch :  73\n",
      "Loss: 4.763001918792725\n",
      "Batch :  74\n",
      "Loss: 4.754763603210449\n",
      "Batch :  75\n",
      "Loss: 5.015296936035156\n",
      "Batch :  76\n",
      "Loss: 4.4993672370910645\n",
      "Batch :  77\n",
      "Loss: 4.788334846496582\n",
      "Batch :  78\n",
      "Loss: 4.889960289001465\n",
      "Batch :  79\n",
      "Loss: 4.881629467010498\n",
      "Batch :  80\n",
      "Loss: 4.712839126586914\n",
      "Batch :  81\n",
      "Loss: 4.9817962646484375\n",
      "Batch :  82\n",
      "Loss: 4.767029762268066\n",
      "Batch :  83\n",
      "Loss: 4.8382248878479\n",
      "Batch :  84\n",
      "Loss: 4.956902027130127\n",
      "Batch :  85\n",
      "Loss: 4.761216640472412\n",
      "Batch :  86\n",
      "Loss: 4.615240097045898\n",
      "Batch :  87\n",
      "Loss: 4.510724067687988\n",
      "Batch :  88\n",
      "Loss: 4.604581832885742\n",
      "Batch :  89\n",
      "Loss: 4.732494831085205\n",
      "Batch :  90\n",
      "Loss: 4.7552361488342285\n",
      "Batch :  91\n",
      "Loss: 4.798222541809082\n",
      "Batch :  92\n",
      "Loss: 4.595059871673584\n",
      "Batch :  93\n",
      "Loss: 4.6919145584106445\n",
      "Batch :  94\n",
      "Loss: 4.838321208953857\n",
      "Batch :  95\n",
      "Loss: 4.991851329803467\n",
      "Batch :  96\n",
      "Loss: 4.418632507324219\n",
      "Batch :  97\n",
      "Loss: 4.843447685241699\n",
      "Batch :  98\n",
      "Loss: 4.839531898498535\n",
      "Batch :  99\n",
      "Loss: 4.974516868591309\n",
      "Batch :  100\n",
      "Loss: 4.972480773925781\n",
      "Batch :  101\n",
      "Loss: 5.126676559448242\n",
      "Batch :  102\n",
      "Loss: 4.896264553070068\n",
      "Batch :  103\n",
      "Loss: 4.911559581756592\n",
      "Batch :  104\n",
      "Loss: 4.784759998321533\n",
      "Batch :  105\n",
      "Loss: 4.786258220672607\n",
      "Batch :  106\n",
      "Loss: 5.000683784484863\n",
      "Batch :  107\n",
      "Loss: 4.9081220626831055\n",
      "Batch :  108\n",
      "Loss: 4.700939178466797\n",
      "Batch :  109\n",
      "Loss: 4.678529739379883\n",
      "Batch :  110\n",
      "Loss: 4.746781349182129\n",
      "Batch :  111\n",
      "Loss: 4.521762371063232\n",
      "Batch :  112\n",
      "Loss: 5.000517845153809\n",
      "Batch :  113\n",
      "Loss: 4.481480121612549\n",
      "Batch :  114\n",
      "Loss: 4.928074836730957\n",
      "Batch :  115\n",
      "Loss: 5.086463451385498\n",
      "Batch :  116\n",
      "Loss: 4.9694929122924805\n",
      "Batch :  117\n",
      "Loss: 4.797807693481445\n",
      "Batch :  118\n",
      "Loss: 4.8223748207092285\n",
      "Batch :  119\n",
      "Loss: 4.528595924377441\n",
      "Batch :  120\n",
      "Loss: 4.829483985900879\n",
      "Batch :  121\n",
      "Loss: 4.795185565948486\n",
      "Batch :  122\n",
      "Loss: 4.704250812530518\n",
      "Batch :  123\n",
      "Loss: 4.666446685791016\n",
      "Batch :  124\n",
      "Loss: 4.8126935958862305\n",
      "Batch :  125\n",
      "Loss: 4.786798000335693\n",
      "Batch :  126\n",
      "Loss: 4.50903844833374\n",
      "Batch :  127\n",
      "Loss: 4.953129768371582\n",
      "Batch :  128\n",
      "Loss: 4.47474479675293\n",
      "Batch :  129\n",
      "Loss: 4.6695404052734375\n",
      "Batch :  130\n",
      "Loss: 4.567784309387207\n",
      "Batch :  131\n",
      "Loss: 4.669028282165527\n",
      "Batch :  132\n",
      "Loss: 4.74185037612915\n",
      "Batch :  133\n",
      "Loss: 4.822277069091797\n",
      "Batch :  134\n",
      "Loss: 4.734471797943115\n",
      "Batch :  135\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.66600227355957\n",
      "Batch :  136\n",
      "Loss: 4.527903079986572\n",
      "Batch :  137\n",
      "Loss: 4.768033027648926\n",
      "Batch :  138\n",
      "Loss: 4.694746971130371\n",
      "Batch :  139\n",
      "Loss: 4.284581661224365\n",
      "Batch :  140\n",
      "Loss: 4.354745864868164\n",
      "Batch :  141\n",
      "Loss: 4.774101257324219\n",
      "Batch :  142\n",
      "Loss: 4.704472064971924\n",
      "Batch :  143\n",
      "Loss: 4.666135787963867\n",
      "Batch :  144\n",
      "Loss: 4.800276756286621\n",
      "Batch :  145\n",
      "Loss: 4.565735816955566\n",
      "Batch :  146\n",
      "Loss: 4.502488613128662\n",
      "Batch :  147\n",
      "Loss: 4.747727870941162\n",
      "Batch :  148\n",
      "Loss: 4.6367597579956055\n",
      "Batch :  149\n",
      "Loss: 4.75193977355957\n",
      "Batch :  150\n",
      "Loss: 4.375037670135498\n",
      "Batch :  151\n",
      "Loss: 4.762884140014648\n",
      "Batch :  152\n",
      "Loss: 4.567432403564453\n",
      "Batch :  153\n",
      "Loss: 4.617840766906738\n",
      "Batch :  154\n",
      "Loss: 4.5652971267700195\n",
      "Batch :  155\n",
      "Loss: 4.633358001708984\n",
      "Batch :  156\n",
      "Loss: 4.609301567077637\n",
      "Batch :  157\n",
      "Loss: 4.770990371704102\n",
      "Batch :  158\n",
      "Loss: 4.3858723640441895\n",
      "Batch :  159\n",
      "Loss: 4.749992847442627\n",
      "Batch :  160\n",
      "Loss: 4.617266654968262\n",
      "Batch :  161\n",
      "Loss: 4.722578525543213\n",
      "Batch :  162\n",
      "Loss: 4.440309047698975\n",
      "Batch :  163\n",
      "Loss: 4.145020008087158\n",
      "Batch :  164\n",
      "Loss: 4.269284248352051\n",
      "Batch :  165\n",
      "Loss: 4.313977241516113\n",
      "Batch :  166\n",
      "Loss: 4.57144021987915\n",
      "Batch :  167\n",
      "Loss: 4.484323978424072\n",
      "Batch :  168\n",
      "Loss: 4.738142013549805\n",
      "Batch :  169\n",
      "Loss: 4.5780134201049805\n",
      "Batch :  170\n",
      "Loss: 4.587634563446045\n",
      "Batch :  171\n",
      "Loss: 4.522449016571045\n",
      "Batch :  172\n",
      "Loss: 4.487649917602539\n",
      "Batch :  173\n",
      "Loss: 4.793398857116699\n",
      "Batch :  174\n",
      "Loss: 4.502161502838135\n",
      "Batch :  175\n",
      "Loss: 4.416040420532227\n",
      "Batch :  176\n",
      "Loss: 4.344313144683838\n",
      "Batch :  177\n",
      "Loss: 4.19396448135376\n",
      "Batch :  178\n",
      "Loss: 4.455525875091553\n",
      "Batch :  179\n",
      "Loss: 4.4441447257995605\n",
      "Batch :  180\n",
      "Loss: 4.292978286743164\n",
      "Batch :  181\n",
      "Loss: 4.357341289520264\n",
      "Batch :  182\n",
      "Loss: 4.669885635375977\n",
      "Batch :  183\n",
      "Loss: 4.468233585357666\n",
      "Batch :  184\n",
      "Loss: 4.548061847686768\n",
      "Batch :  185\n",
      "Loss: 4.707371711730957\n",
      "Batch :  186\n",
      "Loss: 3.916454792022705\n",
      "Batch :  187\n",
      "Loss: 4.560041904449463\n",
      "Batch :  188\n",
      "Loss: 4.1930694580078125\n",
      "Batch :  189\n",
      "Loss: 4.161319255828857\n",
      "Batch :  190\n",
      "Loss: 4.0964741706848145\n",
      "Batch :  191\n",
      "Loss: 4.360520362854004\n",
      "Batch :  192\n",
      "Loss: 4.500954627990723\n",
      "Batch :  193\n",
      "Loss: 4.351944446563721\n",
      "Batch :  194\n",
      "Loss: 4.561047554016113\n",
      "Batch :  195\n",
      "Loss: 4.325354099273682\n",
      "Batch :  196\n",
      "Loss: 4.546509265899658\n",
      "Batch :  197\n",
      "Loss: 4.360156059265137\n",
      "Batch :  198\n",
      "Loss: 4.13565731048584\n",
      "Batch :  199\n",
      "Loss: 4.306905269622803\n",
      "Batch :  200\n",
      "Loss: 4.284265995025635\n",
      "Batch :  201\n",
      "Loss: 4.236023426055908\n",
      "Batch :  202\n",
      "Loss: 4.439524173736572\n",
      "Batch :  203\n",
      "Loss: 4.235121726989746\n",
      "Batch :  204\n",
      "Loss: 4.1312971115112305\n",
      "Batch :  205\n",
      "Loss: 4.285295009613037\n",
      "Batch :  206\n",
      "Loss: 4.233755588531494\n",
      "Batch :  207\n",
      "Loss: 4.393083572387695\n",
      "Batch :  208\n",
      "Loss: 4.425614356994629\n",
      "Batch :  209\n",
      "Loss: 4.324758529663086\n",
      "Batch :  210\n",
      "Loss: 4.584508419036865\n",
      "Batch :  211\n",
      "Loss: 4.313319206237793\n",
      "Batch :  212\n",
      "Loss: 4.4119744300842285\n",
      "Batch :  213\n",
      "Loss: 4.425878047943115\n",
      "Batch :  214\n",
      "Loss: 4.5223388671875\n",
      "Batch :  215\n",
      "Loss: 4.280524730682373\n",
      "Batch :  216\n",
      "Loss: 4.427032470703125\n",
      "Batch :  217\n",
      "Loss: 4.130939960479736\n",
      "Batch :  218\n",
      "Loss: 4.2771077156066895\n",
      "Batch :  219\n",
      "Loss: 4.279351234436035\n",
      "Batch :  220\n",
      "Loss: 4.349026203155518\n",
      "Batch :  221\n",
      "Loss: 4.303313732147217\n",
      "Batch :  222\n",
      "Loss: 4.115766525268555\n",
      "Batch :  223\n",
      "Loss: 4.179029941558838\n",
      "Batch :  224\n",
      "Loss: 4.52321720123291\n",
      "Batch :  225\n",
      "Loss: 4.22936487197876\n",
      "Batch :  226\n",
      "Loss: 4.25322961807251\n",
      "Batch :  227\n",
      "Loss: 4.101550102233887\n",
      "Batch :  228\n",
      "Loss: 4.744076251983643\n",
      "Batch :  229\n",
      "Loss: 4.299459934234619\n",
      "Batch :  230\n",
      "Loss: 3.906297206878662\n",
      "Batch :  231\n",
      "Loss: 4.221724510192871\n",
      "Batch :  232\n",
      "Loss: 4.003885269165039\n",
      "Batch :  233\n",
      "Loss: 4.228018760681152\n",
      "Batch :  234\n",
      "Loss: 4.13950252532959\n",
      "Batch :  235\n",
      "Loss: 4.373671531677246\n",
      "Batch :  236\n",
      "Loss: 4.299671173095703\n",
      "Batch :  237\n",
      "Loss: 4.419466972351074\n",
      "Batch :  238\n",
      "Loss: 4.49473762512207\n",
      "Batch :  239\n",
      "Loss: 4.510636329650879\n",
      "Batch :  240\n",
      "Loss: 4.173959255218506\n",
      "Batch :  241\n",
      "Loss: 4.277601718902588\n",
      "Batch :  242\n",
      "Loss: 4.615653991699219\n",
      "Batch :  243\n",
      "Loss: 4.438945770263672\n",
      "Batch :  244\n",
      "Loss: 4.309299468994141\n",
      "Batch :  245\n",
      "Loss: 4.335353851318359\n",
      "Batch :  246\n",
      "Loss: 4.372696399688721\n",
      "Batch :  247\n",
      "Loss: 4.491764545440674\n",
      "Batch :  248\n",
      "Loss: 4.238072872161865\n",
      "Batch :  249\n",
      "Loss: 4.239006042480469\n",
      "Batch :  250\n",
      "Loss: 4.132175445556641\n",
      "Batch :  251\n",
      "Loss: 4.1788649559021\n",
      "Batch :  252\n",
      "Loss: 4.256282806396484\n",
      "Batch :  253\n",
      "Loss: 4.193639755249023\n",
      "Batch :  254\n",
      "Loss: 4.205750942230225\n",
      "Batch :  255\n",
      "Loss: 4.074004650115967\n",
      "Batch :  256\n",
      "Loss: 4.197801113128662\n",
      "Batch :  257\n",
      "Loss: 4.131531715393066\n",
      "Batch :  258\n",
      "Loss: 4.397365093231201\n",
      "Batch :  259\n",
      "Loss: 4.149960041046143\n",
      "Batch :  260\n",
      "Loss: 4.216573715209961\n",
      "Batch :  261\n",
      "Loss: 4.11018705368042\n",
      "Batch :  262\n",
      "Loss: 4.31847620010376\n",
      "Batch :  263\n",
      "Loss: 4.483647346496582\n",
      "Batch :  264\n",
      "Loss: 4.217834949493408\n",
      "Batch :  265\n",
      "Loss: 4.072720527648926\n",
      "Batch :  266\n",
      "Loss: 4.288844108581543\n",
      "Batch :  267\n",
      "Loss: 4.25842809677124\n",
      "Batch :  268\n",
      "Loss: 4.116779327392578\n",
      "Batch :  269\n",
      "Loss: 4.288234710693359\n",
      "Batch :  270\n",
      "Loss: 4.250827789306641\n",
      "Batch :  271\n",
      "Loss: 4.287316799163818\n",
      "Batch :  272\n",
      "Loss: 4.0619096755981445\n",
      "Batch :  273\n",
      "Loss: 4.118284225463867\n",
      "Batch :  274\n",
      "Loss: 4.427668571472168\n",
      "Batch :  275\n",
      "Loss: 4.263160228729248\n",
      "Batch :  276\n",
      "Loss: 4.113064765930176\n",
      "Batch :  277\n",
      "Loss: 4.357601642608643\n",
      "Batch :  278\n",
      "Loss: 4.286777496337891\n",
      "Batch :  279\n",
      "Loss: 4.029312610626221\n",
      "Batch :  280\n",
      "Loss: 4.249460220336914\n",
      "Batch :  281\n",
      "Loss: 4.420180320739746\n",
      "Batch :  282\n",
      "Loss: 4.1173787117004395\n",
      "Batch :  283\n",
      "Loss: 4.342586040496826\n",
      "Batch :  284\n",
      "Loss: 4.270656585693359\n",
      "Batch :  285\n",
      "Loss: 4.6179280281066895\n",
      "Batch :  286\n",
      "Loss: 4.298959255218506\n",
      "Batch :  287\n",
      "Loss: 4.020367622375488\n",
      "Batch :  288\n",
      "Loss: 4.194092273712158\n",
      "Batch :  289\n",
      "Loss: 4.287880897521973\n",
      "Batch :  290\n",
      "Loss: 3.9235990047454834\n",
      "Batch :  291\n",
      "Loss: 4.1801910400390625\n",
      "Batch :  292\n",
      "Loss: 4.208534240722656\n",
      "Batch :  293\n",
      "Loss: 4.028923034667969\n",
      "Batch :  294\n",
      "Loss: 4.374405384063721\n",
      "Batch :  295\n",
      "Loss: 4.278101921081543\n",
      "Batch :  296\n",
      "Loss: 4.107392311096191\n",
      "Batch :  297\n",
      "Loss: 4.06116247177124\n",
      "Batch :  298\n",
      "Loss: 4.063323974609375\n",
      "Batch :  299\n",
      "Loss: 4.012519359588623\n",
      "Batch :  300\n",
      "Loss: 4.3219990730285645\n",
      "Batch :  301\n",
      "Loss: 4.244694232940674\n",
      "Batch :  302\n",
      "Loss: 3.9707586765289307\n",
      "Batch :  303\n",
      "Loss: 4.184483528137207\n",
      "Batch :  304\n",
      "Loss: 3.9774417877197266\n",
      "Batch :  305\n",
      "Loss: 4.2326979637146\n",
      "Batch :  306\n",
      "Loss: 4.122898578643799\n",
      "Batch :  307\n",
      "Loss: 3.8922553062438965\n",
      "Batch :  308\n",
      "Loss: 3.8571033477783203\n",
      "Batch :  309\n",
      "Loss: 4.270041465759277\n",
      "Batch :  310\n",
      "Loss: 4.3947272300720215\n",
      "Batch :  311\n",
      "Loss: 4.0908050537109375\n",
      "Batch :  312\n",
      "Loss: 3.7286436557769775\n",
      "Batch - Loss: 4.272966401645551\n",
      "Epoch 3\n",
      "Batch :  0\n",
      "Loss: 4.0831451416015625\n",
      "Batch :  1\n",
      "Loss: 4.544839859008789\n",
      "Batch :  2\n",
      "Loss: 4.215366363525391\n",
      "Batch :  3\n",
      "Loss: 3.7436773777008057\n",
      "Batch :  4\n",
      "Loss: 4.172369956970215\n",
      "Batch :  5\n",
      "Loss: 4.1109299659729\n",
      "Batch :  6\n",
      "Loss: 3.9969470500946045\n",
      "Batch :  7\n",
      "Loss: 3.9683570861816406\n",
      "Batch :  8\n",
      "Loss: 4.150571823120117\n",
      "Batch :  9\n",
      "Loss: 4.388520240783691\n",
      "Batch :  10\n",
      "Loss: 4.227285385131836\n",
      "Batch :  11\n",
      "Loss: 4.01107931137085\n",
      "Batch :  12\n",
      "Loss: 4.354336738586426\n",
      "Batch :  13\n",
      "Loss: 4.181114196777344\n",
      "Batch :  14\n",
      "Loss: 4.17749547958374\n",
      "Batch :  15\n",
      "Loss: 4.315980434417725\n",
      "Batch :  16\n",
      "Loss: 4.406390190124512\n",
      "Batch :  17\n",
      "Loss: 4.196069717407227\n",
      "Batch :  18\n",
      "Loss: 4.317890644073486\n",
      "Batch :  19\n",
      "Loss: 4.3399248123168945\n",
      "Batch :  20\n",
      "Loss: 4.330401420593262\n",
      "Batch :  21\n",
      "Loss: 4.0058674812316895\n",
      "Batch :  22\n",
      "Loss: 4.409781455993652\n",
      "Batch :  23\n",
      "Loss: 4.299535751342773\n",
      "Batch :  24\n",
      "Loss: 4.107189178466797\n",
      "Batch :  25\n",
      "Loss: 4.053134918212891\n",
      "Batch :  26\n",
      "Loss: 4.025815963745117\n",
      "Batch :  27\n",
      "Loss: 4.131799221038818\n",
      "Batch :  28\n",
      "Loss: 4.324960708618164\n",
      "Batch :  29\n",
      "Loss: 4.146684169769287\n",
      "Batch :  30\n",
      "Loss: 4.429533004760742\n",
      "Batch :  31\n",
      "Loss: 4.133511543273926\n",
      "Batch :  32\n",
      "Loss: 4.116248607635498\n",
      "Batch :  33\n",
      "Loss: 3.923511028289795\n",
      "Batch :  34\n",
      "Loss: 4.020000457763672\n",
      "Batch :  35\n",
      "Loss: 4.193246364593506\n",
      "Batch :  36\n",
      "Loss: 4.126373767852783\n",
      "Batch :  37\n",
      "Loss: 4.239588260650635\n",
      "Batch :  38\n",
      "Loss: 4.156618118286133\n",
      "Batch :  39\n",
      "Loss: 4.089085578918457\n",
      "Batch :  40\n",
      "Loss: 3.879432201385498\n",
      "Batch :  41\n",
      "Loss: 4.086564064025879\n",
      "Batch :  42\n",
      "Loss: 4.227739334106445\n",
      "Batch :  43\n",
      "Loss: 3.9953653812408447\n",
      "Batch :  44\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.8739068508148193\n",
      "Batch :  45\n",
      "Loss: 3.946866512298584\n",
      "Batch :  46\n",
      "Loss: 4.132576942443848\n",
      "Batch :  47\n",
      "Loss: 3.9089112281799316\n",
      "Batch :  48\n",
      "Loss: 4.015048027038574\n",
      "Batch :  49\n",
      "Loss: 4.1744208335876465\n",
      "Batch :  50\n",
      "Loss: 4.1930832862854\n",
      "Batch :  51\n",
      "Loss: 4.0706329345703125\n",
      "Batch :  52\n",
      "Loss: 3.9548239707946777\n",
      "Batch :  53\n",
      "Loss: 4.120948314666748\n",
      "Batch :  54\n",
      "Loss: 4.110620498657227\n",
      "Batch :  55\n",
      "Loss: 3.9280152320861816\n",
      "Batch :  56\n",
      "Loss: 4.199787139892578\n",
      "Batch :  57\n",
      "Loss: 4.088376998901367\n",
      "Batch :  58\n",
      "Loss: 4.277540683746338\n",
      "Batch :  59\n",
      "Loss: 3.9810328483581543\n",
      "Batch :  60\n",
      "Loss: 3.867018699645996\n",
      "Batch :  61\n",
      "Loss: 3.96267032623291\n",
      "Batch :  62\n",
      "Loss: 4.001058578491211\n",
      "Batch :  63\n",
      "Loss: 4.170701503753662\n",
      "Batch :  64\n",
      "Loss: 3.9758999347686768\n",
      "Batch :  65\n",
      "Loss: 3.9628829956054688\n",
      "Batch :  66\n",
      "Loss: 3.9290785789489746\n",
      "Batch :  67\n",
      "Loss: 3.7595295906066895\n",
      "Batch :  68\n",
      "Loss: 3.862755298614502\n",
      "Batch :  69\n",
      "Loss: 3.9240424633026123\n",
      "Batch :  70\n",
      "Loss: 3.874692678451538\n",
      "Batch :  71\n",
      "Loss: 3.7461564540863037\n",
      "Batch :  72\n",
      "Loss: 3.8052048683166504\n",
      "Batch :  73\n",
      "Loss: 3.793365001678467\n",
      "Batch :  74\n",
      "Loss: 3.876507520675659\n",
      "Batch :  75\n",
      "Loss: 3.9364070892333984\n",
      "Batch :  76\n",
      "Loss: 3.5503342151641846\n",
      "Batch :  77\n",
      "Loss: 3.673813581466675\n",
      "Batch :  78\n",
      "Loss: 3.858659029006958\n",
      "Batch :  79\n",
      "Loss: 3.8855628967285156\n",
      "Batch :  80\n",
      "Loss: 3.816462993621826\n",
      "Batch :  81\n",
      "Loss: 4.067446708679199\n",
      "Batch :  82\n",
      "Loss: 3.8573873043060303\n",
      "Batch :  83\n",
      "Loss: 3.8328781127929688\n",
      "Batch :  84\n",
      "Loss: 4.000000953674316\n",
      "Batch :  85\n",
      "Loss: 3.8631787300109863\n",
      "Batch :  86\n",
      "Loss: 3.6363139152526855\n",
      "Batch :  87\n",
      "Loss: 3.432283878326416\n",
      "Batch :  88\n",
      "Loss: 3.626866579055786\n",
      "Batch :  89\n",
      "Loss: 3.7696304321289062\n",
      "Batch :  90\n",
      "Loss: 3.811295986175537\n",
      "Batch :  91\n",
      "Loss: 3.9422812461853027\n",
      "Batch :  92\n",
      "Loss: 3.770054817199707\n",
      "Batch :  93\n",
      "Loss: 3.9195592403411865\n",
      "Batch :  94\n",
      "Loss: 3.9359612464904785\n",
      "Batch :  95\n",
      "Loss: 4.33812952041626\n",
      "Batch :  96\n",
      "Loss: 3.5876410007476807\n",
      "Batch :  97\n",
      "Loss: 3.904292345046997\n",
      "Batch :  98\n",
      "Loss: 4.0231146812438965\n",
      "Batch :  99\n",
      "Loss: 4.135242462158203\n",
      "Batch :  100\n",
      "Loss: 4.024596691131592\n",
      "Batch :  101\n",
      "Loss: 4.270612716674805\n",
      "Batch :  102\n",
      "Loss: 3.9883320331573486\n",
      "Batch :  103\n",
      "Loss: 3.8841395378112793\n",
      "Batch :  104\n",
      "Loss: 3.9748740196228027\n",
      "Batch :  105\n",
      "Loss: 3.90167236328125\n",
      "Batch :  106\n",
      "Loss: 4.075847625732422\n",
      "Batch :  107\n",
      "Loss: 3.800736904144287\n",
      "Batch :  108\n",
      "Loss: 3.7511277198791504\n",
      "Batch :  109\n",
      "Loss: 3.721503973007202\n",
      "Batch :  110\n",
      "Loss: 3.798701286315918\n",
      "Batch :  111\n",
      "Loss: 3.6318891048431396\n",
      "Batch :  112\n",
      "Loss: 4.139939308166504\n",
      "Batch :  113\n",
      "Loss: 3.6494362354278564\n",
      "Batch :  114\n",
      "Loss: 3.9096693992614746\n",
      "Batch :  115\n",
      "Loss: 4.172216892242432\n",
      "Batch :  116\n",
      "Loss: 4.022032737731934\n",
      "Batch :  117\n",
      "Loss: 3.9167487621307373\n",
      "Batch :  118\n",
      "Loss: 3.805882215499878\n",
      "Batch :  119\n",
      "Loss: 3.6732089519500732\n",
      "Batch :  120\n",
      "Loss: 3.8858466148376465\n",
      "Batch :  121\n",
      "Loss: 3.9150216579437256\n",
      "Batch :  122\n",
      "Loss: 3.754899024963379\n",
      "Batch :  123\n",
      "Loss: 3.6827290058135986\n",
      "Batch :  124\n",
      "Loss: 3.8372883796691895\n",
      "Batch :  125\n",
      "Loss: 3.792431354522705\n",
      "Batch :  126\n",
      "Loss: 3.668290376663208\n",
      "Batch :  127\n",
      "Loss: 4.013269424438477\n",
      "Batch :  128\n",
      "Loss: 3.667607307434082\n",
      "Batch :  129\n",
      "Loss: 3.9542758464813232\n",
      "Batch :  130\n",
      "Loss: 3.8484580516815186\n",
      "Batch :  131\n",
      "Loss: 3.940556526184082\n",
      "Batch :  132\n",
      "Loss: 3.905245780944824\n",
      "Batch :  133\n",
      "Loss: 3.9586994647979736\n",
      "Batch :  134\n",
      "Loss: 3.8568477630615234\n",
      "Batch :  135\n",
      "Loss: 3.592259168624878\n",
      "Batch :  136\n",
      "Loss: 3.648556709289551\n",
      "Batch :  137\n",
      "Loss: 3.8864800930023193\n",
      "Batch :  138\n",
      "Loss: 3.8716585636138916\n",
      "Batch :  139\n",
      "Loss: 3.47019100189209\n",
      "Batch :  140\n",
      "Loss: 3.4694595336914062\n",
      "Batch :  141\n",
      "Loss: 3.853348731994629\n",
      "Batch :  142\n",
      "Loss: 3.832584857940674\n",
      "Batch :  143\n",
      "Loss: 3.7812447547912598\n",
      "Batch :  144\n",
      "Loss: 3.917901039123535\n",
      "Batch :  145\n",
      "Loss: 3.664599895477295\n",
      "Batch :  146\n",
      "Loss: 3.5933234691619873\n",
      "Batch :  147\n",
      "Loss: 3.85711407661438\n",
      "Batch :  148\n",
      "Loss: 3.7220215797424316\n",
      "Batch :  149\n",
      "Loss: 3.7599363327026367\n",
      "Batch :  150\n",
      "Loss: 3.5730667114257812\n",
      "Batch :  151\n",
      "Loss: 3.8689234256744385\n",
      "Batch :  152\n",
      "Loss: 3.8444013595581055\n",
      "Batch :  153\n",
      "Loss: 3.617227792739868\n",
      "Batch :  154\n",
      "Loss: 3.8402504920959473\n",
      "Batch :  155\n",
      "Loss: 3.7070257663726807\n",
      "Batch :  156\n",
      "Loss: 3.6883974075317383\n",
      "Batch :  157\n",
      "Loss: 3.9699559211730957\n",
      "Batch :  158\n",
      "Loss: 3.519359827041626\n",
      "Batch :  159\n",
      "Loss: 3.7544662952423096\n",
      "Batch :  160\n",
      "Loss: 3.7941043376922607\n",
      "Batch :  161\n",
      "Loss: 3.8128767013549805\n",
      "Batch :  162\n",
      "Loss: 3.4803731441497803\n",
      "Batch :  163\n",
      "Loss: 3.3781702518463135\n",
      "Batch :  164\n",
      "Loss: 3.3601863384246826\n",
      "Batch :  165\n",
      "Loss: 3.6534628868103027\n",
      "Batch :  166\n",
      "Loss: 3.624101161956787\n",
      "Batch :  167\n",
      "Loss: 3.514096260070801\n",
      "Batch :  168\n",
      "Loss: 3.8522720336914062\n",
      "Batch :  169\n",
      "Loss: 3.6535651683807373\n",
      "Batch :  170\n",
      "Loss: 3.7778475284576416\n",
      "Batch :  171\n",
      "Loss: 3.61551833152771\n",
      "Batch :  172\n",
      "Loss: 3.658761501312256\n",
      "Batch :  173\n",
      "Loss: 4.000688552856445\n",
      "Batch :  174\n",
      "Loss: 3.6055495738983154\n",
      "Batch :  175\n",
      "Loss: 3.48629093170166\n",
      "Batch :  176\n",
      "Loss: 3.452359437942505\n",
      "Batch :  177\n",
      "Loss: 3.2786343097686768\n",
      "Batch :  178\n",
      "Loss: 3.5895633697509766\n",
      "Batch :  179\n",
      "Loss: 3.5512330532073975\n",
      "Batch :  180\n",
      "Loss: 3.4073660373687744\n",
      "Batch :  181\n",
      "Loss: 3.582430601119995\n",
      "Batch :  182\n",
      "Loss: 3.672558069229126\n",
      "Batch :  183\n",
      "Loss: 3.59740948677063\n",
      "Batch :  184\n",
      "Loss: 3.68977952003479\n",
      "Batch :  185\n",
      "Loss: 3.944758415222168\n",
      "Batch :  186\n",
      "Loss: 3.2485151290893555\n",
      "Batch :  187\n",
      "Loss: 3.682107448577881\n",
      "Batch :  188\n",
      "Loss: 3.3329029083251953\n",
      "Batch :  189\n",
      "Loss: 3.438821315765381\n",
      "Batch :  190\n",
      "Loss: 3.362159490585327\n",
      "Batch :  191\n",
      "Loss: 3.415605068206787\n",
      "Batch :  192\n",
      "Loss: 3.633005142211914\n",
      "Batch :  193\n",
      "Loss: 3.6450672149658203\n",
      "Batch :  194\n",
      "Loss: 3.7419533729553223\n",
      "Batch :  195\n",
      "Loss: 3.4683005809783936\n",
      "Batch :  196\n",
      "Loss: 3.646456480026245\n",
      "Batch :  197\n",
      "Loss: 3.5310428142547607\n",
      "Batch :  198\n",
      "Loss: 3.307579278945923\n",
      "Batch :  199\n",
      "Loss: 3.515568971633911\n",
      "Batch :  200\n",
      "Loss: 3.424391984939575\n",
      "Batch :  201\n",
      "Loss: 3.316450834274292\n",
      "Batch :  202\n",
      "Loss: 3.4865498542785645\n",
      "Batch :  203\n",
      "Loss: 3.3376922607421875\n",
      "Batch :  204\n",
      "Loss: 3.1250622272491455\n",
      "Batch :  205\n",
      "Loss: 3.4129860401153564\n",
      "Batch :  206\n",
      "Loss: 3.5360207557678223\n",
      "Batch :  207\n",
      "Loss: 3.557027578353882\n",
      "Batch :  208\n",
      "Loss: 3.5813589096069336\n",
      "Batch :  209\n",
      "Loss: 3.671192169189453\n",
      "Batch :  210\n",
      "Loss: 3.77706241607666\n",
      "Batch :  211\n",
      "Loss: 3.4029347896575928\n",
      "Batch :  212\n",
      "Loss: 3.5532732009887695\n",
      "Batch :  213\n",
      "Loss: 3.3858652114868164\n",
      "Batch :  214\n",
      "Loss: 3.6133956909179688\n",
      "Batch :  215\n",
      "Loss: 3.4978435039520264\n",
      "Batch :  216\n",
      "Loss: 3.533903121948242\n",
      "Batch :  217\n",
      "Loss: 3.205392360687256\n",
      "Batch :  218\n",
      "Loss: 3.404203176498413\n",
      "Batch :  219\n",
      "Loss: 3.4306623935699463\n",
      "Batch :  220\n",
      "Loss: 3.469465494155884\n",
      "Batch :  221\n",
      "Loss: 3.5820722579956055\n",
      "Batch :  222\n",
      "Loss: 3.4467930793762207\n",
      "Batch :  223\n",
      "Loss: 3.44427490234375\n",
      "Batch :  224\n",
      "Loss: 3.596013307571411\n",
      "Batch :  225\n",
      "Loss: 3.4170780181884766\n",
      "Batch :  226\n",
      "Loss: 3.4945292472839355\n",
      "Batch :  227\n",
      "Loss: 3.304353713989258\n",
      "Batch :  228\n",
      "Loss: 3.8288888931274414\n",
      "Batch :  229\n",
      "Loss: 3.5670974254608154\n",
      "Batch :  230\n",
      "Loss: 3.181378126144409\n",
      "Batch :  231\n",
      "Loss: 3.371429920196533\n",
      "Batch :  232\n",
      "Loss: 3.0636839866638184\n",
      "Batch :  233\n",
      "Loss: 3.390740394592285\n",
      "Batch :  234\n",
      "Loss: 3.419686794281006\n",
      "Batch :  235\n",
      "Loss: 3.7215678691864014\n",
      "Batch :  236\n",
      "Loss: 3.550058603286743\n",
      "Batch :  237\n",
      "Loss: 3.6737029552459717\n",
      "Batch :  238\n",
      "Loss: 3.5535292625427246\n",
      "Batch :  239\n",
      "Loss: 3.4254188537597656\n",
      "Batch :  240\n",
      "Loss: 3.196518898010254\n",
      "Batch :  241\n",
      "Loss: 3.4757657051086426\n",
      "Batch :  242\n",
      "Loss: 3.738358974456787\n",
      "Batch :  243\n",
      "Loss: 3.496997117996216\n",
      "Batch :  244\n",
      "Loss: 3.3169007301330566\n",
      "Batch :  245\n",
      "Loss: 3.358922243118286\n",
      "Batch :  246\n",
      "Loss: 3.537997007369995\n",
      "Batch :  247\n",
      "Loss: 3.636550188064575\n",
      "Batch :  248\n",
      "Loss: 3.5006635189056396\n",
      "Batch :  249\n",
      "Loss: 3.227393388748169\n",
      "Batch :  250\n",
      "Loss: 3.396667718887329\n",
      "Batch :  251\n",
      "Loss: 3.3278160095214844\n",
      "Batch :  252\n",
      "Loss: 3.4124817848205566\n",
      "Batch :  253\n",
      "Loss: 3.329442262649536\n",
      "Batch :  254\n",
      "Loss: 3.316732168197632\n",
      "Batch :  255\n",
      "Loss: 3.1166319847106934\n",
      "Batch :  256\n",
      "Loss: 3.3739349842071533\n",
      "Batch :  257\n",
      "Loss: 3.264317035675049\n",
      "Batch :  258\n",
      "Loss: 3.6877431869506836\n",
      "Batch :  259\n",
      "Loss: 3.35135817527771\n",
      "Batch :  260\n",
      "Loss: 3.386030435562134\n",
      "Batch :  261\n",
      "Loss: 3.368579864501953\n",
      "Batch :  262\n",
      "Loss: 3.4259400367736816\n",
      "Batch :  263\n",
      "Loss: 3.6428186893463135\n",
      "Batch :  264\n",
      "Loss: 3.3746438026428223\n",
      "Batch :  265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.2699408531188965\n",
      "Batch :  266\n",
      "Loss: 3.364609718322754\n",
      "Batch :  267\n",
      "Loss: 3.4416887760162354\n",
      "Batch :  268\n",
      "Loss: 3.2833001613616943\n",
      "Batch :  269\n",
      "Loss: 3.4181859493255615\n",
      "Batch :  270\n",
      "Loss: 3.4282565116882324\n",
      "Batch :  271\n",
      "Loss: 3.586915969848633\n",
      "Batch :  272\n",
      "Loss: 3.1034140586853027\n",
      "Batch :  273\n",
      "Loss: 3.2600677013397217\n",
      "Batch :  274\n",
      "Loss: 3.459303617477417\n",
      "Batch :  275\n",
      "Loss: 3.4173264503479004\n",
      "Batch :  276\n",
      "Loss: 3.3440089225769043\n",
      "Batch :  277\n",
      "Loss: 3.66367244720459\n",
      "Batch :  278\n",
      "Loss: 3.525322675704956\n",
      "Batch :  279\n",
      "Loss: 3.2980809211730957\n",
      "Batch :  280\n",
      "Loss: 3.4135866165161133\n",
      "Batch :  281\n",
      "Loss: 3.550558090209961\n",
      "Batch :  282\n",
      "Loss: 3.227552890777588\n",
      "Batch :  283\n",
      "Loss: 3.549752712249756\n",
      "Batch :  284\n",
      "Loss: 3.5365047454833984\n",
      "Batch :  285\n",
      "Loss: 3.789991855621338\n",
      "Batch :  286\n",
      "Loss: 3.4591214656829834\n",
      "Batch :  287\n",
      "Loss: 3.258127212524414\n",
      "Batch :  288\n",
      "Loss: 3.34641170501709\n",
      "Batch :  289\n",
      "Loss: 3.422328472137451\n",
      "Batch :  290\n",
      "Loss: 3.1517109870910645\n",
      "Batch :  291\n",
      "Loss: 3.2962398529052734\n",
      "Batch :  292\n",
      "Loss: 3.3053102493286133\n",
      "Batch :  293\n",
      "Loss: 3.1899607181549072\n",
      "Batch :  294\n",
      "Loss: 3.3927478790283203\n",
      "Batch :  295\n",
      "Loss: 3.2829813957214355\n",
      "Batch :  296\n",
      "Loss: 3.443031072616577\n",
      "Batch :  297\n",
      "Loss: 3.37861967086792\n",
      "Batch :  298\n",
      "Loss: 3.3583590984344482\n",
      "Batch :  299\n",
      "Loss: 3.2970786094665527\n",
      "Batch :  300\n",
      "Loss: 3.3713736534118652\n",
      "Batch :  301\n",
      "Loss: 3.4825806617736816\n",
      "Batch :  302\n",
      "Loss: 3.0639455318450928\n",
      "Batch :  303\n",
      "Loss: 3.5098085403442383\n",
      "Batch :  304\n",
      "Loss: 3.2721104621887207\n",
      "Batch :  305\n",
      "Loss: 3.2737231254577637\n",
      "Batch :  306\n",
      "Loss: 3.13457989692688\n",
      "Batch :  307\n",
      "Loss: 2.924891233444214\n",
      "Batch :  308\n",
      "Loss: 3.154176712036133\n",
      "Batch :  309\n",
      "Loss: 3.3892457485198975\n",
      "Batch :  310\n",
      "Loss: 3.5069751739501953\n",
      "Batch :  311\n",
      "Loss: 3.2773783206939697\n",
      "Batch :  312\n",
      "Loss: 2.604463577270508\n",
      "Batch - Loss: 3.456621106821127\n",
      "Epoch 4\n",
      "Batch :  0\n",
      "Loss: 3.151111364364624\n",
      "Batch :  1\n",
      "Loss: 3.5778093338012695\n",
      "Batch :  2\n",
      "Loss: 3.3602399826049805\n",
      "Batch :  3\n",
      "Loss: 2.889129400253296\n",
      "Batch :  4\n",
      "Loss: 3.180241584777832\n",
      "Batch :  5\n",
      "Loss: 3.4306328296661377\n",
      "Batch :  6\n",
      "Loss: 3.2794535160064697\n",
      "Batch :  7\n",
      "Loss: 3.193535327911377\n",
      "Batch :  8\n",
      "Loss: 3.401505708694458\n",
      "Batch :  9\n",
      "Loss: 3.573077917098999\n",
      "Batch :  10\n",
      "Loss: 3.5134875774383545\n",
      "Batch :  11\n",
      "Loss: 3.284019708633423\n",
      "Batch :  12\n",
      "Loss: 3.610208749771118\n",
      "Batch :  13\n",
      "Loss: 3.473175525665283\n",
      "Batch :  14\n",
      "Loss: 3.503544569015503\n",
      "Batch :  15\n",
      "Loss: 3.5935232639312744\n",
      "Batch :  16\n",
      "Loss: 3.6429128646850586\n",
      "Batch :  17\n",
      "Loss: 3.4787685871124268\n",
      "Batch :  18\n",
      "Loss: 3.4546871185302734\n",
      "Batch :  19\n",
      "Loss: 3.2638587951660156\n",
      "Batch :  20\n",
      "Loss: 3.4287400245666504\n",
      "Batch :  21\n",
      "Loss: 3.1872005462646484\n",
      "Batch :  22\n",
      "Loss: 3.7225732803344727\n",
      "Batch :  23\n",
      "Loss: 3.6854522228240967\n",
      "Batch :  24\n",
      "Loss: 3.5218827724456787\n",
      "Batch :  25\n",
      "Loss: 3.5319831371307373\n",
      "Batch :  26\n",
      "Loss: 3.3955740928649902\n",
      "Batch :  27\n",
      "Loss: 3.4575607776641846\n",
      "Batch :  28\n",
      "Loss: 3.712916851043701\n",
      "Batch :  29\n",
      "Loss: 3.496816396713257\n",
      "Batch :  30\n",
      "Loss: 3.7228379249572754\n",
      "Batch :  31\n",
      "Loss: 3.403639554977417\n",
      "Batch :  32\n",
      "Loss: 3.3115580081939697\n",
      "Batch :  33\n",
      "Loss: 3.327928066253662\n",
      "Batch :  34\n",
      "Loss: 3.4125900268554688\n",
      "Batch :  35\n",
      "Loss: 3.4397497177124023\n",
      "Batch :  36\n",
      "Loss: 3.458935022354126\n",
      "Batch :  37\n",
      "Loss: 3.561040163040161\n",
      "Batch :  38\n",
      "Loss: 3.520530939102173\n",
      "Batch :  39\n",
      "Loss: 3.40842342376709\n",
      "Batch :  40\n",
      "Loss: 3.2649435997009277\n",
      "Batch :  41\n",
      "Loss: 3.2826991081237793\n",
      "Batch :  42\n",
      "Loss: 3.6065871715545654\n",
      "Batch :  43\n",
      "Loss: 3.336287021636963\n",
      "Batch :  44\n",
      "Loss: 3.235042095184326\n",
      "Batch :  45\n",
      "Loss: 3.2380964756011963\n",
      "Batch :  46\n",
      "Loss: 3.509197235107422\n",
      "Batch :  47\n",
      "Loss: 3.0771219730377197\n",
      "Batch :  48\n",
      "Loss: 3.2669544219970703\n",
      "Batch :  49\n",
      "Loss: 3.3142964839935303\n",
      "Batch :  50\n",
      "Loss: 3.149721145629883\n",
      "Batch :  51\n",
      "Loss: 3.3429183959960938\n",
      "Batch :  52\n",
      "Loss: 3.2291014194488525\n",
      "Batch :  53\n",
      "Loss: 3.368101119995117\n",
      "Batch :  54\n",
      "Loss: 3.397597312927246\n",
      "Batch :  55\n",
      "Loss: 3.2035415172576904\n",
      "Batch :  56\n",
      "Loss: 3.6267895698547363\n",
      "Batch :  57\n",
      "Loss: 3.423858165740967\n",
      "Batch :  58\n",
      "Loss: 3.6773664951324463\n",
      "Batch :  59\n",
      "Loss: 3.311279535293579\n",
      "Batch :  60\n",
      "Loss: 3.193128824234009\n",
      "Batch :  61\n",
      "Loss: 3.2661120891571045\n",
      "Batch :  62\n",
      "Loss: 3.4240005016326904\n",
      "Batch :  63\n",
      "Loss: 3.5186519622802734\n",
      "Batch :  64\n",
      "Loss: 3.4493114948272705\n",
      "Batch :  65\n",
      "Loss: 3.5211751461029053\n",
      "Batch :  66\n",
      "Loss: 3.5503547191619873\n",
      "Batch :  67\n",
      "Loss: 3.39693021774292\n",
      "Batch :  68\n",
      "Loss: 3.3925108909606934\n",
      "Batch :  69\n",
      "Loss: 3.2381398677825928\n",
      "Batch :  70\n",
      "Loss: 3.1642372608184814\n",
      "Batch :  71\n",
      "Loss: 3.0663349628448486\n",
      "Batch :  72\n",
      "Loss: 3.066915988922119\n",
      "Batch :  73\n",
      "Loss: 3.2030229568481445\n",
      "Batch :  74\n",
      "Loss: 3.2278850078582764\n",
      "Batch :  75\n",
      "Loss: 3.1052987575531006\n",
      "Batch :  76\n",
      "Loss: 2.9589486122131348\n",
      "Batch :  77\n",
      "Loss: 3.0345659255981445\n",
      "Batch :  78\n",
      "Loss: 3.2355923652648926\n",
      "Batch :  79\n",
      "Loss: 3.196470022201538\n",
      "Batch :  80\n",
      "Loss: 3.3132309913635254\n",
      "Batch :  81\n",
      "Loss: 3.306429386138916\n",
      "Batch :  82\n",
      "Loss: 3.1839208602905273\n",
      "Batch :  83\n",
      "Loss: 3.191429853439331\n",
      "Batch :  84\n",
      "Loss: 3.1753129959106445\n",
      "Batch :  85\n",
      "Loss: 3.10192608833313\n",
      "Batch :  86\n",
      "Loss: 3.087015390396118\n",
      "Batch :  87\n",
      "Loss: 2.881012439727783\n",
      "Batch :  88\n",
      "Loss: 2.991583824157715\n",
      "Batch :  89\n",
      "Loss: 3.171100378036499\n",
      "Batch :  90\n",
      "Loss: 3.179729461669922\n",
      "Batch :  91\n",
      "Loss: 3.2970190048217773\n",
      "Batch :  92\n",
      "Loss: 3.2482736110687256\n",
      "Batch :  93\n",
      "Loss: 3.207951068878174\n",
      "Batch :  94\n",
      "Loss: 3.405973196029663\n",
      "Batch :  95\n",
      "Loss: 3.596412181854248\n",
      "Batch :  96\n",
      "Loss: 2.9823596477508545\n",
      "Batch :  97\n",
      "Loss: 3.307354211807251\n",
      "Batch :  98\n",
      "Loss: 3.313326358795166\n",
      "Batch :  99\n",
      "Loss: 3.4379122257232666\n",
      "Batch :  100\n",
      "Loss: 3.122403383255005\n",
      "Batch :  101\n",
      "Loss: 3.430116891860962\n",
      "Batch :  102\n",
      "Loss: 3.252108097076416\n",
      "Batch :  103\n",
      "Loss: 2.960012674331665\n",
      "Batch :  104\n",
      "Loss: 3.3238673210144043\n",
      "Batch :  105\n",
      "Loss: 3.1579015254974365\n",
      "Batch :  106\n",
      "Loss: 3.4606597423553467\n",
      "Batch :  107\n",
      "Loss: 3.2725324630737305\n",
      "Batch :  108\n",
      "Loss: 3.172966241836548\n",
      "Batch :  109\n",
      "Loss: 3.102118968963623\n",
      "Batch :  110\n",
      "Loss: 3.255465507507324\n",
      "Batch :  111\n",
      "Loss: 3.108945369720459\n",
      "Batch :  112\n",
      "Loss: 3.6197667121887207\n",
      "Batch :  113\n",
      "Loss: 3.0409247875213623\n",
      "Batch :  114\n",
      "Loss: 3.2894859313964844\n",
      "Batch :  115\n",
      "Loss: 3.4574062824249268\n",
      "Batch :  116\n",
      "Loss: 3.045074701309204\n",
      "Batch :  117\n",
      "Loss: 3.2014214992523193\n",
      "Batch :  118\n",
      "Loss: 3.1462950706481934\n",
      "Batch :  119\n",
      "Loss: 3.122793674468994\n",
      "Batch :  120\n",
      "Loss: 3.2310240268707275\n",
      "Batch :  121\n",
      "Loss: 3.3070013523101807\n",
      "Batch :  122\n",
      "Loss: 3.21612548828125\n",
      "Batch :  123\n",
      "Loss: 3.2377140522003174\n",
      "Batch :  124\n",
      "Loss: 3.2347939014434814\n",
      "Batch :  125\n",
      "Loss: 3.1789140701293945\n",
      "Batch :  126\n",
      "Loss: 3.1263856887817383\n",
      "Batch :  127\n",
      "Loss: 3.3328394889831543\n",
      "Batch :  128\n",
      "Loss: 3.032973051071167\n",
      "Batch :  129\n",
      "Loss: 3.199702262878418\n",
      "Batch :  130\n",
      "Loss: 3.2362279891967773\n",
      "Batch :  131\n",
      "Loss: 3.3033790588378906\n",
      "Batch :  132\n",
      "Loss: 3.177110433578491\n",
      "Batch :  133\n",
      "Loss: 3.242037773132324\n",
      "Batch :  134\n",
      "Loss: 3.1519298553466797\n",
      "Batch :  135\n",
      "Loss: 2.761051654815674\n",
      "Batch :  136\n",
      "Loss: 2.8660318851470947\n",
      "Batch :  137\n",
      "Loss: 3.0982260704040527\n",
      "Batch :  138\n",
      "Loss: 3.1526849269866943\n",
      "Batch :  139\n",
      "Loss: 2.910076141357422\n",
      "Batch :  140\n",
      "Loss: 2.952880620956421\n",
      "Batch :  141\n",
      "Loss: 3.2771546840667725\n",
      "Batch :  142\n",
      "Loss: 3.155400514602661\n",
      "Batch :  143\n",
      "Loss: 3.173372983932495\n",
      "Batch :  144\n",
      "Loss: 3.3235857486724854\n",
      "Batch :  145\n",
      "Loss: 3.077810287475586\n",
      "Batch :  146\n",
      "Loss: 2.9658305644989014\n",
      "Batch :  147\n",
      "Loss: 3.1850461959838867\n",
      "Batch :  148\n",
      "Loss: 3.0881218910217285\n",
      "Batch :  149\n",
      "Loss: 3.210031747817993\n",
      "Batch :  150\n",
      "Loss: 3.0634279251098633\n",
      "Batch :  151\n",
      "Loss: 3.21171236038208\n",
      "Batch :  152\n",
      "Loss: 3.318776845932007\n",
      "Batch :  153\n",
      "Loss: 3.009812355041504\n",
      "Batch :  154\n",
      "Loss: 3.18324613571167\n",
      "Batch :  155\n",
      "Loss: 3.1620442867279053\n",
      "Batch :  156\n",
      "Loss: 3.086007595062256\n",
      "Batch :  157\n",
      "Loss: 3.4188437461853027\n",
      "Batch :  158\n",
      "Loss: 2.7959694862365723\n",
      "Batch :  159\n",
      "Loss: 2.9816908836364746\n",
      "Batch :  160\n",
      "Loss: 3.133655309677124\n",
      "Batch :  161\n",
      "Loss: 3.141303539276123\n",
      "Batch :  162\n",
      "Loss: 2.8644979000091553\n",
      "Batch :  163\n",
      "Loss: 2.922111749649048\n",
      "Batch :  164\n",
      "Loss: 2.6742613315582275\n",
      "Batch :  165\n",
      "Loss: 3.0879735946655273\n",
      "Batch :  166\n",
      "Loss: 2.8708724975585938\n",
      "Batch :  167\n",
      "Loss: 2.6926236152648926\n",
      "Batch :  168\n",
      "Loss: 3.2257096767425537\n",
      "Batch :  169\n",
      "Loss: 3.0595266819000244\n",
      "Batch :  170\n",
      "Loss: 3.164763927459717\n",
      "Batch :  171\n",
      "Loss: 2.939774751663208\n",
      "Batch :  172\n",
      "Loss: 3.055004596710205\n",
      "Batch :  173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 3.450854778289795\n",
      "Batch :  174\n",
      "Loss: 2.798569440841675\n",
      "Batch :  175\n",
      "Loss: 2.9220969676971436\n",
      "Batch :  176\n",
      "Loss: 2.9451727867126465\n",
      "Batch :  177\n",
      "Loss: 2.7294936180114746\n",
      "Batch :  178\n",
      "Loss: 3.0572798252105713\n",
      "Batch :  179\n",
      "Loss: 2.9392971992492676\n",
      "Batch :  180\n",
      "Loss: 2.7815463542938232\n",
      "Batch :  181\n",
      "Loss: 3.043476104736328\n",
      "Batch :  182\n",
      "Loss: 3.2509074211120605\n",
      "Batch :  183\n",
      "Loss: 3.0705039501190186\n",
      "Batch :  184\n",
      "Loss: 3.1211655139923096\n",
      "Batch :  185\n",
      "Loss: 3.3264336585998535\n",
      "Batch :  186\n",
      "Loss: 2.580925941467285\n",
      "Batch :  187\n",
      "Loss: 3.039412498474121\n",
      "Batch :  188\n",
      "Loss: 2.609219789505005\n",
      "Batch :  189\n",
      "Loss: 2.8068103790283203\n",
      "Batch :  190\n",
      "Loss: 2.665544033050537\n",
      "Batch :  191\n",
      "Loss: 2.853842258453369\n",
      "Batch :  192\n",
      "Loss: 2.996521234512329\n",
      "Batch :  193\n",
      "Loss: 3.067275285720825\n",
      "Batch :  194\n",
      "Loss: 3.2421956062316895\n",
      "Batch :  195\n",
      "Loss: 2.8064417839050293\n",
      "Batch :  196\n",
      "Loss: 3.1111721992492676\n",
      "Batch :  197\n",
      "Loss: 2.7848730087280273\n",
      "Batch :  198\n",
      "Loss: 2.8159687519073486\n",
      "Batch :  199\n",
      "Loss: 2.821744441986084\n",
      "Batch :  200\n",
      "Loss: 2.906351089477539\n",
      "Batch :  201\n",
      "Loss: 2.8554930686950684\n",
      "Batch :  202\n",
      "Loss: 3.01633882522583\n",
      "Batch :  203\n",
      "Loss: 2.836289644241333\n",
      "Batch :  204\n",
      "Loss: 2.5977296829223633\n",
      "Batch :  205\n",
      "Loss: 2.886974573135376\n",
      "Batch :  206\n",
      "Loss: 2.9649369716644287\n",
      "Batch :  207\n",
      "Loss: 2.8210132122039795\n",
      "Batch :  208\n",
      "Loss: 3.011737108230591\n",
      "Batch :  209\n",
      "Loss: 2.9837863445281982\n",
      "Batch :  210\n",
      "Loss: 3.1512207984924316\n",
      "Batch :  211\n",
      "Loss: 2.806415319442749\n",
      "Batch :  212\n",
      "Loss: 2.769672155380249\n",
      "Batch :  213\n",
      "Loss: 2.8545167446136475\n",
      "Batch :  214\n",
      "Loss: 2.9730148315429688\n",
      "Batch :  215\n",
      "Loss: 2.7250282764434814\n",
      "Batch :  216\n",
      "Loss: 2.8230056762695312\n",
      "Batch :  217\n",
      "Loss: 2.683401584625244\n",
      "Batch :  218\n",
      "Loss: 2.956664800643921\n",
      "Batch :  219\n",
      "Loss: 2.853742837905884\n",
      "Batch :  220\n",
      "Loss: 2.8733057975769043\n",
      "Batch :  221\n",
      "Loss: 2.893045425415039\n",
      "Batch :  222\n",
      "Loss: 2.906348466873169\n",
      "Batch :  223\n",
      "Loss: 2.915898323059082\n",
      "Batch :  224\n",
      "Loss: 3.103309392929077\n",
      "Batch :  225\n",
      "Loss: 2.863870859146118\n",
      "Batch :  226\n",
      "Loss: 2.7988648414611816\n",
      "Batch :  227\n",
      "Loss: 2.7542812824249268\n",
      "Batch :  228\n",
      "Loss: 3.209362268447876\n",
      "Batch :  229\n",
      "Loss: 2.9105384349823\n",
      "Batch :  230\n",
      "Loss: 2.6836509704589844\n",
      "Batch :  231\n",
      "Loss: 2.680828809738159\n",
      "Batch :  232\n",
      "Loss: 2.5240230560302734\n",
      "Batch :  233\n",
      "Loss: 2.840304136276245\n",
      "Batch :  234\n",
      "Loss: 2.8361051082611084\n",
      "Batch :  235\n",
      "Loss: 2.8675694465637207\n",
      "Batch :  236\n",
      "Loss: 2.8704347610473633\n",
      "Batch :  237\n",
      "Loss: 2.902205467224121\n",
      "Batch :  238\n",
      "Loss: 3.099093437194824\n",
      "Batch :  239\n",
      "Loss: 2.9190008640289307\n",
      "Batch :  240\n",
      "Loss: 2.544890880584717\n",
      "Batch :  241\n",
      "Loss: 2.910323143005371\n",
      "Batch :  242\n",
      "Loss: 3.0021002292633057\n",
      "Batch :  243\n",
      "Loss: 2.870445728302002\n",
      "Batch :  244\n",
      "Loss: 2.68233585357666\n",
      "Batch :  245\n",
      "Loss: 2.8303897380828857\n",
      "Batch :  246\n",
      "Loss: 3.0037505626678467\n",
      "Batch :  247\n",
      "Loss: 2.975017547607422\n",
      "Batch :  248\n",
      "Loss: 3.03414249420166\n",
      "Batch :  249\n",
      "Loss: 2.7304632663726807\n",
      "Batch :  250\n",
      "Loss: 2.771298408508301\n",
      "Batch :  251\n",
      "Loss: 2.92930006980896\n",
      "Batch :  252\n",
      "Loss: 2.9558522701263428\n",
      "Batch :  253\n",
      "Loss: 2.799828052520752\n",
      "Batch :  254\n",
      "Loss: 2.733490228652954\n",
      "Batch :  255\n",
      "Loss: 2.6418256759643555\n",
      "Batch :  256\n",
      "Loss: 2.7345337867736816\n",
      "Batch :  257\n",
      "Loss: 2.7383248805999756\n",
      "Batch :  258\n",
      "Loss: 3.1210837364196777\n",
      "Batch :  259\n",
      "Loss: 2.7084052562713623\n",
      "Batch :  260\n",
      "Loss: 2.7325596809387207\n",
      "Batch :  261\n",
      "Loss: 2.8016510009765625\n",
      "Batch :  262\n",
      "Loss: 2.7010021209716797\n",
      "Batch :  263\n",
      "Loss: 3.0118649005889893\n",
      "Batch :  264\n",
      "Loss: 2.737015962600708\n",
      "Batch :  265\n",
      "Loss: 2.700448513031006\n",
      "Batch :  266\n",
      "Loss: 2.653855323791504\n",
      "Batch :  267\n",
      "Loss: 2.7251226902008057\n",
      "Batch :  268\n",
      "Loss: 2.609724998474121\n",
      "Batch :  269\n",
      "Loss: 2.784863233566284\n",
      "Batch :  270\n",
      "Loss: 2.7825989723205566\n",
      "Batch :  271\n",
      "Loss: 2.8830034732818604\n",
      "Batch :  272\n",
      "Loss: 2.477729320526123\n",
      "Batch :  273\n",
      "Loss: 2.6681277751922607\n",
      "Batch :  274\n",
      "Loss: 2.9001846313476562\n",
      "Batch :  275\n",
      "Loss: 2.736954689025879\n",
      "Batch :  276\n",
      "Loss: 2.6462299823760986\n",
      "Batch :  277\n",
      "Loss: 2.9343202114105225\n",
      "Batch :  278\n",
      "Loss: 2.871000289916992\n",
      "Batch :  279\n",
      "Loss: 2.6786978244781494\n",
      "Batch :  280\n",
      "Loss: 2.6911535263061523\n",
      "Batch :  281\n",
      "Loss: 2.9194207191467285\n",
      "Batch :  282\n",
      "Loss: 2.4545071125030518\n",
      "Batch :  283\n",
      "Loss: 2.937433958053589\n",
      "Batch :  284\n",
      "Loss: 2.9021501541137695\n",
      "Batch :  285\n",
      "Loss: 3.0907394886016846\n",
      "Batch :  286\n",
      "Loss: 2.8910863399505615\n",
      "Batch :  287\n",
      "Loss: 2.763735055923462\n",
      "Batch :  288\n",
      "Loss: 2.8325397968292236\n",
      "Batch :  289\n",
      "Loss: 2.9742369651794434\n",
      "Batch :  290\n",
      "Loss: 2.649484157562256\n",
      "Batch :  291\n",
      "Loss: 2.778985023498535\n",
      "Batch :  292\n",
      "Loss: 2.685424566268921\n",
      "Batch :  293\n",
      "Loss: 2.624401330947876\n",
      "Batch :  294\n",
      "Loss: 2.8236377239227295\n",
      "Batch :  295\n",
      "Loss: 2.8642358779907227\n",
      "Batch :  296\n",
      "Loss: 2.9431161880493164\n",
      "Batch :  297\n",
      "Loss: 2.7711737155914307\n",
      "Batch :  298\n",
      "Loss: 2.7522685527801514\n",
      "Batch :  299\n",
      "Loss: 2.817068099975586\n",
      "Batch :  300\n",
      "Loss: 2.895573139190674\n",
      "Batch :  301\n",
      "Loss: 3.013328790664673\n",
      "Batch :  302\n",
      "Loss: 2.5047781467437744\n",
      "Batch :  303\n",
      "Loss: 2.818329334259033\n",
      "Batch :  304\n",
      "Loss: 2.592893600463867\n",
      "Batch :  305\n",
      "Loss: 2.8367958068847656\n",
      "Batch :  306\n",
      "Loss: 2.548457622528076\n",
      "Batch :  307\n",
      "Loss: 2.4371767044067383\n",
      "Batch :  308\n",
      "Loss: 2.665238380432129\n",
      "Batch :  309\n",
      "Loss: 2.9623937606811523\n",
      "Batch :  310\n",
      "Loss: 2.9814836978912354\n",
      "Batch :  311\n",
      "Loss: 2.7689924240112305\n",
      "Batch :  312\n",
      "Loss: 2.1358249187469482\n",
      "Batch - Loss: 2.8465060898290275\n",
      "Epoch 5\n",
      "Batch :  0\n",
      "Loss: 2.644310235977173\n",
      "Batch :  1\n",
      "Loss: 3.0829765796661377\n",
      "Batch :  2\n",
      "Loss: 2.786010265350342\n",
      "Batch :  3\n",
      "Loss: 2.351595640182495\n",
      "Batch :  4\n",
      "Loss: 2.5117547512054443\n",
      "Batch :  5\n",
      "Loss: 2.8845231533050537\n",
      "Batch :  6\n",
      "Loss: 2.7415874004364014\n",
      "Batch :  7\n",
      "Loss: 2.728391647338867\n",
      "Batch :  8\n",
      "Loss: 2.851682424545288\n",
      "Batch :  9\n",
      "Loss: 3.0094716548919678\n",
      "Batch :  10\n",
      "Loss: 2.980858564376831\n",
      "Batch :  11\n",
      "Loss: 2.540266752243042\n",
      "Batch :  12\n",
      "Loss: 2.918013095855713\n",
      "Batch :  13\n",
      "Loss: 2.8608813285827637\n",
      "Batch :  14\n",
      "Loss: 2.9875714778900146\n",
      "Batch :  15\n",
      "Loss: 2.8790042400360107\n",
      "Batch :  16\n",
      "Loss: 2.9532997608184814\n",
      "Batch :  17\n",
      "Loss: 2.7375435829162598\n",
      "Batch :  18\n",
      "Loss: 2.894087314605713\n",
      "Batch :  19\n",
      "Loss: 2.652562141418457\n",
      "Batch :  20\n",
      "Loss: 2.710238218307495\n",
      "Batch :  21\n",
      "Loss: 2.5361108779907227\n",
      "Batch :  22\n",
      "Loss: 2.958296537399292\n",
      "Batch :  23\n",
      "Loss: 2.892137289047241\n",
      "Batch :  24\n",
      "Loss: 2.821671724319458\n",
      "Batch :  25\n",
      "Loss: 2.8612985610961914\n",
      "Batch :  26\n",
      "Loss: 2.7370352745056152\n",
      "Batch :  27\n",
      "Loss: 2.831362724304199\n",
      "Batch :  28\n",
      "Loss: 3.2034084796905518\n",
      "Batch :  29\n",
      "Loss: 2.801393747329712\n",
      "Batch :  30\n",
      "Loss: 3.046799898147583\n",
      "Batch :  31\n",
      "Loss: 2.8349156379699707\n",
      "Batch :  32\n",
      "Loss: 2.679534912109375\n",
      "Batch :  33\n",
      "Loss: 2.718043804168701\n",
      "Batch :  34\n",
      "Loss: 2.797081708908081\n",
      "Batch :  35\n",
      "Loss: 2.676445484161377\n",
      "Batch :  36\n",
      "Loss: 2.668109655380249\n",
      "Batch :  37\n",
      "Loss: 2.8808422088623047\n",
      "Batch :  38\n",
      "Loss: 2.7068817615509033\n",
      "Batch :  39\n",
      "Loss: 2.6734373569488525\n",
      "Batch :  40\n",
      "Loss: 2.4727866649627686\n",
      "Batch :  41\n",
      "Loss: 2.6129915714263916\n",
      "Batch :  42\n",
      "Loss: 2.775104284286499\n",
      "Batch :  43\n",
      "Loss: 2.600440740585327\n",
      "Batch :  44\n",
      "Loss: 2.6459405422210693\n",
      "Batch :  45\n",
      "Loss: 2.8044967651367188\n",
      "Batch :  46\n",
      "Loss: 2.83199405670166\n",
      "Batch :  47\n",
      "Loss: 2.585142135620117\n",
      "Batch :  48\n",
      "Loss: 2.720005512237549\n",
      "Batch :  49\n",
      "Loss: 2.747830629348755\n",
      "Batch :  50\n",
      "Loss: 2.663463592529297\n",
      "Batch :  51\n",
      "Loss: 2.9155428409576416\n",
      "Batch :  52\n",
      "Loss: 2.8083372116088867\n",
      "Batch :  53\n",
      "Loss: 2.8188889026641846\n",
      "Batch :  54\n",
      "Loss: 2.908611536026001\n",
      "Batch :  55\n",
      "Loss: 2.691432237625122\n",
      "Batch :  56\n",
      "Loss: 3.0634472370147705\n",
      "Batch :  57\n",
      "Loss: 2.8929951190948486\n",
      "Batch :  58\n",
      "Loss: 3.220870018005371\n",
      "Batch :  59\n",
      "Loss: 2.822023868560791\n",
      "Batch :  60\n",
      "Loss: 2.720794916152954\n",
      "Batch :  61\n",
      "Loss: 2.682673692703247\n",
      "Batch :  62\n",
      "Loss: 2.8973400592803955\n",
      "Batch :  63\n",
      "Loss: 2.8522024154663086\n",
      "Batch :  64\n",
      "Loss: 2.741952657699585\n",
      "Batch :  65\n",
      "Loss: 2.8923356533050537\n",
      "Batch :  66\n",
      "Loss: 2.7223641872406006\n",
      "Batch :  67\n",
      "Loss: 2.9135687351226807\n",
      "Batch :  68\n",
      "Loss: 2.7963109016418457\n",
      "Batch :  69\n",
      "Loss: 2.9242982864379883\n",
      "Batch :  70\n",
      "Loss: 2.9920310974121094\n",
      "Batch :  71\n",
      "Loss: 2.867584466934204\n",
      "Batch :  72\n",
      "Loss: 3.026155710220337\n",
      "Batch :  73\n",
      "Loss: 2.8723769187927246\n",
      "Batch :  74\n",
      "Loss: 2.8764214515686035\n",
      "Batch :  75\n",
      "Loss: 2.773683786392212\n",
      "Batch :  76\n",
      "Loss: 2.6636977195739746\n",
      "Batch :  77\n",
      "Loss: 2.554286241531372\n",
      "Batch :  78\n",
      "Loss: 2.5481443405151367\n",
      "Batch :  79\n",
      "Loss: 2.6769967079162598\n",
      "Batch :  80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.853553056716919\n",
      "Batch :  81\n",
      "Loss: 2.906369209289551\n",
      "Batch :  82\n",
      "Loss: 2.776461601257324\n",
      "Batch :  83\n",
      "Loss: 2.6281557083129883\n",
      "Batch :  84\n",
      "Loss: 2.8036465644836426\n",
      "Batch :  85\n",
      "Loss: 2.662114381790161\n",
      "Batch :  86\n",
      "Loss: 2.4758005142211914\n",
      "Batch :  87\n",
      "Loss: 2.295968532562256\n",
      "Batch :  88\n",
      "Loss: 2.4272727966308594\n",
      "Batch :  89\n",
      "Loss: 2.522841215133667\n",
      "Batch :  90\n",
      "Loss: 2.7213685512542725\n",
      "Batch :  91\n",
      "Loss: 2.783456563949585\n",
      "Batch :  92\n",
      "Loss: 2.703197717666626\n",
      "Batch :  93\n",
      "Loss: 2.8685288429260254\n",
      "Batch :  94\n",
      "Loss: 2.8162453174591064\n",
      "Batch :  95\n",
      "Loss: 3.00897216796875\n",
      "Batch :  96\n",
      "Loss: 2.3876168727874756\n",
      "Batch :  97\n",
      "Loss: 2.7135541439056396\n",
      "Batch :  98\n",
      "Loss: 3.005922794342041\n",
      "Batch :  99\n",
      "Loss: 3.042966365814209\n",
      "Batch :  100\n",
      "Loss: 2.60452938079834\n",
      "Batch :  101\n",
      "Loss: 2.9495861530303955\n",
      "Batch :  102\n",
      "Loss: 2.8894033432006836\n",
      "Batch :  103\n",
      "Loss: 2.4884719848632812\n",
      "Batch :  104\n",
      "Loss: 2.8802359104156494\n",
      "Batch :  105\n",
      "Loss: 2.564326524734497\n",
      "Batch :  106\n",
      "Loss: 2.8124074935913086\n",
      "Batch :  107\n",
      "Loss: 2.790701389312744\n",
      "Batch :  108\n",
      "Loss: 2.652350902557373\n",
      "Batch :  109\n",
      "Loss: 2.5451507568359375\n",
      "Batch :  110\n",
      "Loss: 2.4510581493377686\n",
      "Batch :  111\n",
      "Loss: 2.5201363563537598\n",
      "Batch :  112\n",
      "Loss: 3.0900685787200928\n",
      "Batch :  113\n",
      "Loss: 2.570603132247925\n",
      "Batch :  114\n",
      "Loss: 2.8014957904815674\n",
      "Batch :  115\n",
      "Loss: 2.9332423210144043\n",
      "Batch :  116\n",
      "Loss: 2.6622984409332275\n",
      "Batch :  117\n",
      "Loss: 2.7760257720947266\n",
      "Batch :  118\n",
      "Loss: 2.594780206680298\n",
      "Batch :  119\n",
      "Loss: 2.6943531036376953\n",
      "Batch :  120\n",
      "Loss: 2.938161849975586\n",
      "Batch :  121\n",
      "Loss: 2.911064863204956\n",
      "Batch :  122\n",
      "Loss: 2.7203612327575684\n",
      "Batch :  123\n",
      "Loss: 2.769155740737915\n",
      "Batch :  124\n",
      "Loss: 2.8004097938537598\n",
      "Batch :  125\n",
      "Loss: 2.6471664905548096\n",
      "Batch :  126\n",
      "Loss: 2.6940932273864746\n",
      "Batch :  127\n",
      "Loss: 2.846640110015869\n",
      "Batch :  128\n",
      "Loss: 2.6256814002990723\n",
      "Batch :  129\n",
      "Loss: 2.6555120944976807\n",
      "Batch :  130\n",
      "Loss: 2.7800657749176025\n",
      "Batch :  131\n",
      "Loss: 2.6520369052886963\n",
      "Batch :  132\n",
      "Loss: 2.6412670612335205\n",
      "Batch :  133\n",
      "Loss: 2.604705333709717\n",
      "Batch :  134\n",
      "Loss: 2.6570310592651367\n",
      "Batch :  135\n",
      "Loss: 2.3135671615600586\n",
      "Batch :  136\n",
      "Loss: 2.449723958969116\n",
      "Batch :  137\n",
      "Loss: 2.4097321033477783\n",
      "Batch :  138\n",
      "Loss: 2.6013448238372803\n",
      "Batch :  139\n",
      "Loss: 2.2836179733276367\n",
      "Batch :  140\n",
      "Loss: 2.3067405223846436\n",
      "Batch :  141\n",
      "Loss: 2.6421382427215576\n",
      "Batch :  142\n",
      "Loss: 2.65946364402771\n",
      "Batch :  143\n",
      "Loss: 2.6498634815216064\n",
      "Batch :  144\n",
      "Loss: 2.873176097869873\n",
      "Batch :  145\n",
      "Loss: 2.5655465126037598\n",
      "Batch :  146\n",
      "Loss: 2.4601192474365234\n",
      "Batch :  147\n",
      "Loss: 2.6803152561187744\n",
      "Batch :  148\n",
      "Loss: 2.5361690521240234\n",
      "Batch :  149\n",
      "Loss: 2.7115442752838135\n",
      "Batch :  150\n",
      "Loss: 2.6489386558532715\n",
      "Batch :  151\n",
      "Loss: 2.7396228313446045\n",
      "Batch :  152\n",
      "Loss: 2.8183650970458984\n",
      "Batch :  153\n",
      "Loss: 2.5626842975616455\n",
      "Batch :  154\n",
      "Loss: 2.786414384841919\n",
      "Batch :  155\n",
      "Loss: 2.5936479568481445\n",
      "Batch :  156\n",
      "Loss: 2.5636956691741943\n",
      "Batch :  157\n",
      "Loss: 2.790915012359619\n",
      "Batch :  158\n",
      "Loss: 2.5487606525421143\n",
      "Batch :  159\n",
      "Loss: 2.5010156631469727\n",
      "Batch :  160\n",
      "Loss: 2.7790403366088867\n",
      "Batch :  161\n",
      "Loss: 2.7111003398895264\n",
      "Batch :  162\n",
      "Loss: 2.4478070735931396\n",
      "Batch :  163\n",
      "Loss: 2.4028375148773193\n",
      "Batch :  164\n",
      "Loss: 2.271535873413086\n",
      "Batch :  165\n",
      "Loss: 2.703824758529663\n",
      "Batch :  166\n",
      "Loss: 2.417863130569458\n",
      "Batch :  167\n",
      "Loss: 2.3935511112213135\n",
      "Batch :  168\n",
      "Loss: 2.7628800868988037\n",
      "Batch :  169\n",
      "Loss: 2.5713486671447754\n",
      "Batch :  170\n",
      "Loss: 2.724881172180176\n",
      "Batch :  171\n",
      "Loss: 2.373316764831543\n",
      "Batch :  172\n",
      "Loss: 2.4090075492858887\n",
      "Batch :  173\n",
      "Loss: 2.956390380859375\n",
      "Batch :  174\n",
      "Loss: 2.29386830329895\n",
      "Batch :  175\n",
      "Loss: 2.411250591278076\n",
      "Batch :  176\n",
      "Loss: 2.4101390838623047\n",
      "Batch :  177\n",
      "Loss: 2.382124185562134\n",
      "Batch :  178\n",
      "Loss: 2.701793670654297\n",
      "Batch :  179\n",
      "Loss: 2.4460489749908447\n",
      "Batch :  180\n",
      "Loss: 2.2897801399230957\n",
      "Batch :  181\n",
      "Loss: 2.5096309185028076\n",
      "Batch :  182\n",
      "Loss: 2.642833948135376\n",
      "Batch :  183\n",
      "Loss: 2.5373435020446777\n",
      "Batch :  184\n",
      "Loss: 2.574479103088379\n",
      "Batch :  185\n",
      "Loss: 2.972963809967041\n",
      "Batch :  186\n",
      "Loss: 2.2357640266418457\n",
      "Batch :  187\n",
      "Loss: 2.6394903659820557\n",
      "Batch :  188\n",
      "Loss: 2.287381410598755\n",
      "Batch :  189\n",
      "Loss: 2.454292058944702\n",
      "Batch :  190\n",
      "Loss: 2.127990961074829\n",
      "Batch :  191\n",
      "Loss: 2.4044270515441895\n",
      "Batch :  192\n",
      "Loss: 2.5781302452087402\n",
      "Batch :  193\n",
      "Loss: 2.548314332962036\n",
      "Batch :  194\n",
      "Loss: 2.656186819076538\n",
      "Batch :  195\n",
      "Loss: 2.337336778640747\n",
      "Batch :  196\n",
      "Loss: 2.5272903442382812\n",
      "Batch :  197\n",
      "Loss: 2.3584275245666504\n",
      "Batch :  198\n",
      "Loss: 2.253154754638672\n",
      "Batch :  199\n",
      "Loss: 2.320338487625122\n",
      "Batch :  200\n",
      "Loss: 2.349592924118042\n",
      "Batch :  201\n",
      "Loss: 2.484780788421631\n",
      "Batch :  202\n",
      "Loss: 2.5925889015197754\n",
      "Batch :  203\n",
      "Loss: 2.395132064819336\n",
      "Batch :  204\n",
      "Loss: 2.1275393962860107\n",
      "Batch :  205\n",
      "Loss: 2.4735844135284424\n",
      "Batch :  206\n",
      "Loss: 2.599181890487671\n",
      "Batch :  207\n",
      "Loss: 2.323141098022461\n",
      "Batch :  208\n",
      "Loss: 2.5189476013183594\n",
      "Batch :  209\n",
      "Loss: 2.6475017070770264\n",
      "Batch :  210\n",
      "Loss: 2.83746337890625\n",
      "Batch :  211\n",
      "Loss: 2.329022169113159\n",
      "Batch :  212\n",
      "Loss: 2.48323917388916\n",
      "Batch :  213\n",
      "Loss: 2.3050150871276855\n",
      "Batch :  214\n",
      "Loss: 2.6190240383148193\n",
      "Batch :  215\n",
      "Loss: 2.3389980792999268\n",
      "Batch :  216\n",
      "Loss: 2.403282880783081\n",
      "Batch :  217\n",
      "Loss: 2.325281858444214\n",
      "Batch :  218\n",
      "Loss: 2.3840928077697754\n",
      "Batch :  219\n",
      "Loss: 2.568988561630249\n",
      "Batch :  220\n",
      "Loss: 2.4361391067504883\n",
      "Batch :  221\n",
      "Loss: 2.509091854095459\n",
      "Batch :  222\n",
      "Loss: 2.4739999771118164\n",
      "Batch :  223\n",
      "Loss: 2.4174880981445312\n",
      "Batch :  224\n",
      "Loss: 2.5224404335021973\n",
      "Batch :  225\n",
      "Loss: 2.3926889896392822\n",
      "Batch :  226\n",
      "Loss: 2.4014880657196045\n",
      "Batch :  227\n",
      "Loss: 2.4843976497650146\n",
      "Batch :  228\n",
      "Loss: 2.690993547439575\n",
      "Batch :  229\n",
      "Loss: 2.429898738861084\n",
      "Batch :  230\n",
      "Loss: 2.284442186355591\n",
      "Batch :  231\n",
      "Loss: 2.244810104370117\n",
      "Batch :  232\n",
      "Loss: 2.143150806427002\n",
      "Batch :  233\n",
      "Loss: 2.4466001987457275\n",
      "Batch :  234\n",
      "Loss: 2.470752716064453\n",
      "Batch :  235\n",
      "Loss: 2.6812596321105957\n",
      "Batch :  236\n",
      "Loss: 2.5608208179473877\n",
      "Batch :  237\n",
      "Loss: 2.517388105392456\n",
      "Batch :  238\n",
      "Loss: 2.579775810241699\n",
      "Batch :  239\n",
      "Loss: 2.4282290935516357\n",
      "Batch :  240\n",
      "Loss: 2.1395955085754395\n",
      "Batch :  241\n",
      "Loss: 2.428499937057495\n",
      "Batch :  242\n",
      "Loss: 2.47835111618042\n",
      "Batch :  243\n",
      "Loss: 2.378533124923706\n",
      "Batch :  244\n",
      "Loss: 2.184415817260742\n",
      "Batch :  245\n",
      "Loss: 2.5759408473968506\n",
      "Batch :  246\n",
      "Loss: 2.5941944122314453\n",
      "Batch :  247\n",
      "Loss: 2.4749200344085693\n",
      "Batch :  248\n",
      "Loss: 2.617159128189087\n",
      "Batch :  249\n",
      "Loss: 2.2551982402801514\n",
      "Batch :  250\n",
      "Loss: 2.2868518829345703\n",
      "Batch :  251\n",
      "Loss: 2.378732204437256\n",
      "Batch :  252\n",
      "Loss: 2.378852128982544\n",
      "Batch :  253\n",
      "Loss: 2.404175043106079\n",
      "Batch :  254\n",
      "Loss: 2.2916433811187744\n",
      "Batch :  255\n",
      "Loss: 2.200831890106201\n",
      "Batch :  256\n",
      "Loss: 2.4394776821136475\n",
      "Batch :  257\n",
      "Loss: 2.2274010181427\n",
      "Batch :  258\n",
      "Loss: 2.630251407623291\n",
      "Batch :  259\n",
      "Loss: 2.2966206073760986\n",
      "Batch :  260\n",
      "Loss: 2.3193581104278564\n",
      "Batch :  261\n",
      "Loss: 2.3516218662261963\n",
      "Batch :  262\n",
      "Loss: 2.423703908920288\n",
      "Batch :  263\n",
      "Loss: 2.6103365421295166\n",
      "Batch :  264\n",
      "Loss: 2.3991448879241943\n",
      "Batch :  265\n",
      "Loss: 2.2634360790252686\n",
      "Batch :  266\n",
      "Loss: 2.0851352214813232\n",
      "Batch :  267\n",
      "Loss: 2.381033182144165\n",
      "Batch :  268\n",
      "Loss: 2.1369807720184326\n",
      "Batch :  269\n",
      "Loss: 2.3257086277008057\n",
      "Batch :  270\n",
      "Loss: 2.470365047454834\n",
      "Batch :  271\n",
      "Loss: 2.4281411170959473\n",
      "Batch :  272\n",
      "Loss: 2.2149460315704346\n",
      "Batch :  273\n",
      "Loss: 2.289034128189087\n",
      "Batch :  274\n",
      "Loss: 2.474393129348755\n",
      "Batch :  275\n",
      "Loss: 2.3804497718811035\n",
      "Batch :  276\n",
      "Loss: 2.3756864070892334\n",
      "Batch :  277\n",
      "Loss: 2.623882532119751\n",
      "Batch :  278\n",
      "Loss: 2.4682493209838867\n",
      "Batch :  279\n",
      "Loss: 2.261449098587036\n",
      "Batch :  280\n",
      "Loss: 2.139641284942627\n",
      "Batch :  281\n",
      "Loss: 2.511749029159546\n",
      "Batch :  282\n",
      "Loss: 2.232327699661255\n",
      "Batch :  283\n",
      "Loss: 2.4912564754486084\n",
      "Batch :  284\n",
      "Loss: 2.327479600906372\n",
      "Batch :  285\n",
      "Loss: 2.565523624420166\n",
      "Batch :  286\n",
      "Loss: 2.5167431831359863\n",
      "Batch :  287\n",
      "Loss: 2.2661266326904297\n",
      "Batch :  288\n",
      "Loss: 2.3840131759643555\n",
      "Batch :  289\n",
      "Loss: 2.4053714275360107\n",
      "Batch :  290\n",
      "Loss: 2.242643117904663\n",
      "Batch :  291\n",
      "Loss: 2.2940399646759033\n",
      "Batch :  292\n",
      "Loss: 2.251436233520508\n",
      "Batch :  293\n",
      "Loss: 2.1361277103424072\n",
      "Batch :  294\n",
      "Loss: 2.3988499641418457\n",
      "Batch :  295\n",
      "Loss: 2.3739404678344727\n",
      "Batch :  296\n",
      "Loss: 2.462402582168579\n",
      "Batch :  297\n",
      "Loss: 2.3417131900787354\n",
      "Batch :  298\n",
      "Loss: 2.313227891921997\n",
      "Batch :  299\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2962043285369873\n",
      "Batch :  300\n",
      "Loss: 2.411257028579712\n",
      "Batch :  301\n",
      "Loss: 2.402548313140869\n",
      "Batch :  302\n",
      "Loss: 2.118891716003418\n",
      "Batch :  303\n",
      "Loss: 2.453585147857666\n",
      "Batch :  304\n",
      "Loss: 2.396054744720459\n",
      "Batch :  305\n",
      "Loss: 2.3147644996643066\n",
      "Batch :  306\n",
      "Loss: 2.070505380630493\n",
      "Batch :  307\n",
      "Loss: 1.9851592779159546\n",
      "Batch :  308\n",
      "Loss: 2.1464579105377197\n",
      "Batch :  309\n",
      "Loss: 2.454075574874878\n",
      "Batch :  310\n",
      "Loss: 2.314377784729004\n",
      "Batch :  311\n",
      "Loss: 2.2324864864349365\n",
      "Batch :  312\n",
      "Loss: 1.6449772119522095\n",
      "Batch - Loss: 2.3665644669304258\n",
      "Saving model\n",
      "Epoch 6\n",
      "Batch :  0\n",
      "Loss: 2.140312671661377\n",
      "Batch :  1\n",
      "Loss: 2.5062689781188965\n",
      "Batch :  2\n",
      "Loss: 2.217703104019165\n",
      "Batch :  3\n",
      "Loss: 1.8754558563232422\n",
      "Batch :  4\n",
      "Loss: 2.0199294090270996\n",
      "Batch :  5\n",
      "Loss: 2.2777631282806396\n",
      "Batch :  6\n",
      "Loss: 2.2527968883514404\n",
      "Batch :  7\n",
      "Loss: 2.09658145904541\n",
      "Batch :  8\n",
      "Loss: 2.2076005935668945\n",
      "Batch :  9\n",
      "Loss: 2.36757230758667\n",
      "Batch :  10\n",
      "Loss: 2.407931089401245\n",
      "Batch :  11\n",
      "Loss: 2.103924512863159\n",
      "Batch :  12\n",
      "Loss: 2.3534975051879883\n",
      "Batch :  13\n",
      "Loss: 2.3515050411224365\n",
      "Batch :  14\n",
      "Loss: 2.39812970161438\n",
      "Batch :  15\n",
      "Loss: 2.2539823055267334\n",
      "Batch :  16\n",
      "Loss: 2.3699893951416016\n",
      "Batch :  17\n",
      "Loss: 2.1446073055267334\n",
      "Batch :  18\n",
      "Loss: 2.314891815185547\n",
      "Batch :  19\n",
      "Loss: 2.177032232284546\n",
      "Batch :  20\n",
      "Loss: 2.21112060546875\n",
      "Batch :  21\n",
      "Loss: 2.2001585960388184\n",
      "Batch :  22\n",
      "Loss: 2.5375399589538574\n",
      "Batch :  23\n",
      "Loss: 2.2932302951812744\n",
      "Batch :  24\n",
      "Loss: 2.312434673309326\n",
      "Batch :  25\n",
      "Loss: 2.372866153717041\n",
      "Batch :  26\n",
      "Loss: 2.2449049949645996\n",
      "Batch :  27\n",
      "Loss: 2.151416063308716\n",
      "Batch :  28\n",
      "Loss: 2.543205499649048\n",
      "Batch :  29\n",
      "Loss: 2.2003302574157715\n",
      "Batch :  30\n",
      "Loss: 2.546922206878662\n",
      "Batch :  31\n",
      "Loss: 2.336799383163452\n",
      "Batch :  32\n",
      "Loss: 2.300380229949951\n",
      "Batch :  33\n",
      "Loss: 2.311903715133667\n",
      "Batch :  34\n",
      "Loss: 2.3626887798309326\n",
      "Batch :  35\n",
      "Loss: 2.3933324813842773\n",
      "Batch :  36\n",
      "Loss: 2.2404720783233643\n",
      "Batch :  37\n",
      "Loss: 2.450692892074585\n",
      "Batch :  38\n",
      "Loss: 2.383385419845581\n",
      "Batch :  39\n",
      "Loss: 2.449958324432373\n",
      "Batch :  40\n",
      "Loss: 2.2913668155670166\n",
      "Batch :  41\n",
      "Loss: 2.4439544677734375\n",
      "Batch :  42\n",
      "Loss: 2.45426607131958\n",
      "Batch :  43\n",
      "Loss: 2.251558780670166\n",
      "Batch :  44\n",
      "Loss: 2.269469738006592\n",
      "Batch :  45\n",
      "Loss: 2.300320863723755\n",
      "Batch :  46\n",
      "Loss: 2.409989833831787\n",
      "Batch :  47\n",
      "Loss: 2.1188409328460693\n",
      "Batch :  48\n",
      "Loss: 2.1157853603363037\n",
      "Batch :  49\n",
      "Loss: 2.3140547275543213\n",
      "Batch :  50\n",
      "Loss: 2.0226962566375732\n",
      "Batch :  51\n",
      "Loss: 2.3115358352661133\n",
      "Batch :  52\n",
      "Loss: 2.093327760696411\n",
      "Batch :  53\n",
      "Loss: 2.3253817558288574\n",
      "Batch :  54\n",
      "Loss: 2.428508758544922\n",
      "Batch :  55\n",
      "Loss: 2.1974668502807617\n",
      "Batch :  56\n",
      "Loss: 2.58866810798645\n",
      "Batch :  57\n",
      "Loss: 2.3845083713531494\n",
      "Batch :  58\n",
      "Loss: 2.65630841255188\n",
      "Batch :  59\n",
      "Loss: 2.2655394077301025\n",
      "Batch :  60\n",
      "Loss: 2.321500301361084\n",
      "Batch :  61\n",
      "Loss: 2.248328447341919\n",
      "Batch :  62\n",
      "Loss: 2.3348145484924316\n",
      "Batch :  63\n",
      "Loss: 2.4968836307525635\n",
      "Batch :  64\n",
      "Loss: 2.2899041175842285\n",
      "Batch :  65\n",
      "Loss: 2.4047789573669434\n",
      "Batch :  66\n",
      "Loss: 2.274271011352539\n",
      "Batch :  67\n",
      "Loss: 2.404975175857544\n",
      "Batch :  68\n",
      "Loss: 2.106505870819092\n",
      "Batch :  69\n",
      "Loss: 2.276958703994751\n",
      "Batch :  70\n",
      "Loss: 2.318347454071045\n",
      "Batch :  71\n",
      "Loss: 2.140885591506958\n",
      "Batch :  72\n",
      "Loss: 2.2107021808624268\n",
      "Batch :  73\n",
      "Loss: 2.263270616531372\n",
      "Batch :  74\n",
      "Loss: 2.5102763175964355\n",
      "Batch :  75\n",
      "Loss: 2.3475263118743896\n",
      "Batch :  76\n",
      "Loss: 2.3370890617370605\n",
      "Batch :  77\n",
      "Loss: 2.3444888591766357\n",
      "Batch :  78\n",
      "Loss: 2.752246141433716\n",
      "Batch :  79\n",
      "Loss: 2.6456992626190186\n",
      "Batch :  80\n",
      "Loss: 2.754025459289551\n",
      "Batch :  81\n",
      "Loss: 2.5895583629608154\n",
      "Batch :  82\n",
      "Loss: 2.583015203475952\n",
      "Batch :  83\n",
      "Loss: 2.4621264934539795\n",
      "Batch :  84\n",
      "Loss: 2.640829563140869\n",
      "Batch :  85\n",
      "Loss: 2.547576665878296\n",
      "Batch :  86\n",
      "Loss: 2.3639583587646484\n",
      "Batch :  87\n",
      "Loss: 2.242575168609619\n",
      "Batch :  88\n",
      "Loss: 2.2402148246765137\n",
      "Batch :  89\n",
      "Loss: 2.2822206020355225\n",
      "Batch :  90\n",
      "Loss: 2.378401517868042\n",
      "Batch :  91\n",
      "Loss: 2.3952975273132324\n",
      "Batch :  92\n",
      "Loss: 2.3018054962158203\n",
      "Batch :  93\n",
      "Loss: 2.1875877380371094\n",
      "Batch :  94\n",
      "Loss: 2.2535877227783203\n",
      "Batch :  95\n",
      "Loss: 2.5313174724578857\n",
      "Batch :  96\n",
      "Loss: 1.9994409084320068\n",
      "Batch :  97\n",
      "Loss: 2.2634382247924805\n",
      "Batch :  98\n",
      "Loss: 2.4439198970794678\n",
      "Batch :  99\n",
      "Loss: 2.4704368114471436\n",
      "Batch :  100\n",
      "Loss: 2.0831756591796875\n",
      "Batch :  101\n",
      "Loss: 2.4738547801971436\n",
      "Batch :  102\n",
      "Loss: 2.3052072525024414\n",
      "Batch :  103\n",
      "Loss: 1.9965466260910034\n",
      "Batch :  104\n",
      "Loss: 2.5272722244262695\n",
      "Batch :  105\n",
      "Loss: 2.245335817337036\n",
      "Batch :  106\n",
      "Loss: 2.563223361968994\n",
      "Batch :  107\n",
      "Loss: 2.37249493598938\n",
      "Batch :  108\n",
      "Loss: 2.242091178894043\n",
      "Batch :  109\n",
      "Loss: 2.2287254333496094\n",
      "Batch :  110\n",
      "Loss: 2.227825164794922\n",
      "Batch :  111\n",
      "Loss: 2.1586928367614746\n",
      "Batch :  112\n",
      "Loss: 2.6686670780181885\n",
      "Batch :  113\n",
      "Loss: 2.1261141300201416\n",
      "Batch :  114\n",
      "Loss: 2.3109748363494873\n",
      "Batch :  115\n",
      "Loss: 2.4752964973449707\n",
      "Batch :  116\n",
      "Loss: 2.309872627258301\n",
      "Batch :  117\n",
      "Loss: 2.4440438747406006\n",
      "Batch :  118\n",
      "Loss: 2.3804609775543213\n",
      "Batch :  119\n",
      "Loss: 2.3992152214050293\n",
      "Batch :  120\n",
      "Loss: 2.4274957180023193\n",
      "Batch :  121\n",
      "Loss: 2.5313916206359863\n",
      "Batch :  122\n",
      "Loss: 2.352456569671631\n",
      "Batch :  123\n",
      "Loss: 2.5054664611816406\n",
      "Batch :  124\n",
      "Loss: 2.3702001571655273\n",
      "Batch :  125\n",
      "Loss: 2.268465757369995\n",
      "Batch :  126\n",
      "Loss: 2.298473596572876\n",
      "Batch :  127\n",
      "Loss: 2.437863826751709\n",
      "Batch :  128\n",
      "Loss: 2.3164570331573486\n",
      "Batch :  129\n",
      "Loss: 2.463210344314575\n",
      "Batch :  130\n",
      "Loss: 2.568131446838379\n",
      "Batch :  131\n",
      "Loss: 2.4944307804107666\n",
      "Batch :  132\n",
      "Loss: 2.4097397327423096\n",
      "Batch :  133\n",
      "Loss: 2.2630093097686768\n",
      "Batch :  134\n",
      "Loss: 2.191068649291992\n",
      "Batch :  135\n",
      "Loss: 2.004502058029175\n",
      "Batch :  136\n",
      "Loss: 2.2232635021209717\n",
      "Batch :  137\n",
      "Loss: 2.321664333343506\n",
      "Batch :  138\n",
      "Loss: 2.2856998443603516\n",
      "Batch :  139\n",
      "Loss: 2.1628637313842773\n",
      "Batch :  140\n",
      "Loss: 2.1533303260803223\n",
      "Batch :  141\n",
      "Loss: 2.44934344291687\n",
      "Batch :  142\n",
      "Loss: 2.4187068939208984\n",
      "Batch :  143\n",
      "Loss: 2.523951530456543\n",
      "Batch :  144\n",
      "Loss: 2.4927477836608887\n",
      "Batch :  145\n",
      "Loss: 2.295804738998413\n",
      "Batch :  146\n",
      "Loss: 2.1561968326568604\n",
      "Batch :  147\n",
      "Loss: 2.4266629219055176\n",
      "Batch :  148\n",
      "Loss: 2.174950361251831\n",
      "Batch :  149\n",
      "Loss: 2.366933822631836\n",
      "Batch :  150\n",
      "Loss: 2.4276223182678223\n",
      "Batch :  151\n",
      "Loss: 2.3419647216796875\n",
      "Batch :  152\n",
      "Loss: 2.3947296142578125\n",
      "Batch :  153\n",
      "Loss: 2.2429566383361816\n",
      "Batch :  154\n",
      "Loss: 2.435762405395508\n",
      "Batch :  155\n",
      "Loss: 2.3124802112579346\n",
      "Batch :  156\n",
      "Loss: 2.0882270336151123\n",
      "Batch :  157\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EPOCHS = 40\n",
    "loss_flag = np.array([1,1,0,0])\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    batch_loss = 0\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    for batchNum in range(len(batch_input[0])):\n",
    "        print(\"Batch : \",batchNum)\n",
    "        t = session.run([optimizer, loss, question_loss, answer_loss, suppression_loss, expression_loss], {\n",
    "            d_tokens: batch_input[0][batchNum],\n",
    "            d_lengths: batch_input[1][batchNum],\n",
    "            a_labels: batch_input[2][batchNum],\n",
    "            encoder_input_mask: batch_input[3][batchNum],\n",
    "            encoder_lengths: batch_input[4][batchNum],\n",
    "            decoder_inputs: batch_input[5][batchNum],\n",
    "            decoder_labels: batch_input[6][batchNum],\n",
    "            decoder_lengths: batch_input[7][batchNum],\n",
    "            s_answer: batch_input[8][batchNum],\n",
    "            e_context: batch_input[9][batchNum],\n",
    "            lossFlags : loss_flag,\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(t[1]))\n",
    "        batch_loss += t[2]\n",
    "    batch_loss /= len(batch_input[0])\n",
    "    print(\"Batch - Loss: {0}\".format(batch_loss))\n",
    "    if batch_loss < minQuestionLoss:\n",
    "        loss_flag = np.array([1,1,1,1])\n",
    "    if(epoch%5 == 0):\n",
    "        print(\"Saving model\")\n",
    "        #saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(session, 'qgen-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = session.run(answer_tags, {\n",
    "    d_tokens: batch_input[0][2],\n",
    "    d_lengths: batch_input[1][2],\n",
    "})\n",
    "answers = np.argmax(answers, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    print(\"Prediction\")\n",
    "    printAllAns(answers,2,0)\n",
    "    print(\"Ground Truth\")\n",
    "    printAllAns(batch_input[2][2],2,0)\n",
    "answers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDoc(batch,num):\n",
    "    for i in batch_input[0][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "def printQues(batch,num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAnsForQuestion(batch, num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAllAns(answers, batch, num):\n",
    "    for i,word in enumerate(batch_input[0][batch][num]):\n",
    "        if answers[num][i] == 1 :\n",
    "            print(look_up_token_reduced(word),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batchNum = 2\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch_input[0][batchNum].shape[0]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    d_tokens: batch_input[0][batchNum],\n",
    "    d_lengths: batch_input[1][batchNum],\n",
    "    a_labels: batch_input[2][batchNum],\n",
    "    encoder_input_mask: batch_input[3][batchNum],\n",
    "    encoder_lengths: batch_input[4][batchNum],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[0][batchNum].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[:,:,END_TOKEN] = 0\n",
    "qs = np.argmax(questions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# q = set()\n",
    "l = []\n",
    "for i in range(batch_input[0][batchNum].shape[0]):\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, qs[i])\n",
    "    print(\"Generated Question: \" + \" \".join(look_up_token_reduced(token) for token in question))\n",
    "    print(\"Ground Truth Question: \")\n",
    "    printQues(batchNum,i)\n",
    "    print(\"Ground Truth Answer: \", X_train_ans_shuffled[batch_size*batchNum + i])\n",
    "    print(\"Context:\")\n",
    "    printDoc(batchNum,i)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[6][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(batch_input[7][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
