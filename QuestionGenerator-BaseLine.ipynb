{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "import tensorflow as tf\n",
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 50000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "1510\n",
      "rev\n",
      ".\n",
      "john\n",
      "j.\n",
      "cavanaugh\n",
      ",\n",
      "c.s.c\n",
      ".\n",
      "served\n",
      "more\n",
      "than\n",
      "half\n",
      ",\n",
      "lobund\n",
      "institute\n",
      "for\n",
      "animal\n",
      "studies\n",
      "and\n",
      "medieval\n",
      "institute\n",
      ".\n",
      "hall\n",
      "of\n",
      "liberal\n",
      "arts\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'s\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '``', 'ad', 'me', 'omnes', \"''\", '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in', '1858.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n",
      "['saint', 'bernadette', 'soubirous']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 104)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1510\n",
      "['the', 'success', 'of', 'its', 'football', 'team', 'made', 'notre', 'dame', 'a', 'household', 'name', '.', 'the', 'success', 'of', 'note', 'dame', 'reflected', 'rising', 'status', 'of', 'irish', 'americans', 'and', 'catholics', 'in', 'the', '1920s', '.', 'catholics', 'rallied', 'up', 'around', 'the', 'team', 'and', 'listen', 'to', 'the', 'games', 'on', 'the', 'radio', ',', 'especially', 'when', 'it', 'knocked', 'off', 'the', 'schools', 'that', 'symbolized', 'the', 'protestant', 'establishment', 'in', 'america', 'â€”', 'harvard', ',', 'yale', ',', 'princeton', ',', 'and', 'army', '.', 'yet', 'this', 'role', 'as', 'high-profile', 'flagship', 'institution', 'of', 'catholicism', 'made', 'it', 'an', 'easy', 'target', 'of', 'anti-catholicism', '.', 'the', 'most', 'remarkable', 'episode', 'of', 'violence', 'was', 'the', 'clash', 'between', 'notre', 'dame', 'students', 'and', 'the', 'ku', 'klux', 'klan', 'in', '1924.', 'nativism', 'and', 'anti-catholicism', ',', 'especially', 'when', 'directed', 'towards', 'immigrants', ',', 'were', 'cornerstones', 'of', 'the', 'kkk', \"'s\", 'rhetoric', ',', 'and', 'notre', 'dame', 'was', 'seen', 'as', 'a', 'symbol', 'of', 'the', 'threat', 'posed', 'by', 'the', 'catholic', 'church', '.', 'the', 'klan', 'decided', 'to', 'have', 'a', 'week-long', 'klavern', 'in', 'south', 'bend', '.', 'clashes', 'with', 'the', 'student', 'body', 'started', 'on', 'march', '17', ',', 'when', 'students', ',', 'aware', 'of', 'the', 'anti-catholic', 'animosity', ',', 'blocked', 'the', 'klansmen', 'from', 'descending', 'from', 'their', 'trains', 'in', 'the', 'south', 'bend', 'station', 'and', 'ripped', 'the', 'kkk', 'clothes', 'and', 'regalia', '.', 'on', 'may', '19', 'thousands', 'of', 'students', 'massed', 'downtown', 'protesting', 'the', 'klavern', ',', 'and', 'only', 'the', 'arrival', 'of', 'college', 'president', 'fr', '.', 'matthew', 'walsh', 'prevented', 'any', 'further', 'clashes', '.', 'the', 'next', 'day', ',', 'football', 'coach', 'knute', 'rockne', 'spoke', 'at', 'a', 'campus', 'rally', 'and', 'implored', 'the', 'students', 'to', 'obey', 'the', 'college', 'president', 'and', 'refrain', 'from', 'further', 'violence', '.', 'a', 'few', 'days', 'later', 'the', 'klavern', 'broke', 'up', ',', 'but', 'the', 'hostility', 'shown', 'by', 'the', 'students', 'was', 'an', 'omen', 'and', 'a', 'contribution', 'to', 'the', 'downfall', 'of', 'the', 'kkk', 'in', 'indiana', '.']\n",
      "['the', 'ku', 'klux', 'klan']\n",
      "['notre', 'dame', 'students', 'had', 'a', 'showdown', 'in', '1924', 'with', 'which', 'anti-catholic', 'group', '?']\n",
      "['air', 'defence', 'in', 'naval', 'tactics', ',', 'especially', 'within', 'a', 'carrier', 'group', ',', 'is', 'often', 'built', 'around', 'a', 'system', 'of', 'concentric', 'layers', 'with', 'the', 'aircraft', 'carrier', 'at', 'the', 'centre', '.', 'the', 'outer', 'layer', 'will', 'usually', 'be', 'provided', 'by', 'the', 'carrier', \"'s\", 'aircraft', ',', 'specifically', 'its', 'aew', '&', 'c', 'aircraft', 'combined', 'with', 'the', 'cap', '.', 'if', 'an', 'attacker', 'is', 'able', 'to', 'penetrate', 'this', 'layer', ',', 'then', 'the', 'next', 'layers', 'would', 'come', 'from', 'the', 'surface-to-air', 'missiles', 'carried', 'by', 'the', 'carrier', \"'s\", 'escorts', ';', 'the', 'area-defence', 'missiles', ',', 'such', 'as', 'the', 'rim-67', 'standard', ',', 'with', 'a', 'range', 'of', 'up', 'to', '100', 'nmi', ',', 'and', 'the', 'point-defence', 'missiles', ',', 'like', 'the', 'rim-162', 'essm', ',', 'with', 'a', 'range', 'of', 'up', 'to', '30', 'nmi', '.', 'finally', ',', 'virtually', 'every', 'modern', 'warship', 'will', 'be', 'fitted', 'with', 'small-calibre', 'guns', ',', 'including', 'a', 'ciws', ',', 'which', 'is', 'usually', 'a', 'radar-controlled', 'gatling', 'gun', 'of', 'between', '20mm', 'and', '30mm', 'calibre', 'capable', 'of', 'firing', 'several', 'thousand', 'rounds', 'per', 'minute', '.']\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "['aew', '&', 'c', 'aircraft', 'combined', 'with', 'the', 'cap']\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.\n",
      "  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.  1.  1.  1.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  1.  1.  1.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.  1.  1.  1.  1.  1.]\n",
      "['what', 'protects', 'the', 'outer', 'layer', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 10000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 50000, 1)\n"
     ]
    }
   ],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "print(suppression_answer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 766)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 300)\n",
      "62106\n",
      "5834\n",
      "59802\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   22,  1795,   385, 19053,     4,  2629,  4553,  7562, 30963,\n",
       "          35,  3788, 16025,   139,     3,  2592,     5, 24499,    31,\n",
       "           3,  4237,     5,     3,    65,  4329,   104,     4,     3,\n",
       "        2493,   167,     8, 19252,     6,     3, 16025,  6960,     7,\n",
       "        5149,     3,   167,     7,     3,  5646,     4,    51,  2760,\n",
       "         482,     3,  2592,     5, 24499,     7,  7018,  1734,    91,\n",
       "       28738,     4,   601,  4932,     4,     7,     3,   700,     5,\n",
       "           3,   911,     6,    48,    85,   110,  2355,   464,    22,\n",
       "           3,   545,  9146,     9,  2582,   765,     4,     3, 23165,\n",
       "           5,     7,  9146,     9,     3,  4329,     4,     3, 20042,\n",
       "           4,  5304,  8759, 14065,     4, 15324,     4,   338,    22,\n",
       "           3,   700,     5,     3,   911,     4,     3,  5357,   157,\n",
       "           4, 22835,     7, 13628,     4,     3,   393,     5,  2276,\n",
       "        2427,     8,   277,  2642,     4,     3,   855,    73,  1376,\n",
       "           7,     3,   157,     4,   746,   464,     4,     7,     3,\n",
       "       20042,     6,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,\n",
       "           2,     2,     2,     2,     2,     2,     2,     2,     2,     2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 766)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellFlag = 'LSTM'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "embedding = tf.get_variable(\"embedding\", initializer=reduced_glove)\n",
    "embedding = tf.cast(embedding, dtype=tf.float32)\n",
    "EMBEDDING_DIMENS = glove.shape[1]\n",
    "\n",
    "d_tokens = tf.placeholder(tf.int32, shape=[None, None], name=\"d_tokens\")\n",
    "d_lengths = tf.placeholder(tf.int32, shape=[None], name=\"d_lengths\")\n",
    "\n",
    "\n",
    "document_emb = tf.nn.embedding_lookup(embedding, d_tokens, name=\"document_emb\")\n",
    "document_emb = tf.cast(document_emb, dtype=tf.float64, name=\"casted_document_emb\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cellFlag == 'LSTM':\n",
    "    forward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "elif cellFlag == 'GRU':\n",
    "    forward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "\n",
    "answer_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, document_emb, d_lengths, dtype=tf.float64,\n",
    "    scope=\"answer_rnn\")\n",
    "\n",
    "answer_outputs = tf.concat(answer_outputs, 2, name=\"answer_output_concat\")\n",
    "\n",
    "answer_outputs = tf.cast(answer_outputs,tf.float32, name=\"answer_output_concat\")\n",
    "\n",
    "answer_tags = tf.layers.dense(inputs=answer_outputs, units=2, name=\"answer_tags\")\n",
    "\n",
    "\n",
    "a_labels = tf.placeholder(tf.int32, shape=[None, None], name=\"a_labels\")\n",
    "\n",
    "answer_mask = tf.sequence_mask(d_lengths, dtype=tf.float32, name=\"answer_mask\")\n",
    "\n",
    "answer_loss = seq2seq.sequence_loss(\n",
    "    logits=answer_tags, targets=a_labels, weights=answer_mask, name=\"answer_loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_mask = tf.placeholder(\n",
    "    tf.float32, shape=[None, None, None], name=\"encoder_input_mask\")\n",
    "encoder_inputs = tf.matmul(encoder_input_mask, answer_outputs, name=\"encoder_inputs\")\n",
    "encoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"encoder_lengths\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    encoder_cell = tf.contrib.rnn.GRUCell(forward_cell.state_size + backward_cell.state_size)\n",
    "elif cellFlag == 'LSTM':\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder_rnn/while/Exit_3:0\", shape=(?, 600), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(encoder_state.h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs, encoder_lengths, dtype=tf.float32, scope=\"encoder_rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMStateTuple(c=600, h=600)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_cell.state_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "\n",
    "decoder_inputs = tf.placeholder(tf.int32, shape=[None, None], name=\"decoder_inputs\")\n",
    "decoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"decoder_lengths\")\n",
    "\n",
    "decoder_emb = tf.nn.embedding_lookup(embedding, decoder_inputs,name=\"decoder_embedding\")\n",
    "decoder_emb = tf.cast(decoder_emb,tf.float32,name=\"decoder_embedding_cast\")\n",
    "\n",
    "helper = seq2seq.TrainingHelper(decoder_emb , decoder_lengths, name=\"helper\")\n",
    "\n",
    "\n",
    "projection = Dense(embedding.shape[0], use_bias=False, name=\"projection\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    decoder_cell = tf.contrib.rnn.GRUCell(encoder_cell.state_size)\n",
    "elif cellFlag == \"LSTM\":\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, scope=\"decoder\")\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "decoder_labels = tf.placeholder(tf.int64, shape=[None, None], name=\"decoder_labels\")\n",
    "question_mask = tf.sequence_mask(decoder_lengths ,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL Loss\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "\n",
    "\n",
    "#Suppression Loss\n",
    "s_answer = tf.placeholder(tf.float32, shape=[None,None,None], name=\"suppression_answer\")\n",
    "lambdaSuppress = 0.5\n",
    "decoder_outputs = tf.Print(decoder_outputs, [decoder_outputs], message=\"This is decoder_outputs: \")\n",
    "suppression_loss = lambdaSuppress * tf.reduce_sum(tf.matmul(tf.exp(decoder_outputs), s_answer))\n",
    "suppression_loss = tf.Print(suppression_loss, [suppression_loss], message=\"This is suppression_loss: \")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Maximize Entropy Loss\n",
    "entropy_loss = tf.matmul(tf.transpose(decoder_outputs),decoder_outputs)\n",
    "print(decoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.add(answer_loss, question_loss, name=\"loss\")\n",
    "loss = tf.add(loss, suppression_loss, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train,max_answer_len),dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(*ls):\n",
    "    l =list(zip(*ls))\n",
    "    np.random.shuffle(l)\n",
    "    return zip(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index == 8: # \n",
    "                output.append(inp[start:start+batch_size,:,:]) \n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        maxD = max(inputs[1][start:])\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ]) \n",
    "        elif index ==8:\n",
    "            output.append(inp[start:,:, :]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer]\n",
    "                    ,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('No of batches:', 313)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"No of batches:\",len( batch_input[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saved_vars = []\n",
    "l = len(tf.all_variables())\n",
    "for i,var in enumerate(tf.all_variables()):\n",
    "    print(i,\"/\",l)\n",
    "    saved_vars.append(var)\n",
    "        \n",
    "print(len(saved_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Batch :  0\n",
      "Loss: 1069.5816650390625\n",
      "Batch :  1\n",
      "Loss: 977.320556640625\n",
      "Batch :  2\n",
      "Loss: 419.386962890625\n",
      "Batch :  3\n",
      "Loss: 819.1289672851562\n",
      "Batch :  4\n",
      "Loss: 406.09136962890625\n",
      "Batch :  5\n",
      "Loss: 393.1398620605469\n",
      "Batch :  6\n",
      "Loss: 245.859130859375\n",
      "Batch :  7\n",
      "Loss: 285.4755859375\n",
      "Batch :  8\n",
      "Loss: 224.34027099609375\n",
      "Batch :  9\n",
      "Loss: 58.62446212768555\n",
      "Batch :  10\n",
      "Loss: 254.43963623046875\n",
      "Batch :  11\n",
      "Loss: 141.78228759765625\n",
      "Batch :  12\n",
      "Loss: 73.60932922363281\n",
      "Batch :  13\n",
      "Loss: 301.4520263671875\n",
      "Batch :  14\n",
      "Loss: 116.12511444091797\n",
      "Batch :  15\n",
      "Loss: 55.39506912231445\n",
      "Batch :  16\n",
      "Loss: 222.0098876953125\n",
      "Batch :  17\n",
      "Loss: 82.1900634765625\n",
      "Batch :  18\n",
      "Loss: 78.52726745605469\n",
      "Batch :  19\n",
      "Loss: 56.948158264160156\n",
      "Batch :  20\n",
      "Loss: 62.54273223876953\n",
      "Batch :  21\n",
      "Loss: 45.830867767333984\n",
      "Batch :  22\n",
      "Loss: 85.80056762695312\n",
      "Batch :  23\n",
      "Loss: 19.57417869567871\n",
      "Batch :  24\n",
      "Loss: 33.038509368896484\n",
      "Batch :  25\n",
      "Loss: 43.18663787841797\n",
      "Batch :  26\n",
      "Loss: 61.19582748413086\n",
      "Batch :  27\n",
      "Loss: 30.80021858215332\n",
      "Batch :  28\n",
      "Loss: 30.430686950683594\n",
      "Batch :  29\n",
      "Loss: 25.219207763671875\n",
      "Batch :  30\n",
      "Loss: 13.124004364013672\n",
      "Batch :  31\n",
      "Loss: 15.941400527954102\n",
      "Batch :  32\n",
      "Loss: 25.552520751953125\n",
      "Batch :  33\n",
      "Loss: 13.675737380981445\n",
      "Batch :  34\n",
      "Loss: 15.957218170166016\n",
      "Batch :  35\n",
      "Loss: 14.01059341430664\n",
      "Batch :  36\n",
      "Loss: 12.509970664978027\n",
      "Batch :  37\n",
      "Loss: 15.714960098266602\n",
      "Batch :  38\n",
      "Loss: 11.33695125579834\n",
      "Batch :  39\n",
      "Loss: 10.803255081176758\n",
      "Batch :  40\n",
      "Loss: 13.406636238098145\n",
      "Batch :  41\n",
      "Loss: 11.478189468383789\n",
      "Batch :  42\n",
      "Loss: 10.726788520812988\n",
      "Batch :  43\n",
      "Loss: 10.068269729614258\n",
      "Batch :  44\n",
      "Loss: 12.029891014099121\n",
      "Batch :  45\n",
      "Loss: 11.872663497924805\n",
      "Batch :  46\n",
      "Loss: 10.83910846710205\n",
      "Batch :  47\n",
      "Loss: 11.235491752624512\n",
      "Batch :  48\n",
      "Loss: 11.33191967010498\n",
      "Batch :  49\n",
      "Loss: 10.428071022033691\n",
      "Batch :  50\n",
      "Loss: 12.779891014099121\n",
      "Batch :  51\n",
      "Loss: 14.385969161987305\n",
      "Batch :  52\n",
      "Loss: 11.510531425476074\n",
      "Batch :  53\n",
      "Loss: 10.69678783416748\n",
      "Batch :  54\n",
      "Loss: 10.663616180419922\n",
      "Batch :  55\n",
      "Loss: 10.25935173034668\n",
      "Batch :  56\n",
      "Loss: 10.279224395751953\n",
      "Batch :  57\n",
      "Loss: 13.818605422973633\n",
      "Batch :  58\n",
      "Loss: 11.873677253723145\n",
      "Batch :  59\n",
      "Loss: 10.190220832824707\n",
      "Batch :  60\n",
      "Loss: 9.885711669921875\n",
      "Batch :  61\n",
      "Loss: 10.296638488769531\n",
      "Batch :  62\n",
      "Loss: 9.994065284729004\n",
      "Batch :  63\n",
      "Loss: 10.480388641357422\n",
      "Batch :  64\n",
      "Loss: 10.073774337768555\n",
      "Batch :  65\n",
      "Loss: 9.684358596801758\n",
      "Batch :  66\n",
      "Loss: 10.047025680541992\n",
      "Batch :  67\n",
      "Loss: 9.3524169921875\n",
      "Batch :  68\n",
      "Loss: 10.186365127563477\n",
      "Batch :  69\n",
      "Loss: 10.31808090209961\n",
      "Batch :  70\n",
      "Loss: 10.003972053527832\n",
      "Batch :  71\n",
      "Loss: 9.870405197143555\n",
      "Batch :  72\n",
      "Loss: 9.628812789916992\n",
      "Batch :  73\n",
      "Loss: 8.935585021972656\n",
      "Batch :  74\n",
      "Loss: 8.934745788574219\n",
      "Batch :  75\n",
      "Loss: 9.27735710144043\n",
      "Batch :  76\n",
      "Loss: 9.128417015075684\n",
      "Batch :  77\n",
      "Loss: 9.332629203796387\n",
      "Batch :  78\n",
      "Loss: 9.688051223754883\n",
      "Batch :  79\n",
      "Loss: 9.465519905090332\n",
      "Batch :  80\n",
      "Loss: 8.902674674987793\n",
      "Batch :  81\n",
      "Loss: 9.886893272399902\n",
      "Batch :  82\n",
      "Loss: 9.139742851257324\n",
      "Batch :  83\n",
      "Loss: 8.772355079650879\n",
      "Batch :  84\n",
      "Loss: 8.960053443908691\n",
      "Batch :  85\n",
      "Loss: 9.016210556030273\n",
      "Batch :  86\n",
      "Loss: 10.459355354309082\n",
      "Batch :  87\n",
      "Loss: 9.124631881713867\n",
      "Batch :  88\n",
      "Loss: 9.03258991241455\n",
      "Batch :  89\n",
      "Loss: 9.024072647094727\n",
      "Batch :  90\n",
      "Loss: 9.54187297821045\n",
      "Batch :  91\n",
      "Loss: 8.80616569519043\n",
      "Batch :  92\n",
      "Loss: 9.271939277648926\n",
      "Batch :  93\n",
      "Loss: 8.910334587097168\n",
      "Batch :  94\n",
      "Loss: 8.890661239624023\n",
      "Batch :  95\n",
      "Loss: 9.102897644042969\n",
      "Batch :  96\n",
      "Loss: 9.084946632385254\n",
      "Batch :  97\n",
      "Loss: 8.485394477844238\n",
      "Batch :  98\n",
      "Loss: 8.67020320892334\n",
      "Batch :  99\n",
      "Loss: 8.387398719787598\n",
      "Batch :  100\n",
      "Loss: 9.087540626525879\n",
      "Batch :  101\n",
      "Loss: 8.549772262573242\n",
      "Batch :  102\n",
      "Loss: 9.238326072692871\n",
      "Batch :  103\n",
      "Loss: 8.466163635253906\n",
      "Batch :  104\n",
      "Loss: 8.8036470413208\n",
      "Batch :  105\n",
      "Loss: 8.76553726196289\n",
      "Batch :  106\n",
      "Loss: 8.398126602172852\n",
      "Batch :  107\n",
      "Loss: 9.098641395568848\n",
      "Batch :  108\n",
      "Loss: 8.286924362182617\n",
      "Batch :  109\n",
      "Loss: 8.911922454833984\n",
      "Batch :  110\n",
      "Loss: 8.680208206176758\n",
      "Batch :  111\n",
      "Loss: 8.682796478271484\n",
      "Batch :  112\n",
      "Loss: 8.385397911071777\n",
      "Batch :  113\n",
      "Loss: 8.168608665466309\n",
      "Batch :  114\n",
      "Loss: 8.531438827514648\n",
      "Batch :  115\n",
      "Loss: 8.047928810119629\n",
      "Batch :  116\n",
      "Loss: 8.54830265045166\n",
      "Batch :  117\n",
      "Loss: 8.327471733093262\n",
      "Batch :  118\n",
      "Loss: 7.962161064147949\n",
      "Batch :  119\n",
      "Loss: 7.856678009033203\n",
      "Batch :  120\n",
      "Loss: 7.923849105834961\n",
      "Batch :  121\n",
      "Loss: 7.732223033905029\n",
      "Batch :  122\n",
      "Loss: 8.338462829589844\n",
      "Batch :  123\n",
      "Loss: 8.304577827453613\n",
      "Batch :  124\n",
      "Loss: 8.142904281616211\n",
      "Batch :  125\n",
      "Loss: 7.934488296508789\n",
      "Batch :  126\n",
      "Loss: 8.176076889038086\n",
      "Batch :  127\n",
      "Loss: 8.667801856994629\n",
      "Batch :  128\n",
      "Loss: 7.89448881149292\n",
      "Batch :  129\n",
      "Loss: 7.980177879333496\n",
      "Batch :  130\n",
      "Loss: 8.614795684814453\n",
      "Batch :  131\n",
      "Loss: 7.6591081619262695\n",
      "Batch :  132\n",
      "Loss: 8.839803695678711\n",
      "Batch :  133\n",
      "Loss: 8.110291481018066\n",
      "Batch :  134\n",
      "Loss: 7.685697078704834\n",
      "Batch :  135\n",
      "Loss: 7.834448337554932\n",
      "Batch :  136\n",
      "Loss: 7.631844997406006\n",
      "Batch :  137\n",
      "Loss: 8.070394515991211\n",
      "Batch :  138\n",
      "Loss: 8.178962707519531\n",
      "Batch :  139\n",
      "Loss: 8.198148727416992\n",
      "Batch :  140\n",
      "Loss: 7.7288289070129395\n",
      "Batch :  141\n",
      "Loss: 8.193757057189941\n",
      "Batch :  142\n",
      "Loss: 8.131902694702148\n",
      "Batch :  143\n",
      "Loss: 7.800851821899414\n",
      "Batch :  144\n",
      "Loss: 8.756399154663086\n",
      "Batch :  145\n",
      "Loss: 8.030096054077148\n",
      "Batch :  146\n",
      "Loss: 8.145594596862793\n",
      "Batch :  147\n",
      "Loss: 8.166119575500488\n",
      "Batch :  148\n",
      "Loss: 8.50313949584961\n",
      "Batch :  149\n",
      "Loss: 8.050548553466797\n",
      "Batch :  150\n",
      "Loss: 7.991335391998291\n",
      "Batch :  151\n",
      "Loss: 7.865800857543945\n",
      "Batch :  152\n",
      "Loss: 8.008002281188965\n",
      "Batch :  153\n",
      "Loss: 7.785142421722412\n",
      "Batch :  154\n",
      "Loss: 8.440339088439941\n",
      "Batch :  155\n",
      "Loss: 8.121928215026855\n",
      "Batch :  156\n",
      "Loss: 7.797175407409668\n",
      "Batch :  157\n",
      "Loss: 7.756202220916748\n",
      "Batch :  158\n",
      "Loss: 8.158756256103516\n",
      "Batch :  159\n",
      "Loss: 7.290487289428711\n",
      "Batch :  160\n",
      "Loss: 7.819794654846191\n",
      "Batch :  161\n",
      "Loss: 7.8765716552734375\n",
      "Batch :  162\n",
      "Loss: 8.114266395568848\n",
      "Batch :  163\n",
      "Loss: 7.826141357421875\n",
      "Batch :  164\n",
      "Loss: 7.685741901397705\n",
      "Batch :  165\n",
      "Loss: 8.058683395385742\n",
      "Batch :  166\n",
      "Loss: 8.18528938293457\n",
      "Batch :  167\n",
      "Loss: 7.348488807678223\n",
      "Batch :  168\n",
      "Loss: 7.225694179534912\n",
      "Batch :  169\n",
      "Loss: 8.282663345336914\n",
      "Batch :  170\n",
      "Loss: 7.392401695251465\n",
      "Batch :  171\n",
      "Loss: 7.74871826171875\n",
      "Batch :  172\n",
      "Loss: 7.796055793762207\n",
      "Batch :  173\n",
      "Loss: 7.929276943206787\n",
      "Batch :  174\n",
      "Loss: 7.779770851135254\n",
      "Batch :  175\n",
      "Loss: 7.450793266296387\n",
      "Batch :  176\n",
      "Loss: 7.723343372344971\n",
      "Batch :  177\n",
      "Loss: 7.8288397789001465\n",
      "Batch :  178\n",
      "Loss: 7.805771350860596\n",
      "Batch :  179\n",
      "Loss: 7.524040699005127\n",
      "Batch :  180\n",
      "Loss: 7.98198127746582\n",
      "Batch :  181\n",
      "Loss: 7.419041156768799\n",
      "Batch :  182\n",
      "Loss: 7.8284783363342285\n",
      "Batch :  183\n",
      "Loss: 7.478076457977295\n",
      "Batch :  184\n",
      "Loss: 7.399316310882568\n",
      "Batch :  185\n",
      "Loss: 7.759599208831787\n",
      "Batch :  186\n",
      "Loss: 7.220201015472412\n",
      "Batch :  187\n",
      "Loss: 7.672468185424805\n",
      "Batch :  188\n",
      "Loss: 8.023736953735352\n",
      "Batch :  189\n",
      "Loss: 7.42946720123291\n",
      "Batch :  190\n",
      "Loss: 7.692564010620117\n",
      "Batch :  191\n",
      "Loss: 7.975518703460693\n",
      "Batch :  192\n",
      "Loss: 7.53213357925415\n",
      "Batch :  193\n",
      "Loss: 7.149827480316162\n",
      "Batch :  194\n",
      "Loss: 7.733063697814941\n",
      "Batch :  195\n",
      "Loss: 7.584357261657715\n",
      "Batch :  196\n",
      "Loss: 7.532736301422119\n",
      "Batch :  197\n",
      "Loss: 8.003706932067871\n",
      "Batch :  198\n",
      "Loss: 7.618490695953369\n",
      "Batch :  199\n",
      "Loss: 7.6860175132751465\n",
      "Batch :  200\n",
      "Loss: 7.40880012512207\n",
      "Batch :  201\n",
      "Loss: 7.200442790985107\n",
      "Batch :  202\n",
      "Loss: 7.803788661956787\n",
      "Batch :  203\n",
      "Loss: 7.674897193908691\n",
      "Batch :  204\n",
      "Loss: 7.290482521057129\n",
      "Batch :  205\n",
      "Loss: 7.390129089355469\n",
      "Batch :  206\n",
      "Loss: 7.9144697189331055\n",
      "Batch :  207\n",
      "Loss: 7.1697516441345215\n",
      "Batch :  208\n",
      "Loss: 7.407076835632324\n",
      "Batch :  209\n",
      "Loss: 7.785183906555176\n",
      "Batch :  210\n",
      "Loss: 7.900733470916748\n",
      "Batch :  211\n",
      "Loss: 7.660765171051025\n",
      "Batch :  212\n",
      "Loss: 7.441903114318848\n",
      "Batch :  213\n",
      "Loss: 7.4660468101501465\n",
      "Batch :  214\n",
      "Loss: 7.5035881996154785\n",
      "Batch :  215\n",
      "Loss: 7.446351051330566\n",
      "Batch :  216\n",
      "Loss: 7.490532875061035\n",
      "Batch :  217\n",
      "Loss: 7.590871810913086\n",
      "Batch :  218\n",
      "Loss: 7.207264423370361\n",
      "Batch :  219\n",
      "Loss: 7.173399448394775\n",
      "Batch :  220\n",
      "Loss: 7.675034523010254\n",
      "Batch :  221\n",
      "Loss: 7.74869966506958\n",
      "Batch :  222\n",
      "Loss: 7.54093074798584\n",
      "Batch :  223\n",
      "Loss: 7.322507858276367\n",
      "Batch :  224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 7.141891956329346\n",
      "Batch :  225\n",
      "Loss: 7.27922248840332\n",
      "Batch :  226\n",
      "Loss: 7.382074356079102\n",
      "Batch :  227\n",
      "Loss: 7.54067325592041\n",
      "Batch :  228\n",
      "Loss: 7.6405720710754395\n",
      "Batch :  229\n",
      "Loss: 7.702033042907715\n",
      "Batch :  230\n",
      "Loss: 7.076685905456543\n",
      "Batch :  231\n",
      "Loss: 7.32241678237915\n",
      "Batch :  232\n",
      "Loss: 7.936300277709961\n",
      "Batch :  233\n",
      "Loss: 7.358598709106445\n",
      "Batch :  234\n",
      "Loss: 7.489475250244141\n",
      "Batch :  235\n",
      "Loss: 7.19539213180542\n",
      "Batch :  236\n",
      "Loss: 7.758790016174316\n",
      "Batch :  237\n",
      "Loss: 8.009986877441406\n",
      "Batch :  238\n",
      "Loss: 7.802896499633789\n",
      "Batch :  239\n",
      "Loss: 6.998081684112549\n",
      "Batch :  240\n",
      "Loss: 7.184821128845215\n",
      "Batch :  241\n",
      "Loss: 7.107460975646973\n",
      "Batch :  242\n",
      "Loss: 7.66880464553833\n",
      "Batch :  243\n",
      "Loss: 7.034924030303955\n",
      "Batch :  244\n",
      "Loss: 7.411591053009033\n",
      "Batch :  245\n",
      "Loss: 7.551497459411621\n",
      "Batch :  246\n",
      "Loss: 7.055408000946045\n",
      "Batch :  247\n",
      "Loss: 7.2994561195373535\n",
      "Batch :  248\n",
      "Loss: 7.537991523742676\n",
      "Batch :  249\n",
      "Loss: 7.435757637023926\n",
      "Batch :  250\n",
      "Loss: 7.13217306137085\n",
      "Batch :  251\n",
      "Loss: 7.5008769035339355\n",
      "Batch :  252\n",
      "Loss: 8.364673614501953\n",
      "Batch :  253\n",
      "Loss: 7.307641506195068\n",
      "Batch :  254\n",
      "Loss: 7.201327323913574\n",
      "Batch :  255\n",
      "Loss: 7.2177910804748535\n",
      "Batch :  256\n",
      "Loss: 7.814838886260986\n",
      "Batch :  257\n",
      "Loss: 7.740360260009766\n",
      "Batch :  258\n",
      "Loss: 7.7076005935668945\n",
      "Batch :  259\n",
      "Loss: 7.348976135253906\n",
      "Batch :  260\n",
      "Loss: 7.489649295806885\n",
      "Batch :  261\n",
      "Loss: 7.332343101501465\n",
      "Batch :  262\n",
      "Loss: 7.30170202255249\n",
      "Batch :  263\n",
      "Loss: 7.454248905181885\n",
      "Batch :  264\n",
      "Loss: 7.182575702667236\n",
      "Batch :  265\n",
      "Loss: 7.5163493156433105\n",
      "Batch :  266\n",
      "Loss: 7.396274566650391\n",
      "Batch :  267\n",
      "Loss: 7.347692012786865\n",
      "Batch :  268\n",
      "Loss: 7.406756401062012\n",
      "Batch :  269\n",
      "Loss: 7.651689529418945\n",
      "Batch :  270\n",
      "Loss: 6.9147419929504395\n",
      "Batch :  271\n",
      "Loss: 7.287363529205322\n",
      "Batch :  272\n",
      "Loss: 7.360053062438965\n",
      "Batch :  273\n",
      "Loss: 7.264634609222412\n",
      "Batch :  274\n",
      "Loss: 7.520321846008301\n",
      "Batch :  275\n",
      "Loss: 7.732729911804199\n",
      "Batch :  276\n",
      "Loss: 6.892622470855713\n",
      "Batch :  277\n",
      "Loss: 7.344167232513428\n",
      "Batch :  278\n",
      "Loss: 7.3485822677612305\n",
      "Batch :  279\n",
      "Loss: 7.209025859832764\n",
      "Batch :  280\n",
      "Loss: 7.912593364715576\n",
      "Batch :  281\n",
      "Loss: 7.168185710906982\n",
      "Batch :  282\n",
      "Loss: 7.166965484619141\n",
      "Batch :  283\n",
      "Loss: 7.5542216300964355\n",
      "Batch :  284\n",
      "Loss: 7.368837356567383\n",
      "Batch :  285\n",
      "Loss: 7.433711051940918\n",
      "Batch :  286\n",
      "Loss: 7.610337734222412\n",
      "Batch :  287\n",
      "Loss: 7.292332649230957\n",
      "Batch :  288\n",
      "Loss: 7.386960506439209\n",
      "Batch :  289\n",
      "Loss: 7.010680198669434\n",
      "Batch :  290\n",
      "Loss: 7.212094306945801\n",
      "Batch :  291\n",
      "Loss: 7.40127420425415\n",
      "Batch :  292\n",
      "Loss: 7.275351524353027\n",
      "Batch :  293\n",
      "Loss: 6.975888729095459\n",
      "Batch :  294\n",
      "Loss: 6.823385238647461\n",
      "Batch :  295\n",
      "Loss: 7.3875956535339355\n",
      "Batch :  296\n",
      "Loss: 7.058675765991211\n",
      "Batch :  297\n",
      "Loss: 6.798255443572998\n",
      "Batch :  298\n",
      "Loss: 6.999172687530518\n",
      "Batch :  299\n",
      "Loss: 6.844644546508789\n",
      "Batch :  300\n",
      "Loss: 7.073492050170898\n",
      "Batch :  301\n",
      "Loss: 7.208803176879883\n",
      "Batch :  302\n",
      "Loss: 6.97719144821167\n",
      "Batch :  303\n",
      "Loss: 7.028719425201416\n",
      "Batch :  304\n",
      "Loss: 6.737930774688721\n",
      "Batch :  305\n",
      "Loss: 6.845107555389404\n",
      "Batch :  306\n",
      "Loss: 7.080633640289307\n",
      "Batch :  307\n",
      "Loss: 6.58466100692749\n",
      "Batch :  308\n",
      "Loss: 7.443665981292725\n",
      "Batch :  309\n",
      "Loss: 7.2768874168396\n",
      "Batch :  310\n",
      "Loss: 7.099698066711426\n",
      "Batch :  311\n",
      "Loss: 7.143216133117676\n",
      "Batch :  312\n",
      "Loss: 7.987175941467285\n",
      "Epoch 2\n",
      "Batch :  0\n",
      "Loss: 6.81371545791626\n",
      "Batch :  1\n",
      "Loss: 7.100931167602539\n",
      "Batch :  2\n",
      "Loss: 6.7832231521606445\n",
      "Batch :  3\n",
      "Loss: 7.005591869354248\n",
      "Batch :  4\n",
      "Loss: 7.342940330505371\n",
      "Batch :  5\n",
      "Loss: 6.7152509689331055\n",
      "Batch :  6\n",
      "Loss: 7.012344837188721\n",
      "Batch :  7\n",
      "Loss: 7.211039066314697\n",
      "Batch :  8\n",
      "Loss: 7.375027656555176\n",
      "Batch :  9\n",
      "Loss: 7.064749717712402\n",
      "Batch :  10\n",
      "Loss: 7.054429531097412\n",
      "Batch :  11\n",
      "Loss: 6.828675270080566\n",
      "Batch :  12\n",
      "Loss: 7.06419563293457\n",
      "Batch :  13\n",
      "Loss: 7.1908860206604\n",
      "Batch :  14\n",
      "Loss: 6.8776326179504395\n",
      "Batch :  15\n",
      "Loss: 6.338761806488037\n",
      "Batch :  16\n",
      "Loss: 7.133303642272949\n",
      "Batch :  17\n",
      "Loss: 6.914844036102295\n",
      "Batch :  18\n",
      "Loss: 7.02921199798584\n",
      "Batch :  19\n",
      "Loss: 7.1903581619262695\n",
      "Batch :  20\n",
      "Loss: 7.008872032165527\n",
      "Batch :  21\n",
      "Loss: 6.795442581176758\n",
      "Batch :  22\n",
      "Loss: 6.798452377319336\n",
      "Batch :  23\n",
      "Loss: 6.623137950897217\n",
      "Batch :  24\n",
      "Loss: 6.951641082763672\n",
      "Batch :  25\n",
      "Loss: 6.879176616668701\n",
      "Batch :  26\n",
      "Loss: 6.568977355957031\n",
      "Batch :  27\n",
      "Loss: 7.286600589752197\n",
      "Batch :  28\n",
      "Loss: 6.858401775360107\n",
      "Batch :  29\n",
      "Loss: 6.7626237869262695\n",
      "Batch :  30\n",
      "Loss: 6.799447536468506\n",
      "Batch :  31\n",
      "Loss: 7.257699012756348\n",
      "Batch :  32\n",
      "Loss: 7.115440368652344\n",
      "Batch :  33\n",
      "Loss: 6.816562175750732\n",
      "Batch :  34\n",
      "Loss: 6.632612705230713\n",
      "Batch :  35\n",
      "Loss: 6.675117492675781\n",
      "Batch :  36\n",
      "Loss: 6.654654026031494\n",
      "Batch :  37\n",
      "Loss: 6.901334762573242\n",
      "Batch :  38\n",
      "Loss: 6.954120635986328\n",
      "Batch :  39\n",
      "Loss: 6.744236946105957\n",
      "Batch :  40\n",
      "Loss: 6.800573825836182\n",
      "Batch :  41\n",
      "Loss: 6.625433444976807\n",
      "Batch :  42\n",
      "Loss: 6.62942361831665\n",
      "Batch :  43\n",
      "Loss: 6.572447299957275\n",
      "Batch :  44\n",
      "Loss: 6.864503383636475\n",
      "Batch :  45\n",
      "Loss: 6.571544170379639\n",
      "Batch :  46\n",
      "Loss: 6.602477550506592\n",
      "Batch :  47\n",
      "Loss: 6.757086277008057\n",
      "Batch :  48\n",
      "Loss: 6.9675774574279785\n",
      "Batch :  49\n",
      "Loss: 6.599440097808838\n",
      "Batch :  50\n",
      "Loss: 6.885862350463867\n",
      "Batch :  51\n",
      "Loss: 7.286599159240723\n",
      "Batch :  52\n",
      "Loss: 6.6312127113342285\n",
      "Batch :  53\n",
      "Loss: 6.493844032287598\n",
      "Batch :  54\n",
      "Loss: 6.658806800842285\n",
      "Batch :  55\n",
      "Loss: 6.3180389404296875\n",
      "Batch :  56\n",
      "Loss: 6.929721355438232\n",
      "Batch :  57\n",
      "Loss: 6.606307506561279\n",
      "Batch :  58\n",
      "Loss: 6.704461574554443\n",
      "Batch :  59\n",
      "Loss: 6.871926307678223\n",
      "Batch :  60\n",
      "Loss: 6.638664245605469\n",
      "Batch :  61\n",
      "Loss: 6.717063903808594\n",
      "Batch :  62\n",
      "Loss: 6.675158977508545\n",
      "Batch :  63\n",
      "Loss: 6.609799385070801\n",
      "Batch :  64\n",
      "Loss: 6.805225372314453\n",
      "Batch :  65\n",
      "Loss: 6.753809928894043\n",
      "Batch :  66\n",
      "Loss: 6.747304439544678\n",
      "Batch :  67\n",
      "Loss: 6.451299667358398\n",
      "Batch :  68\n",
      "Loss: 6.975716590881348\n",
      "Batch :  69\n",
      "Loss: 6.553107261657715\n",
      "Batch :  70\n",
      "Loss: 6.750287055969238\n",
      "Batch :  71\n",
      "Loss: 6.8666558265686035\n",
      "Batch :  72\n",
      "Loss: 6.702566146850586\n",
      "Batch :  73\n",
      "Loss: 6.453178882598877\n",
      "Batch :  74\n",
      "Loss: 6.578894138336182\n",
      "Batch :  75\n",
      "Loss: 6.516772270202637\n",
      "Batch :  76\n",
      "Loss: 6.486613750457764\n",
      "Batch :  77\n",
      "Loss: 6.643050670623779\n",
      "Batch :  78\n",
      "Loss: 6.881049633026123\n",
      "Batch :  79\n",
      "Loss: 6.718513488769531\n",
      "Batch :  80\n",
      "Loss: 6.459083080291748\n",
      "Batch :  81\n",
      "Loss: 6.759914398193359\n",
      "Batch :  82\n",
      "Loss: 6.513333797454834\n",
      "Batch :  83\n",
      "Loss: 6.437594890594482\n",
      "Batch :  84\n",
      "Loss: 6.54534387588501\n",
      "Batch :  85\n",
      "Loss: 6.706647872924805\n",
      "Batch :  86\n",
      "Loss: 6.843109130859375\n",
      "Batch :  87\n",
      "Loss: 6.573163032531738\n",
      "Batch :  88\n",
      "Loss: 6.669651508331299\n",
      "Batch :  89\n",
      "Loss: 6.523986339569092\n",
      "Batch :  90\n",
      "Loss: 6.973207950592041\n",
      "Batch :  91\n",
      "Loss: 6.644354343414307\n",
      "Batch :  92\n",
      "Loss: 6.884017467498779\n",
      "Batch :  93\n",
      "Loss: 6.694010257720947\n",
      "Batch :  94\n",
      "Loss: 6.5651140213012695\n",
      "Batch :  95\n",
      "Loss: 6.998652458190918\n",
      "Batch :  96\n",
      "Loss: 6.9048662185668945\n",
      "Batch :  97\n",
      "Loss: 6.462554454803467\n",
      "Batch :  98\n",
      "Loss: 6.7493391036987305\n",
      "Batch :  99\n",
      "Loss: 6.603344440460205\n",
      "Batch :  100\n",
      "Loss: 6.879144668579102\n",
      "Batch :  101\n",
      "Loss: 6.693864822387695\n",
      "Batch :  102\n",
      "Loss: 6.922871112823486\n",
      "Batch :  103\n",
      "Loss: 6.497783184051514\n",
      "Batch :  104\n",
      "Loss: 6.730278015136719\n",
      "Batch :  105\n",
      "Loss: 6.600279331207275\n",
      "Batch :  106\n",
      "Loss: 6.490333080291748\n",
      "Batch :  107\n",
      "Loss: 6.994610786437988\n",
      "Batch :  108\n",
      "Loss: 6.455545425415039\n",
      "Batch :  109\n",
      "Loss: 6.789340972900391\n",
      "Batch :  110\n",
      "Loss: 6.719531536102295\n",
      "Batch :  111\n",
      "Loss: 6.58772087097168\n",
      "Batch :  112\n",
      "Loss: 6.603492736816406\n",
      "Batch :  113\n",
      "Loss: 6.438499927520752\n",
      "Batch :  114\n",
      "Loss: 6.727541446685791\n",
      "Batch :  115\n",
      "Loss: 6.593483924865723\n",
      "Batch :  116\n",
      "Loss: 6.771304130554199\n",
      "Batch :  117\n",
      "Loss: 6.5487518310546875\n",
      "Batch :  118\n",
      "Loss: 6.205610752105713\n",
      "Batch :  119\n",
      "Loss: 6.365013599395752\n",
      "Batch :  120\n",
      "Loss: 6.393750190734863\n",
      "Batch :  121\n",
      "Loss: 6.270371913909912\n",
      "Batch :  122\n",
      "Loss: 6.551454544067383\n",
      "Batch :  123\n",
      "Loss: 6.663871765136719\n",
      "Batch :  124\n",
      "Loss: 6.477416038513184\n",
      "Batch :  125\n",
      "Loss: 6.4064106941223145\n",
      "Batch :  126\n",
      "Loss: 6.549799919128418\n",
      "Batch :  127\n",
      "Loss: 6.910849571228027\n",
      "Batch :  128\n",
      "Loss: 6.370715618133545\n",
      "Batch :  129\n",
      "Loss: 6.386500835418701\n",
      "Batch :  130\n",
      "Loss: 6.922389984130859\n",
      "Batch :  131\n",
      "Loss: 6.352396488189697\n",
      "Batch :  132\n",
      "Loss: 6.832973957061768\n",
      "Batch :  133\n",
      "Loss: 6.32542610168457\n",
      "Batch :  134\n",
      "Loss: 6.200031757354736\n",
      "Batch :  135\n",
      "Loss: 6.340480804443359\n",
      "Batch :  136\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.255951881408691\n",
      "Batch :  137\n",
      "Loss: 6.501692771911621\n",
      "Batch :  138\n",
      "Loss: 6.581567764282227\n",
      "Batch :  139\n",
      "Loss: 6.7375264167785645\n",
      "Batch :  140\n",
      "Loss: 6.394173622131348\n",
      "Batch :  141\n",
      "Loss: 6.724205493927002\n",
      "Batch :  142\n",
      "Loss: 6.7190093994140625\n",
      "Batch :  143\n",
      "Loss: 6.411349296569824\n",
      "Batch :  144\n",
      "Loss: 6.999265670776367\n",
      "Batch :  145\n",
      "Loss: 6.5244927406311035\n",
      "Batch :  146\n",
      "Loss: 6.767501354217529\n",
      "Batch :  147\n",
      "Loss: 6.7523980140686035\n",
      "Batch :  148\n",
      "Loss: 6.9549479484558105\n",
      "Batch :  149\n",
      "Loss: 6.6109795570373535\n",
      "Batch :  150\n",
      "Loss: 6.711129188537598\n",
      "Batch :  151\n",
      "Loss: 6.5176591873168945\n",
      "Batch :  152\n",
      "Loss: 6.588394641876221\n",
      "Batch :  153\n",
      "Loss: 6.421726703643799\n",
      "Batch :  154\n",
      "Loss: 6.941173076629639\n",
      "Batch :  155\n",
      "Loss: 6.745369911193848\n",
      "Batch :  156\n",
      "Loss: 6.522328853607178\n",
      "Batch :  157\n",
      "Loss: 6.51903772354126\n",
      "Batch :  158\n",
      "Loss: 6.646259784698486\n",
      "Batch :  159\n",
      "Loss: 6.2275285720825195\n",
      "Batch :  160\n",
      "Loss: 6.637674331665039\n",
      "Batch :  161\n",
      "Loss: 6.670886039733887\n",
      "Batch :  162\n",
      "Loss: 6.758313179016113\n",
      "Batch :  163\n",
      "Loss: 6.478430271148682\n",
      "Batch :  164\n",
      "Loss: 6.45502233505249\n",
      "Batch :  165\n",
      "Loss: 6.733329772949219\n",
      "Batch :  166\n",
      "Loss: 6.93464994430542\n",
      "Batch :  167\n",
      "Loss: 6.359338760375977\n",
      "Batch :  168\n",
      "Loss: 6.18351936340332\n",
      "Batch :  169\n",
      "Loss: 6.864455223083496\n",
      "Batch :  170\n",
      "Loss: 6.281161785125732\n",
      "Batch :  171\n",
      "Loss: 6.580748081207275\n",
      "Batch :  172\n",
      "Loss: 6.600278377532959\n",
      "Batch :  173\n",
      "Loss: 6.676133155822754\n",
      "Batch :  174\n",
      "Loss: 6.560095310211182\n",
      "Batch :  175\n",
      "Loss: 6.444753170013428\n",
      "Batch :  176\n",
      "Loss: 6.559100151062012\n",
      "Batch :  177\n",
      "Loss: 6.690684795379639\n",
      "Batch :  178\n",
      "Loss: 6.648350715637207\n",
      "Batch :  179\n",
      "Loss: 6.510171890258789\n",
      "Batch :  180\n",
      "Loss: 6.762267112731934\n",
      "Batch :  181\n",
      "Loss: 6.401477336883545\n",
      "Batch :  182\n",
      "Loss: 6.690152168273926\n",
      "Batch :  183\n",
      "Loss: 6.419355392456055\n",
      "Batch :  184\n",
      "Loss: 6.423433303833008\n",
      "Batch :  185\n",
      "Loss: 6.6045942306518555\n",
      "Batch :  186\n",
      "Loss: 6.172554016113281\n",
      "Batch :  187\n",
      "Loss: 6.559271335601807\n",
      "Batch :  188\n",
      "Loss: 6.76101016998291\n",
      "Batch :  189\n",
      "Loss: 6.298080921173096\n",
      "Batch :  190\n",
      "Loss: 6.56209135055542\n",
      "Batch :  191\n",
      "Loss: 6.746009826660156\n",
      "Batch :  192\n",
      "Loss: 6.341906547546387\n",
      "Batch :  193\n",
      "Loss: 6.2514824867248535\n",
      "Batch :  194\n",
      "Loss: 6.762654781341553\n",
      "Batch :  195\n",
      "Loss: 6.405157089233398\n",
      "Batch :  196\n",
      "Loss: 6.403181552886963\n",
      "Batch :  197\n",
      "Loss: 6.794355392456055\n",
      "Batch :  198\n",
      "Loss: 6.546611785888672\n",
      "Batch :  199\n",
      "Loss: 6.62807559967041\n",
      "Batch :  200\n",
      "Loss: 6.291876316070557\n",
      "Batch :  201\n",
      "Loss: 6.393173694610596\n",
      "Batch :  202\n",
      "Loss: 6.804400444030762\n",
      "Batch :  203\n",
      "Loss: 6.591184139251709\n",
      "Batch :  204\n",
      "Loss: 6.266411781311035\n",
      "Batch :  205\n",
      "Loss: 6.503593921661377\n",
      "Batch :  206\n",
      "Loss: 6.74431037902832\n",
      "Batch :  207\n",
      "Loss: 6.135069370269775\n",
      "Batch :  208\n",
      "Loss: 6.575697422027588\n",
      "Batch :  209\n",
      "Loss: 6.76521635055542\n",
      "Batch :  210\n",
      "Loss: 6.74462890625\n",
      "Batch :  211\n",
      "Loss: 6.720856189727783\n",
      "Batch :  212\n",
      "Loss: 6.429423809051514\n",
      "Batch :  213\n",
      "Loss: 6.466765880584717\n",
      "Batch :  214\n",
      "Loss: 6.554179668426514\n",
      "Batch :  215\n",
      "Loss: 6.517386436462402\n",
      "Batch :  216\n",
      "Loss: 6.439188003540039\n",
      "Batch :  217\n",
      "Loss: 6.595028877258301\n",
      "Batch :  218\n",
      "Loss: 6.371732711791992\n",
      "Batch :  219\n",
      "Loss: 6.40785026550293\n",
      "Batch :  220\n",
      "Loss: 6.566925525665283\n",
      "Batch :  221\n",
      "Loss: 6.796517372131348\n",
      "Batch :  222\n",
      "Loss: 6.594360828399658\n",
      "Batch :  223\n",
      "Loss: 6.479084491729736\n",
      "Batch :  224\n",
      "Loss: 6.286349296569824\n",
      "Batch :  225\n",
      "Loss: 6.285009384155273\n",
      "Batch :  226\n",
      "Loss: 6.4230170249938965\n",
      "Batch :  227\n",
      "Loss: 6.574885845184326\n",
      "Batch :  228\n",
      "Loss: 6.690598964691162\n",
      "Batch :  229\n",
      "Loss: 6.658492565155029\n",
      "Batch :  230\n",
      "Loss: 6.138757705688477\n",
      "Batch :  231\n",
      "Loss: 6.336014270782471\n",
      "Batch :  232\n",
      "Loss: 6.792573928833008\n",
      "Batch :  233\n",
      "Loss: 6.429512023925781\n",
      "Batch :  234\n",
      "Loss: 6.491366863250732\n",
      "Batch :  235\n",
      "Loss: 6.2730488777160645\n",
      "Batch :  236\n",
      "Loss: 6.803043365478516\n",
      "Batch :  237\n",
      "Loss: 6.866234302520752\n",
      "Batch :  238\n",
      "Loss: 6.672203540802002\n",
      "Batch :  239\n",
      "Loss: 6.266180992126465\n",
      "Batch :  240\n",
      "Loss: 6.274501800537109\n",
      "Batch :  241\n",
      "Loss: 6.3179497718811035\n",
      "Batch :  242\n",
      "Loss: 6.648568153381348\n",
      "Batch :  243\n",
      "Loss: 6.193558216094971\n",
      "Batch :  244\n",
      "Loss: 6.467566013336182\n",
      "Batch :  245\n",
      "Loss: 6.487061023712158\n",
      "Batch :  246\n",
      "Loss: 6.157815456390381\n",
      "Batch :  247\n",
      "Loss: 6.490147590637207\n",
      "Batch :  248\n",
      "Loss: 6.578600883483887\n",
      "Batch :  249\n",
      "Loss: 6.359119415283203\n",
      "Batch :  250\n",
      "Loss: 6.238395690917969\n",
      "Batch :  251\n",
      "Loss: 6.593034744262695\n",
      "Batch :  252\n",
      "Loss: 6.727634906768799\n",
      "Batch :  253\n",
      "Loss: 6.299101829528809\n",
      "Batch :  254\n",
      "Loss: 6.2585272789001465\n",
      "Batch :  255\n",
      "Loss: 6.282082557678223\n",
      "Batch :  256\n",
      "Loss: 6.70228910446167\n",
      "Batch :  257\n",
      "Loss: 6.67417049407959\n",
      "Batch :  258\n",
      "Loss: 6.783542633056641\n",
      "Batch :  259\n",
      "Loss: 6.248908996582031\n",
      "Batch :  260\n",
      "Loss: 6.43727970123291\n",
      "Batch :  261\n",
      "Loss: 6.306567192077637\n",
      "Batch :  262\n",
      "Loss: 6.3866472244262695\n",
      "Batch :  263\n",
      "Loss: 6.425991535186768\n",
      "Batch :  264\n",
      "Loss: 6.01693868637085\n",
      "Batch :  265\n",
      "Loss: 6.541039943695068\n",
      "Batch :  266\n",
      "Loss: 6.343262672424316\n",
      "Batch :  267\n",
      "Loss: 6.279658317565918\n",
      "Batch :  268\n",
      "Loss: 6.347958087921143\n",
      "Batch :  269\n",
      "Loss: 6.561019420623779\n",
      "Batch :  270\n",
      "Loss: 6.14410400390625\n",
      "Batch :  271\n",
      "Loss: 6.329087257385254\n",
      "Batch :  272\n",
      "Loss: 6.442828178405762\n",
      "Batch :  273\n",
      "Loss: 6.342817306518555\n",
      "Batch :  274\n",
      "Loss: 6.67905855178833\n",
      "Batch :  275\n",
      "Loss: 6.8127360343933105\n",
      "Batch :  276\n",
      "Loss: 6.006671905517578\n",
      "Batch :  277\n",
      "Loss: 6.463904857635498\n",
      "Batch :  278\n",
      "Loss: 6.424047470092773\n",
      "Batch :  279\n",
      "Loss: 6.353662014007568\n",
      "Batch :  280\n",
      "Loss: 6.756579875946045\n",
      "Batch :  281\n",
      "Loss: 6.325451850891113\n",
      "Batch :  282\n",
      "Loss: 6.312561988830566\n",
      "Batch :  283\n",
      "Loss: 6.599327564239502\n",
      "Batch :  284\n",
      "Loss: 6.481838703155518\n",
      "Batch :  285\n",
      "Loss: 6.636009216308594\n",
      "Batch :  286\n",
      "Loss: 6.757418632507324\n",
      "Batch :  287\n",
      "Loss: 6.4994659423828125\n",
      "Batch :  288\n",
      "Loss: 6.520315170288086\n",
      "Batch :  289\n",
      "Loss: 6.260258674621582\n",
      "Batch :  290\n",
      "Loss: 6.436427593231201\n",
      "Batch :  291\n",
      "Loss: 6.473727703094482\n",
      "Batch :  292\n",
      "Loss: 6.43002986907959\n",
      "Batch :  293\n",
      "Loss: 6.260131359100342\n",
      "Batch :  294\n",
      "Loss: 5.952690124511719\n",
      "Batch :  295\n",
      "Loss: 6.469161033630371\n",
      "Batch :  296\n",
      "Loss: 6.26444149017334\n",
      "Batch :  297\n",
      "Loss: 6.137368202209473\n",
      "Batch :  298\n",
      "Loss: 6.147750377655029\n",
      "Batch :  299\n",
      "Loss: 6.0016703605651855\n",
      "Batch :  300\n",
      "Loss: 6.335732936859131\n",
      "Batch :  301\n",
      "Loss: 6.458272933959961\n",
      "Batch :  302\n",
      "Loss: 6.230889797210693\n",
      "Batch :  303\n",
      "Loss: 6.278851509094238\n",
      "Batch :  304\n",
      "Loss: 5.994260787963867\n",
      "Batch :  305\n",
      "Loss: 6.240342617034912\n",
      "Batch :  306\n",
      "Loss: 6.185637950897217\n",
      "Batch :  307\n",
      "Loss: 5.9090352058410645\n",
      "Batch :  308\n",
      "Loss: 6.650828838348389\n",
      "Batch :  309\n",
      "Loss: 6.390013217926025\n",
      "Batch :  310\n",
      "Loss: 6.368612766265869\n",
      "Batch :  311\n",
      "Loss: 6.4550065994262695\n",
      "Batch :  312\n",
      "Loss: 6.555354595184326\n",
      "Epoch 3\n",
      "Batch :  0\n",
      "Loss: 6.249904155731201\n",
      "Batch :  1\n",
      "Loss: 6.522588729858398\n",
      "Batch :  2\n",
      "Loss: 6.2803263664245605\n",
      "Batch :  3\n",
      "Loss: 6.362411022186279\n",
      "Batch :  4\n",
      "Loss: 6.722054958343506\n",
      "Batch :  5\n",
      "Loss: 6.122875690460205\n",
      "Batch :  6\n",
      "Loss: 6.429120063781738\n",
      "Batch :  7\n",
      "Loss: 6.568893909454346\n",
      "Batch :  8\n",
      "Loss: 6.633677005767822\n",
      "Batch :  9\n",
      "Loss: 6.543792724609375\n",
      "Batch :  10\n",
      "Loss: 6.39788818359375\n",
      "Batch :  11\n",
      "Loss: 6.194031715393066\n",
      "Batch :  12\n",
      "Loss: 6.44290018081665\n",
      "Batch :  13\n",
      "Loss: 6.309120178222656\n",
      "Batch :  14\n",
      "Loss: 6.301830768585205\n",
      "Batch :  15\n",
      "Loss: 5.722106456756592\n",
      "Batch :  16\n",
      "Loss: 6.4463653564453125\n",
      "Batch :  17\n",
      "Loss: 6.298643112182617\n",
      "Batch :  18\n",
      "Loss: 6.4106621742248535\n",
      "Batch :  19\n",
      "Loss: 6.572987079620361\n",
      "Batch :  20\n",
      "Loss: 6.464207172393799\n",
      "Batch :  21\n",
      "Loss: 6.1686577796936035\n",
      "Batch :  22\n",
      "Loss: 6.20681619644165\n",
      "Batch :  23\n",
      "Loss: 6.101829528808594\n",
      "Batch :  24\n",
      "Loss: 6.397871494293213\n",
      "Batch :  25\n",
      "Loss: 6.286875247955322\n",
      "Batch :  26\n",
      "Loss: 5.9499030113220215\n",
      "Batch :  27\n",
      "Loss: 6.775052070617676\n",
      "Batch :  28\n",
      "Loss: 6.290519714355469\n",
      "Batch :  29\n",
      "Loss: 6.210425853729248\n",
      "Batch :  30\n",
      "Loss: 6.32370662689209\n",
      "Batch :  31\n",
      "Loss: 6.773467063903809\n",
      "Batch :  32\n",
      "Loss: 6.511332988739014\n",
      "Batch :  33\n",
      "Loss: 6.380847454071045\n",
      "Batch :  34\n",
      "Loss: 6.108159065246582\n",
      "Batch :  35\n",
      "Loss: 6.14209508895874\n",
      "Batch :  36\n",
      "Loss: 6.075165748596191\n",
      "Batch :  37\n",
      "Loss: 6.340038776397705\n",
      "Batch :  38\n",
      "Loss: 6.536183834075928\n",
      "Batch :  39\n",
      "Loss: 6.252055644989014\n",
      "Batch :  40\n",
      "Loss: 6.325317859649658\n",
      "Batch :  41\n",
      "Loss: 6.236048221588135\n",
      "Batch :  42\n",
      "Loss: 6.186204433441162\n",
      "Batch :  43\n",
      "Loss: 6.134284019470215\n",
      "Batch :  44\n",
      "Loss: 6.4025115966796875\n",
      "Batch :  45\n",
      "Loss: 6.097976207733154\n",
      "Batch :  46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.057738780975342\n",
      "Batch :  47\n",
      "Loss: 6.218738555908203\n",
      "Batch :  48\n",
      "Loss: 6.443546295166016\n",
      "Batch :  49\n",
      "Loss: 6.1426777839660645\n",
      "Batch :  50\n",
      "Loss: 6.352998733520508\n",
      "Batch :  51\n",
      "Loss: 6.621384143829346\n",
      "Batch :  52\n",
      "Loss: 6.137950420379639\n",
      "Batch :  53\n",
      "Loss: 5.965452671051025\n",
      "Batch :  54\n",
      "Loss: 6.2577104568481445\n",
      "Batch :  55\n",
      "Loss: 5.940954208374023\n",
      "Batch :  56\n",
      "Loss: 6.438726425170898\n",
      "Batch :  57\n",
      "Loss: 6.123147487640381\n",
      "Batch :  58\n",
      "Loss: 6.248194217681885\n",
      "Batch :  59\n",
      "Loss: 6.376138687133789\n",
      "Batch :  60\n",
      "Loss: 6.162285804748535\n",
      "Batch :  61\n",
      "Loss: 6.2060866355896\n",
      "Batch :  62\n",
      "Loss: 6.156921863555908\n",
      "Batch :  63\n",
      "Loss: 6.117175102233887\n",
      "Batch :  64\n",
      "Loss: 6.279160976409912\n",
      "Batch :  65\n",
      "Loss: 6.3153181076049805\n",
      "Batch :  66\n",
      "Loss: 6.2412590980529785\n",
      "Batch :  67\n",
      "Loss: 5.984312534332275\n",
      "Batch :  68\n",
      "Loss: 6.487259387969971\n",
      "Batch :  69\n",
      "Loss: 6.015294551849365\n",
      "Batch :  70\n",
      "Loss: 6.20359468460083\n",
      "Batch :  71\n",
      "Loss: 6.406589031219482\n",
      "Batch :  72\n",
      "Loss: 6.2306694984436035\n",
      "Batch :  73\n",
      "Loss: 6.051054000854492\n",
      "Batch :  74\n",
      "Loss: 6.127244472503662\n",
      "Batch :  75\n",
      "Loss: 6.0231194496154785\n",
      "Batch :  76\n",
      "Loss: 6.058025360107422\n",
      "Batch :  77\n",
      "Loss: 6.116708278656006\n",
      "Batch :  78\n",
      "Loss: 6.393728256225586\n",
      "Batch :  79\n",
      "Loss: 6.253540515899658\n",
      "Batch :  80\n",
      "Loss: 6.01907205581665\n",
      "Batch :  81\n",
      "Loss: 6.320828437805176\n",
      "Batch :  82\n",
      "Loss: 5.977176666259766\n",
      "Batch :  83\n",
      "Loss: 6.022185802459717\n",
      "Batch :  84\n",
      "Loss: 6.126369953155518\n",
      "Batch :  85\n",
      "Loss: 6.29794454574585\n",
      "Batch :  86\n",
      "Loss: 6.330388069152832\n",
      "Batch :  87\n",
      "Loss: 6.112087726593018\n",
      "Batch :  88\n",
      "Loss: 6.205626010894775\n",
      "Batch :  89\n",
      "Loss: 6.041018486022949\n",
      "Batch :  90\n",
      "Loss: 6.517944812774658\n",
      "Batch :  91\n",
      "Loss: 6.210827350616455\n",
      "Batch :  92\n",
      "Loss: 6.436332702636719\n",
      "Batch :  93\n",
      "Loss: 6.223220348358154\n",
      "Batch :  94\n",
      "Loss: 6.086893558502197\n",
      "Batch :  95\n",
      "Loss: 6.519927501678467\n",
      "Batch :  96\n",
      "Loss: 6.45430850982666\n",
      "Batch :  97\n",
      "Loss: 6.027952671051025\n",
      "Batch :  98\n",
      "Loss: 6.302002906799316\n",
      "Batch :  99\n",
      "Loss: 6.129397392272949\n",
      "Batch :  100\n",
      "Loss: 6.4302520751953125\n",
      "Batch :  101\n",
      "Loss: 6.297048568725586\n",
      "Batch :  102\n",
      "Loss: 6.4654059410095215\n",
      "Batch :  103\n",
      "Loss: 6.0315046310424805\n",
      "Batch :  104\n",
      "Loss: 6.218182563781738\n",
      "Batch :  105\n",
      "Loss: 6.180929660797119\n",
      "Batch :  106\n",
      "Loss: 6.151620388031006\n",
      "Batch :  107\n",
      "Loss: 6.49118185043335\n",
      "Batch :  108\n",
      "Loss: 6.040445804595947\n",
      "Batch :  109\n",
      "Loss: 6.3642120361328125\n",
      "Batch :  110\n",
      "Loss: 6.292970657348633\n",
      "Batch :  111\n",
      "Loss: 6.1086649894714355\n",
      "Batch :  112\n",
      "Loss: 6.2377119064331055\n",
      "Batch :  113\n",
      "Loss: 5.984674453735352\n",
      "Batch :  114\n",
      "Loss: 6.213869571685791\n",
      "Batch :  115\n",
      "Loss: 6.2338080406188965\n",
      "Batch :  116\n",
      "Loss: 6.343911170959473\n",
      "Batch :  117\n",
      "Loss: 6.116510391235352\n",
      "Batch :  118\n",
      "Loss: 5.750433921813965\n",
      "Batch :  119\n",
      "Loss: 5.999302387237549\n",
      "Batch :  120\n",
      "Loss: 5.975633144378662\n",
      "Batch :  121\n",
      "Loss: 5.886531829833984\n",
      "Batch :  122\n",
      "Loss: 6.082607746124268\n",
      "Batch :  123\n",
      "Loss: 6.289000988006592\n",
      "Batch :  124\n",
      "Loss: 6.038815498352051\n",
      "Batch :  125\n",
      "Loss: 5.998932361602783\n",
      "Batch :  126\n",
      "Loss: 6.032775402069092\n",
      "Batch :  127\n",
      "Loss: 6.5007171630859375\n",
      "Batch :  128\n",
      "Loss: 5.997244834899902\n",
      "Batch :  129\n",
      "Loss: 5.952826499938965\n",
      "Batch :  130\n",
      "Loss: 6.4905476570129395\n",
      "Batch :  131\n",
      "Loss: 6.021114349365234\n",
      "Batch :  132\n",
      "Loss: 6.340807914733887\n",
      "Batch :  133\n",
      "Loss: 5.894680976867676\n",
      "Batch :  134\n",
      "Loss: 5.822028160095215\n",
      "Batch :  135\n",
      "Loss: 5.937703609466553\n",
      "Batch :  136\n",
      "Loss: 5.870270252227783\n",
      "Batch :  137\n",
      "Loss: 6.034673690795898\n",
      "Batch :  138\n",
      "Loss: 6.143682479858398\n",
      "Batch :  139\n",
      "Loss: 6.328864574432373\n",
      "Batch :  140\n",
      "Loss: 6.01849889755249\n",
      "Batch :  141\n",
      "Loss: 6.336101531982422\n",
      "Batch :  142\n",
      "Loss: 6.364116668701172\n",
      "Batch :  143\n",
      "Loss: 6.037588119506836\n",
      "Batch :  144\n",
      "Loss: 6.547693252563477\n",
      "Batch :  145\n",
      "Loss: 6.096357345581055\n",
      "Batch :  146\n",
      "Loss: 6.348118305206299\n",
      "Batch :  147\n",
      "Loss: 6.328378200531006\n",
      "Batch :  148\n",
      "Loss: 6.5314412117004395\n",
      "Batch :  149\n",
      "Loss: 6.208159923553467\n",
      "Batch :  150\n",
      "Loss: 6.342154026031494\n",
      "Batch :  151\n",
      "Loss: 6.138628005981445\n",
      "Batch :  152\n",
      "Loss: 6.138600826263428\n",
      "Batch :  153\n",
      "Loss: 5.976384162902832\n",
      "Batch :  154\n",
      "Loss: 6.471174240112305\n",
      "Batch :  155\n",
      "Loss: 6.350739479064941\n",
      "Batch :  156\n",
      "Loss: 6.0989766120910645\n",
      "Batch :  157\n",
      "Loss: 6.095218181610107\n",
      "Batch :  158\n",
      "Loss: 6.225054740905762\n",
      "Batch :  159\n",
      "Loss: 5.895505428314209\n",
      "Batch :  160\n",
      "Loss: 6.276429653167725\n",
      "Batch :  161\n",
      "Loss: 6.257054805755615\n",
      "Batch :  162\n",
      "Loss: 6.326716899871826\n",
      "Batch :  163\n",
      "Loss: 6.052881717681885\n",
      "Batch :  164\n",
      "Loss: 6.026721000671387\n",
      "Batch :  165\n",
      "Loss: 6.319390773773193\n",
      "Batch :  166\n",
      "Loss: 6.533379077911377\n",
      "Batch :  167\n",
      "Loss: 5.979433536529541\n",
      "Batch :  168\n",
      "Loss: 5.83062219619751\n",
      "Batch :  169\n",
      "Loss: 6.3904852867126465\n",
      "Batch :  170\n",
      "Loss: 5.885591983795166\n",
      "Batch :  171\n",
      "Loss: 6.230742931365967\n",
      "Batch :  172\n",
      "Loss: 6.258662223815918\n",
      "Batch :  173\n",
      "Loss: 6.311618328094482\n",
      "Batch :  174\n",
      "Loss: 6.143920421600342\n",
      "Batch :  175\n",
      "Loss: 6.138763904571533\n",
      "Batch :  176\n",
      "Loss: 6.2919087409973145\n",
      "Batch :  177\n",
      "Loss: 6.376805305480957\n",
      "Batch :  178\n",
      "Loss: 6.194853782653809\n",
      "Batch :  179\n",
      "Loss: 6.155052661895752\n",
      "Batch :  180\n",
      "Loss: 6.427611351013184\n",
      "Batch :  181\n",
      "Loss: 6.0914306640625\n",
      "Batch :  182\n",
      "Loss: 6.344920635223389\n",
      "Batch :  183\n",
      "Loss: 6.011189937591553\n",
      "Batch :  184\n",
      "Loss: 6.068282127380371\n",
      "Batch :  185\n",
      "Loss: 6.292623996734619\n",
      "Batch :  186\n",
      "Loss: 5.853200912475586\n",
      "Batch :  187\n",
      "Loss: 6.137079238891602\n",
      "Batch :  188\n",
      "Loss: 6.32620906829834\n",
      "Batch :  189\n",
      "Loss: 5.9425435066223145\n",
      "Batch :  190\n",
      "Loss: 6.225790023803711\n",
      "Batch :  191\n",
      "Loss: 6.375200271606445\n",
      "Batch :  192\n",
      "Loss: 5.899049758911133\n",
      "Batch :  193\n",
      "Loss: 5.8839216232299805\n",
      "Batch :  194\n",
      "Loss: 6.395711898803711\n",
      "Batch :  195\n",
      "Loss: 5.991046905517578\n",
      "Batch :  196\n",
      "Loss: 6.016199588775635\n",
      "Batch :  197\n",
      "Loss: 6.302903175354004\n",
      "Batch :  198\n",
      "Loss: 6.127734184265137\n",
      "Batch :  199\n",
      "Loss: 6.252374172210693\n",
      "Batch :  200\n",
      "Loss: 5.878587245941162\n",
      "Batch :  201\n",
      "Loss: 6.0193705558776855\n",
      "Batch :  202\n",
      "Loss: 6.42682409286499\n",
      "Batch :  203\n",
      "Loss: 6.202300071716309\n",
      "Batch :  204\n",
      "Loss: 5.915356159210205\n",
      "Batch :  205\n",
      "Loss: 6.137810707092285\n",
      "Batch :  206\n",
      "Loss: 6.312863826751709\n",
      "Batch :  207\n",
      "Loss: 5.718457221984863\n",
      "Batch :  208\n",
      "Loss: 6.190084934234619\n",
      "Batch :  209\n",
      "Loss: 6.365678310394287\n",
      "Batch :  210\n",
      "Loss: 6.319736003875732\n",
      "Batch :  211\n",
      "Loss: 6.390160083770752\n",
      "Batch :  212\n",
      "Loss: 5.976174354553223\n",
      "Batch :  213\n",
      "Loss: 5.988590240478516\n",
      "Batch :  214\n",
      "Loss: 6.203372955322266\n",
      "Batch :  215\n",
      "Loss: 6.127841949462891\n",
      "Batch :  216\n",
      "Loss: 6.077239036560059\n",
      "Batch :  217\n",
      "Loss: 6.222268581390381\n",
      "Batch :  218\n",
      "Loss: 5.99249792098999\n",
      "Batch :  219\n",
      "Loss: 5.993690013885498\n",
      "Batch :  220\n",
      "Loss: 6.162557601928711\n",
      "Batch :  221\n",
      "Loss: 6.449954509735107\n",
      "Batch :  222\n",
      "Loss: 6.160208225250244\n",
      "Batch :  223\n",
      "Loss: 6.116545677185059\n",
      "Batch :  224\n",
      "Loss: 5.931618690490723\n",
      "Batch :  225\n",
      "Loss: 5.9545207023620605\n",
      "Batch :  226\n",
      "Loss: 6.038019180297852\n",
      "Batch :  227\n",
      "Loss: 6.119225025177002\n",
      "Batch :  228\n",
      "Loss: 6.312442779541016\n",
      "Batch :  229\n",
      "Loss: 6.263650894165039\n",
      "Batch :  230\n",
      "Loss: 5.7492995262146\n",
      "Batch :  231\n",
      "Loss: 5.934109687805176\n",
      "Batch :  232\n",
      "Loss: 6.3272013664245605\n",
      "Batch :  233\n",
      "Loss: 6.028648853302002\n",
      "Batch :  234\n",
      "Loss: 6.12096643447876\n",
      "Batch :  235\n",
      "Loss: 5.903839111328125\n",
      "Batch :  236\n",
      "Loss: 6.392707347869873\n",
      "Batch :  237\n",
      "Loss: 6.407260417938232\n",
      "Batch :  238\n",
      "Loss: 6.2346720695495605\n",
      "Batch :  239\n",
      "Loss: 5.8590192794799805\n",
      "Batch :  240\n",
      "Loss: 5.8623151779174805\n",
      "Batch :  241\n",
      "Loss: 5.908960819244385\n",
      "Batch :  242\n",
      "Loss: 6.259861469268799\n",
      "Batch :  243\n",
      "Loss: 5.847524642944336\n",
      "Batch :  244\n",
      "Loss: 6.048573017120361\n",
      "Batch :  245\n",
      "Loss: 6.086145401000977\n",
      "Batch :  246\n",
      "Loss: 5.7276201248168945\n",
      "Batch :  247\n",
      "Loss: 6.150530815124512\n",
      "Batch :  248\n",
      "Loss: 6.156734943389893\n",
      "Batch :  249\n",
      "Loss: 5.951607704162598\n",
      "Batch :  250\n",
      "Loss: 5.832827091217041\n",
      "Batch :  251\n",
      "Loss: 6.179574966430664\n",
      "Batch :  252\n",
      "Loss: 6.2496209144592285\n",
      "Batch :  253\n",
      "Loss: 5.898031711578369\n",
      "Batch :  254\n",
      "Loss: 5.87815523147583\n",
      "Batch :  255\n",
      "Loss: 5.882822513580322\n",
      "Batch :  256\n",
      "Loss: 6.309359550476074\n",
      "Batch :  257\n",
      "Loss: 6.252004623413086\n",
      "Batch :  258\n",
      "Loss: 6.423224449157715\n",
      "Batch :  259\n",
      "Loss: 5.875636100769043\n",
      "Batch :  260\n",
      "Loss: 6.015657901763916\n",
      "Batch :  261\n",
      "Loss: 5.9474992752075195\n",
      "Batch :  262\n",
      "Loss: 6.007802486419678\n",
      "Batch :  263\n",
      "Loss: 6.054716110229492\n",
      "Batch :  264\n",
      "Loss: 5.530737400054932\n",
      "Batch :  265\n",
      "Loss: 6.125450134277344\n",
      "Batch :  266\n",
      "Loss: 5.940962314605713\n",
      "Batch :  267\n",
      "Loss: 5.907163619995117\n",
      "Batch :  268\n",
      "Loss: 5.927876949310303\n",
      "Batch :  269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 6.119964599609375\n",
      "Batch :  270\n",
      "Loss: 5.754729747772217\n",
      "Batch :  271\n",
      "Loss: 5.935001373291016\n",
      "Batch :  272\n",
      "Loss: 6.035041332244873\n",
      "Batch :  273\n",
      "Loss: 5.941882133483887\n",
      "Batch :  274\n",
      "Loss: 6.335686206817627\n",
      "Batch :  275\n",
      "Loss: 6.381246566772461\n",
      "Batch :  276\n",
      "Loss: 5.634225845336914\n",
      "Batch :  277\n",
      "Loss: 6.082145690917969\n",
      "Batch :  278\n",
      "Loss: 6.041434288024902\n",
      "Batch :  279\n",
      "Loss: 5.968863010406494\n",
      "Batch :  280\n",
      "Loss: 6.304779052734375\n",
      "Batch :  281\n",
      "Loss: 5.952790260314941\n",
      "Batch :  282\n",
      "Loss: 5.857866287231445\n",
      "Batch :  283\n",
      "Loss: 6.179609298706055\n",
      "Batch :  284\n",
      "Loss: 6.11755895614624\n",
      "Batch :  285\n",
      "Loss: 6.248010635375977\n",
      "Batch :  286\n",
      "Loss: 6.363430976867676\n",
      "Batch :  287\n",
      "Loss: 6.169281482696533\n",
      "Batch :  288\n",
      "Loss: 6.135061264038086\n",
      "Batch :  289\n",
      "Loss: 5.878298282623291\n",
      "Batch :  290\n",
      "Loss: 6.069104194641113\n",
      "Batch :  291\n",
      "Loss: 6.06389045715332\n",
      "Batch :  292\n",
      "Loss: 6.023507118225098\n",
      "Batch :  293\n",
      "Loss: 5.903846263885498\n",
      "Batch :  294\n",
      "Loss: 5.547488689422607\n",
      "Batch :  295\n",
      "Loss: 6.106835842132568\n",
      "Batch :  296\n",
      "Loss: 5.8426384925842285\n",
      "Batch :  297\n",
      "Loss: 5.789602756500244\n",
      "Batch :  298\n",
      "Loss: 5.763829231262207\n",
      "Batch :  299\n",
      "Loss: 5.614463806152344\n",
      "Batch :  300\n",
      "Loss: 5.946896553039551\n",
      "Batch :  301\n",
      "Loss: 6.093533039093018\n",
      "Batch :  302\n",
      "Loss: 5.778916358947754\n",
      "Batch :  303\n",
      "Loss: 5.907000541687012\n",
      "Batch :  304\n",
      "Loss: 5.668575763702393\n",
      "Batch :  305\n",
      "Loss: 5.907270908355713\n",
      "Batch :  306\n",
      "Loss: 5.862693786621094\n",
      "Batch :  307\n",
      "Loss: 5.54074764251709\n",
      "Batch :  308\n",
      "Loss: 6.250627040863037\n",
      "Batch :  309\n",
      "Loss: 5.961605072021484\n",
      "Batch :  310\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-3).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.variables_initializer(saved_vars))\n",
    "\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    for batchNum in range(len(batch_input[0])):\n",
    "        print(\"Batch : \",batchNum)\n",
    "        t = session.run([optimizer, loss, decoder_outputs, suppression_loss], {\n",
    "            d_tokens: batch_input[0][batchNum],\n",
    "            d_lengths: batch_input[1][batchNum],\n",
    "            a_labels: batch_input[2][batchNum],\n",
    "            encoder_input_mask: batch_input[3][batchNum],\n",
    "            encoder_lengths: batch_input[4][batchNum],\n",
    "            decoder_inputs: batch_input[5][batchNum],\n",
    "            decoder_labels: batch_input[6][batchNum],\n",
    "            decoder_lengths: batch_input[7][batchNum],\n",
    "            s_answer: batch_input[8][batchNum],\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(t[1]))\n",
    "    if(epoch%5 == 0):\n",
    "        print(\"Saving model\")\n",
    "        #saver.save(session, \"qgen-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'qgen-model'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[3][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from qgen-model\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(session, 'qgen-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = session.run(answer_tags, {\n",
    "    d_tokens: batch_input[0][48],\n",
    "    d_lengths: batch_input[1][48],\n",
    "})\n",
    "answers = np.argmax(answers, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(28):\n",
    "    print(\"Prediction\")\n",
    "    printAllAns(answers,48,0)\n",
    "    print(\"Ground Truth\")\n",
    "    printAllAns(batch_input[2][48],48,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDoc(batch,num):\n",
    "    for i in batch_input[0][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "def printQues(batch,num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAnsForQuestion(batch, num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAllAns(answers, batch, num):\n",
    "    for i,word in enumerate(batch_input[0][batch][num]):\n",
    "        if answers[num][i] == 1 :\n",
    "            print(look_up_token_reduced(word),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batchNum = 234\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch_input[0][batchNum].shape[0]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    d_tokens: batch_input[0][batchNum],\n",
    "    d_lengths: batch_input[1][batchNum],\n",
    "    a_labels: batch_input[2][batchNum],\n",
    "    encoder_input_mask: batch_input[3][batchNum],\n",
    "    encoder_lengths: batch_input[4][batchNum],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 235)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[0][batchNum].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[:,:,END_TOKEN] = 0\n",
    "qs = np.argmax(questions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: who was the name of the treaty developed that the island of the treaty ?\n",
      "Ground Truth Question: \n",
      "<START> which company filed suit against harding , <UNK> , <UNK> & <UNK> 's client ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['healthcare', 'advocates']\n",
      "Context:\n",
      "in 2003 , harding <UNK> <UNK> & <UNK> defended a client from a trademark dispute using the archive 's wayback machine . the attorneys were able to demonstrate that the claims made by the plaintiff were invalid , based on the content of their web site from several years prior . the plaintiff , healthcare advocates , then amended their complaint to include the internet archive , accusing the organization of copyright infringement as well as violations of the dmca and the computer fraud and abuse act . healthcare advocates claimed that , since they had installed a robots.txt file on their web site , even if after the initial lawsuit was filed , the archive should have removed all previous copies of the plaintiff web site from the wayback machine . the lawsuit was settled out of court . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what was the name of the treaty ?\n",
      "Ground Truth Question: \n",
      "<START> why did publisher change the cover picture on feynman 's book ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['connections', 'to', 'drugs', 'and', 'rock', 'and', 'roll']\n",
      "Context:\n",
      "feynman was elected a foreign member of the royal society ( <UNK> ) in <UNK> at this time in the early 1960s , feynman exhausted himself by working on multiple major projects at the same time , including a request , while at caltech , to `` spruce up '' the teaching of undergraduates . after three years devoted to the task , he produced a series of lectures that eventually became the feynman lectures on physics . he wanted a picture of a <UNK> sprinkled with powder to show the modes of vibration at the beginning of the book . concerned over the connections to drugs and rock and roll that could be made from the image , the publishers changed the cover to plain red , though they included a picture of him playing drums in the foreword . the feynman lectures on physics occupied two physicists , robert b. leighton and matthew sands , as part-time co-authors for several years . even though the books were not adopted by most universities as textbooks , they continue to sell well because they provide a deep understanding of physics . many of his lectures and miscellaneous talks were turned into other books , including the character of physical law , qed : the strange theory of light and matter , statistical mechanics , lectures on gravitation , and the feynman lectures on computation .  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what did chopin 's elite universities still be to ?\n",
      "Ground Truth Question: \n",
      "<START> what is energy abbreviated to in science ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['e']\n",
      "Context:\n",
      "first recognized in 1900 by max planck , it was originally the proportionality constant between the minimal increment of energy , e , of a hypothetical electrically charged oscillator in a cavity that contained black body radiation , and the frequency , f , of its associated electromagnetic wave . in 1905 the value e , the minimal energy increment of a hypothetical oscillator , was theoretically associated by einstein with a `` quantum '' or minimal element of the energy of the electromagnetic wave itself . the light quantum behaved in some respects as an electrically neutral particle , as opposed to an electromagnetic wave . it was eventually called the photon . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: which war did the settlers came from ?\n",
      "Ground Truth Question: \n",
      "<START> in what battle did murad ii leave as the victory in 1448 ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['second', 'battle', 'of', 'kosovo']\n",
      "Context:\n",
      "part of the ottoman territories in the balkans ( such as thessaloniki , macedonia and kosovo ) were temporarily lost after 1402 but were later recovered by murad ii between the 1430s and 1450s . on 10 november 1444 , murad ii defeated the hungarian , polish , and wallachian armies under wÅ‚adysÅ‚aw iii of poland ( also king of hungary ) and john hunyadi at the battle of varna , the final battle of the crusade of varna , although albanians under skanderbeg continued to resist . four years later , john hunyadi prepared another army ( of hungarian and wallachian forces ) to attack the turks but was again defeated by murad ii at the second battle of kosovo in 1448 . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what did the island sell in increased revenue during which style ?\n",
      "Ground Truth Question: \n",
      "<START> what 's the term for bacillus that can be exposed to acidic solutions without losing their stains ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['acid-fast']\n",
      "Context:\n",
      "using <UNK> stains on <UNK> samples from phlegm ( also called `` sputum '' ) , scientists can identify mtb under a microscope . since mtb retains certain stains even after being treated with acidic solution , it is classified as an acid-fast bacillus . the most common acid-fast staining techniques are the <UNK> stain and the <UNK> stain , which dye acid-fast bacilli a bright red that stands out against a blue background . <UNK> staining and fluorescence microscopy are also used . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: where is the glaciers to descend to be to ?\n",
      "Ground Truth Question: \n",
      "<START> what educational institution is at home in seattle ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['university', 'of', 'washington']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seattle is home to the university of washington , as well as the institution 's professional and continuing education unit , the university of washington educational outreach . a study by newsweek international in 2006 cited the university of washington as the twenty-second best university in the world . seattle also has a number of smaller private universities including seattle university and seattle pacific university , the former a jesuit catholic institution , the latter free methodist ; universities aimed at the working adult , like city university and antioch university ; colleges within the seattle colleges district system , comprising north , central , and south ; seminaries , including western seminary and a number of arts colleges , such as cornish college of the arts , pratt fine arts center , and the art institute of seattle . in 2001 , time magazine selected seattle central community college as community college of the year , stating the school `` pushes diverse students to work together in small teams '' . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what did the punishing losses were the u.s. army military ?\n",
      "Ground Truth Question: \n",
      "<START> what did elizabeth do to diffuse hostile public sentiment ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['live', 'television', 'broadcast']\n",
      "Context:\n",
      "in 1997 , a year after the divorce , diana was killed in a car crash in paris . the queen was on holiday with her extended family at balmoral . diana 's two sons by <UNK> william and <UNK> to attend church and so the queen and prince philip took them that morning . after that single public appearance , for five days the queen and the duke shielded their grandsons from the intense press interest by keeping them at balmoral where they could grieve in private , but the royal family 's seclusion and the failure to fly a flag at <UNK> over buckingham palace caused public dismay . pressured by the hostile reaction , the queen agreed to return to london and do a live television broadcast on 5 september , the day before diana 's funeral . in the broadcast , she expressed admiration for diana and her feelings `` as a grandmother '' for the two princes . as a result , much of the public hostility evaporated . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: which two areas of the most of the treaty ?\n",
      "Ground Truth Question: \n",
      "<START> where was at & t 's `` picturephone '' device unveiled ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['1964', 'new', 'york', 'world', \"'s\", 'fair']\n",
      "Context:\n",
      "one of the first demonstrations of the ability for telecommunications to help sign language users communicate with each other occurred when at & t 's videophone ( trademarked as the `` picturephone '' ) was introduced to the public at the 1964 new york world 's fair <UNK> deaf users were able to communicate freely with each other between the fair and another city . various universities and other organizations , including british telecom 's <UNK> facility , have also conducted extensive research on signing via videotelephony . the use of sign language via videotelephony was hampered for many years due to the difficulty of its use over slow analogue copper phone lines , coupled with the high cost of better quality isdn ( data ) phone lines . those factors largely disappeared with the introduction of more efficient video codecs and the advent of lower cost high-speed isdn data and ip ( internet ) services in the 1990s . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: how much of the states ' in greece 's public debt forecast to hit as high of the new mets in the nlcs ?\n",
      "Ground Truth Question: \n",
      "<START> what percentage of japanese warships were sunk by u.s. submarines ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['28', '%']\n",
      "Context:\n",
      "u.s. submarines accounted for 56 % of the japanese merchantmen sunk ; mines or aircraft destroyed most of the rest . us <UNK> also claimed 28 % of japanese warships destroyed . furthermore , they played important reconnaissance roles , as at the battles of the philippine sea ( june 1944 ) and leyte gulf ( october 1944 ) ( and , coincidentally , [ clarification needed ] at midway in june 1942 ) , when they gave accurate and timely warning of the approach of the japanese fleet . submarines also rescued hundreds of downed <UNK> , including future u.s. president george <UNK> . bush . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: where is the largest of what body the the foundation of them ?\n",
      "Ground Truth Question: \n",
      "<START> what is the schauspielhaus in english ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['play', 'house']\n",
      "Context:\n",
      "around 40 theatres are located in hanover . the opera house , the schauspielhaus ( play house ) , the <UNK> , the <UNK> and the <UNK> galerie belong to the lower saxony state theatre . the theater am <UNK> is hanover 's big theatre for musicals , shows and guest performances . the <UNK> theater ( new theatre ) is the boulevard theatre of hanover . the theater fÃ¼r niedersachsen is another big theatre in hanover , which also has an own <UNK> . some of the most important <UNK> are the rock musicals of the german rock musician heinz rudolph kunze , which take place at the <UNK> in the great garden . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: which year did the french emperor napoleon iii take control of longwood house ?\n",
      "Ground Truth Question: \n",
      "<START> when was the treatise that allowed a congregation to elect or remove a minister ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['1523']\n",
      "Context:\n",
      "in the middle ages , the church and the worldly authorities were closely related . martin luther separated the religious and the worldly realms in principle ( doctrine of the two kingdoms ) . the believers were obliged to use reason to govern the worldly sphere in an orderly and peaceful way . luther 's doctrine of the priesthood of all believers upgraded the role of laymen in the church considerably . the members of a congregation had the right to elect a minister and , if necessary , to vote for his dismissal ( treatise on the right and authority of a christian assembly or congregation to judge all doctrines and to call , install and dismiss teachers , as testified in scripture ; 1523 ) . calvin strengthened this basically democratic approach by including elected laymen ( church elders , presbyters ) in his representative church government . the huguenots added regional synods and a national synod , whose members were elected by the congregations , to calvin 's system of church self-government . this system was taken over by the other reformed churches . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: who praised israel , in new york mets in the us\n",
      "Ground Truth Question: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> what did the english east india company have difficulty attracting ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['new', 'immigrants']\n",
      "Context:\n",
      "between january and may 1673 , the dutch east india company forcibly took the island , before english reinforcements restored english east india company control . the company experienced difficulty attracting new immigrants , and sentiments of unrest and rebellion fomented among the inhabitants . ecological problems , including deforestation , soil erosion , vermin and drought , led governor isaac <UNK> to suggest in 1715 that the population be moved to mauritius , but this was not acted upon and the company continued to subsidise the community because of the island 's strategic location . a census in 1723 recorded 1,110 people , including 610 slaves . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: which language is used to whom ?\n",
      "Ground Truth Question: \n",
      "<START> what common sea creature produces a collectable item ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['conch']\n",
      "Context:\n",
      "the marine life found here consists of anemones , <UNK> , sea cucumbers , and <UNK> , which all live on the reefs along with turtles , conch and many varieties of marine fishes . the marine <UNK> is rich in conch , which has <UNK> shells . its meat is a favourite food supplement item and their shells are a collectors item . other species of fish which are recorded close to the shore line in shallow waters are : sergeant majors , the blue chromis , brown chromis , surgeon fish ; blue tangs and trumpet fish . on the shore are ghost crabs , which always live on the beach in small <UNK> tunnels made in sand , and the hermit crabs , which live in land but lay eggs in water and which also eat garbage and sewerage . they spend some months in the sea during and after the hatching season . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what was the name of the treaty ?\n",
      "Ground Truth Question: \n",
      "<START> when were settlements started in southern mesopotamia ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['early', 'ubaid', 'period']\n",
      "Context:\n",
      "the sumerians were a non-semitic caucasoid people , and spoke a language isolate ; a number of linguists believed they could detect a substrate language beneath sumerian , because names of some of sumer 's major cities are not sumerian , revealing influences of earlier inhabitants . however , the archaeological record shows clear uninterrupted cultural continuity from the time of the early ubaid period ( 5300 â€“ <UNK> bc c-14 ) settlements in southern mesopotamia . the sumerian people who settled here farmed the lands in this region that were made fertile by silt deposited by the tigris and the euphrates rivers . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: how is the name of the big game of film that the times ?\n",
      "Ground Truth Question: \n",
      "<START> in what year were hops first known to be used in beer ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['822', 'ad']\n",
      "Context:\n",
      "the first historical mention of the use of hops in beer was from 822 ad in monastery rules written by <UNK> the elder , also known as <UNK> of corbie , though the date normally given for widespread cultivation of hops for use in beer is the thirteenth century . before the thirteenth century , and until the sixteenth century , during which hops took over as the dominant flavouring , beer was flavoured with other plants ; for instance , grains of paradise or <UNK> . combinations of various aromatic herbs , berries , and even ingredients like <UNK> would be combined into a mixture known as <UNK> and used as hops are now used . some beers today , such as <UNK> ' by the scottish heather ales company and <UNK> <UNK> by the french <UNK> company , use plants other than hops for flavouring . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: how much of the bronx was non-hispanic white as of 2009 ?\n",
      "Ground Truth Question: \n",
      "<START> what percentage of the population in 2011 were <UNK> ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['27.1', '%']\n",
      "Context:\n",
      "the u.s. census bureau reported that in 2000 , 24.0 % of san diego residents were under 18 , and 10.5 % were 65 and over . as of 2011 [ update ] the median age was 35.6 ; more than a quarter of residents were under age 20 and 11 % were over age <UNK> <UNK> ( ages 18 through 34 ) constitute <UNK> % of san diego 's population , the second-highest percentage in a major u.s. city . the san diego county regional planning agency , <UNK> , provides tables and graphs breaking down the city population into 5-year age groups . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what is the most of the two finalists made together ?\n",
      "Ground Truth Question: \n",
      "<START> how can religious beliefs contribute to a person remaining in pain ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['prevent', 'the', 'individual', 'from', 'seeking', 'help']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cultural barriers can also keep a person from telling someone they are in pain . religious beliefs may prevent the individual from seeking help . they may feel certain pain treatment is against their religion . they may not report pain because they feel it is a sign that death is near . many people fear the stigma of addiction and avoid pain treatment so as not to be prescribed potentially <UNK> drugs . many asians do not want to lose respect in society by admitting they are in pain and need help , believing the pain should be borne in silence , while other cultures feel they should report pain right away and get immediate relief . gender can also be a factor in reporting pain . sexual differences can be the result of social and cultural expectations , with women expected to be emotional and show pain and men stoic , keeping pain to themselves . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what does the name of everton 's official supporters club ?\n",
      "Ground Truth Question: \n",
      "<START> the group of animals that can detect magnetic fields is what ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['birds']\n",
      "Context:\n",
      "one of the primary functions of a brain is to extract biologically relevant information from sensory inputs . the human brain is provided with information about light , sound , the chemical composition of the atmosphere , temperature , head orientation , limb position , the chemical composition of the bloodstream , and more . in other animals additional senses may be present , such as the infrared <UNK> of snakes , the magnetic field sense of some birds , or the electric field sense of some types of fish . moreover , other animals may develop existing sensory systems in new ways , such as the adaptation by bats of the auditory sense into a form of sonar . one way or another , all of these sensory modalities are initially detected by specialized sensors that project signals into the brain . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what did the burke and wills expedition give an indication of immigration flows ?\n",
      "Ground Truth Question: \n",
      "<START> what do present day dominican order associates do ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['share', 'the', 'mission', 'and', 'charism', 'of', 'the', 'dominican', 'family']\n",
      "Context:\n",
      "today , there is a growing number of associates who share the dominican charism . dominican associates are christian women and men ; married , single , divorced , and widowed ; clergy members and lay persons who were first drawn to and then called to live out the charism and continue the mission of the dominican order - to praise , to bless , to preach . associates do not take vows , but rather make a commitment to be partners with vowed members , and to share the mission and charism of the dominican family in their own lives , families , churches , neighborhoods , workplaces , and cities . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what is the inrush current affected by the two finalists made ?\n",
      "Ground Truth Question: \n",
      "<START> what type of capacitor can cause signal distortion in the downstream tube ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['a', 'leaky', 'capacitor']\n",
      "Context:\n",
      "leakage is equivalent to a resistor in parallel with the capacitor . constant exposure to heat can cause dielectric breakdown and excessive leakage , a problem often seen in older vacuum tube circuits , particularly where <UNK> paper and foil capacitors were used . in many vacuum tube circuits , <UNK> coupling capacitors are used to conduct a varying signal from the plate of one tube to the grid circuit of the next stage . a <UNK> capacitor can cause the grid circuit voltage to be raised from its normal bias setting , causing excessive current or signal distortion in the downstream tube . in power amplifiers this can cause the plates to glow red , or current limiting resistors to overheat , even fail . similar considerations apply to component fabricated solid-state ( transistor ) amplifiers , but owing to lower heat production and the use of modern polyester dielectric barriers this <UNK> problem has become relatively rare . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: in what year did the vienna philharmonic first accept women ?\n",
      "Ground Truth Question: \n",
      "<START> from what city did aeneas bring the lares cult ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['troy']\n",
      "Context:\n",
      "a pater familias was the senior priest of his household . he offered daily cult to his lares and penates , and to his di <UNK> <UNK> at his domestic shrines and in the fires of the household hearth . his wife ( mater familias ) was responsible for the household 's cult to vesta . in rural estates , bailiffs seem to have been responsible for at least some of the household shrines ( <UNK> ) and their deities . household cults had state counterparts . in vergil 's aeneid , aeneas brought the trojan cult of the lares and penates from troy , along with the palladium which was later installed in the temple of vesta . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: how many birds funnel through batumi , georgia each areas ?\n",
      "Ground Truth Question: \n",
      "<START> the earliest device to help count was what ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['a', 'form', 'of', 'tally', 'stick']\n",
      "Context:\n",
      "devices have been used to aid computation for thousands of years , mostly using one-to-one correspondence with fingers . the earliest counting device was probably a form of tally stick . later record keeping aids throughout the fertile crescent included calculi ( clay spheres , cones , etc . ) which represented counts of items , probably livestock or grains , sealed in hollow <UNK> clay containers . the use of counting rods is one example . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: who created the arsenal team kit exactly , what nickname was in ?\n",
      "Ground Truth Question: \n",
      "<START> who organized a free concert by queen in 1976 ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['richard', 'branson']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "during 1976 , queen played one of their most famous gigs , a free concert in hyde park , london . a concert organised by the entrepreneur richard <UNK> , it set an attendance record with 150,000 people confirmed in the audience . on 1 december 1976 , queen were the intended guests on london 's early evening today programme , but they pulled out at the last-minute , which saw their late replacement on the show , emi <UNK> the sex pistols , give their seminal interview . during the a day at the races tour in 1977 , queen performed sold-out shows at madison square garden , new york , in february , and earls court , london , in june . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what did the island sell in increased revenue during which war ?\n",
      "Ground Truth Question: \n",
      "<START> how many patients received zocor ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['4,444']\n",
      "Context:\n",
      "in april 1994 , the results of a <UNK> study , the scandinavian simvastatin survival study , were announced . researchers tested simvastatin , later sold by merck as zocor , on 4,444 patients with high cholesterol and heart disease . after five years , the study concluded the patients saw a 35 % reduction in their cholesterol , and their chances of dying of a heart attack were reduced by 42 % . in 1995 , zocor and mevacor both made merck over us $ 1 billion . endo was awarded the 2006 japan prize , and the <UNK> clinical medical research award in <UNK> for his `` pioneering research into a new class of molecules '' for `` lowering cholesterol , '' [ sentence fragment ] <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: in what year did the us take control of the world ?\n",
      "Ground Truth Question: \n",
      "<START> what natives comprise a very small percentage of the population ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['portuguese']\n",
      "Context:\n",
      "portuguese natives comprise a very small percentage of <UNK> . after guinea-bissau gained independence , most of the portuguese nationals left the country . the country has a tiny chinese population . these include traders and merchants of mixed portuguese and chinese ancestry from macau , a former asian portuguese colony . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: how did the us intervene against in the us\n",
      "Ground Truth Question: \n",
      "<START> what supported the gl carpet ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['six', 'gci', 'sets', 'controlling', 'radar-equipped', 'night-fighters']\n",
      "Context:\n",
      "whitehall 's <UNK> at the failures of the raf led to the replacement of dowding ( who was already due for retirement ) with <UNK> douglas on 25 november . douglas set about introducing more squadrons and dispersing the few gl sets to create a carpet effect in the southern counties . still , in february 1941 , there remained only seven squadrons with 87 pilots , under half the required strength . the gl carpet was supported by six gci sets controlling <UNK> <UNK> . by the height of the blitz , they were becoming more successful . the number of contacts and combats rose in 1941 , from 44 and two in 48 sorties in january 1941 , to 204 and 74 in may ( 643 sorties ) . but even in may , 67 % of the sorties were visual <UNK> missions . curiously , while 43 % of the contacts in may 1941 were by visual sightings , they accounted for 61 % of the combats . yet when compared with luftwaffe daylight operations , there was a sharp decline in german losses to 1 % . if a vigilant bomber crew could spot the fighter first , they had a decent chance at evading it . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what is the name of everton 's official supporters club ?\n",
      "Ground Truth Question: \n",
      "<START> the stock holder of the original east india company <UNK> how much money to try and deal with the parallel east india company ?  \n",
      "Ground Truth Answer:  ['Â£315,000']\n",
      "Context:\n",
      "this allowed any english firm to trade with india , unless specifically prohibited by act of parliament , thereby <UNK> the charter that had been in force for almost 100 years . by an act that was passed in 1698 , a new `` parallel '' east india company ( officially titled the english company trading to the east indies ) was <UNK> under a <UNK> indemnity of <UNK> million . the powerful stockholders of the old company quickly subscribed a sum of <UNK> in the new concern , and dominated the new body . the two companies wrestled with each other for some time , both in england and in india , for a dominant share of the trade . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: what is the name of the marshall islands economy ?\n",
      "Ground Truth Question: \n",
      "<START> who is the doris <UNK> ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['energy', 'minister']\n",
      "Context:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 25 may 2011 the swiss government announced that it plans to end its use of nuclear energy in the next 2 or 3 decades . `` the government has voted for a <UNK> because we want to ensure a secure and autonomous supply of energy '' , energy minister doris <UNK> said that day at a press conference in bern . `` fukushima showed that the risk of nuclear power is too high , which in turn has also increased the costs of this energy form . '' the first reactor would reportedly be taken offline in 2019 and the last one in <UNK> parliament will discuss the plan in june 2011 , and there could be a referendum as well . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: the top 10 of the `` time to the the process in what ?\n",
      "Ground Truth Question: \n",
      "<START> in what year did ma yuan die ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['ad', '49']\n",
      "Context:\n",
      "the eastern han , also known as the later han , formally began on 5 august 25 , when liu xiu became emperor guangwu of han . during the widespread rebellion against wang mang , the state of <UNK> was free to raid han 's korean commanderies ; han did not reaffirm its control over the region until ad <UNK> the <UNK> sisters of vietnam rebelled against han in ad <UNK> their rebellion was crushed by han general ma yuan ( d. ad 49 ) in a campaign from ad <UNK> . wang mang renewed hostilities against the xiongnu , who were estranged from han until their leader bi ( <UNK> ) , a rival claimant to the throne against his cousin <UNK> ( <UNK> ) , submitted to han as a tributary vassal in ad <UNK> this created two rival xiongnu states : the southern xiongnu led by bi , an ally of han , and the northern xiongnu led by <UNK> , an enemy of han . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: which two areas does the world 's largest what ?\n",
      "Ground Truth Question: \n",
      "<START> why did the 2 districts have to be redrawn in 2014 <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['that', 'at', 'least', 'two', 'districts', 'had', 'to', 'be', 'redrawn', 'because', 'of', 'gerrymandering']\n",
      "Context:\n",
      "the court ruled in 2014 , after lengthy testimony , that at least two districts had to be redrawn because of <UNK> . after this was appealed , in july 2015 the florida supreme court ruled that lawmakers had followed an illegal and unconstitutional process overly influenced by party operatives , and ruled that at least eight districts had to be redrawn . on december 2 , 2015 , a <UNK> majority of the court accepted a new map of congressional districts , some of which was drawn by challengers . their ruling affirmed the map previously approved by leon county judge terry lewis , who had overseen the original trial . it particularly makes changes in south florida . there are likely to be additional challenges to the map and districts . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: who did the us intervene against in the two areas ?\n",
      "Ground Truth Question: \n",
      "<START> what <UNK> did montini become cardinal of ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['ss', '.', 'silvestro', 'e', 'martino']\n",
      "Context:\n",
      "although some cardinals seem to have viewed him as papabile , a likely candidate to become pope , and may have received some votes in the 1958 conclave , montini was not yet a cardinal , which made him an unlikely choice . [ c ] angelo roncalli was elected pope on 28 october 1958 and assumed the name john xxiii . on 17 november 1958 , <UNK> romano announced a consistory for the creation of new cardinals . montini 's name led the list . when the pope raised montini to the cardinalate on 15 december 1958 , he became cardinal-priest of ss . <UNK> e martino ai monti . he appointed him simultaneously to several vatican congregations which resulted in many visits by montini to rome in the coming years . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------\n",
      "Generated Question: how did the island sell in french ?\n",
      "Ground Truth Question: \n",
      "<START> where does the highest rainfall occur in namibia ? <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>  \n",
      "Ground Truth Answer:  ['caprivi']\n",
      "Context:\n",
      "namibia is the driest country in sub-saharan africa and depends largely on groundwater . with an average rainfall of about 350 mm per annum , the highest rainfall occurs in the caprivi in the northeast ( about 600 mm per annum ) and decreases in a westerly and <UNK> direction to as little as 50 mm and less per annum at the coast . the only perennial rivers are found on the national borders with south africa , angola , zambia , and the short border with botswana in the caprivi . in the interior of the country surface water is available only in the summer months when rivers are in flood after exceptional rainfalls . otherwise , surface water is restricted to a few large storage dams retaining and damming up these seasonal floods and their runoff . thus , where people <UNK> live near perennial rivers or make use of the storage dams , they are dependent on groundwater . the advantage of using groundwater sources is that even isolated communities and those economic activities located far from good surface water sources such as mining , agriculture , and tourism can be supplied from groundwater over nearly 80 % of the country . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "---------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# q = set()\n",
    "l = []\n",
    "for i in range(batch_input[0][batchNum].shape[0]):\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, qs[i])\n",
    "    print(\"Generated Question: \" + \" \".join(look_up_token_reduced(token) for token in question))\n",
    "    print(\"Ground Truth Question: \")\n",
    "    printQues(batchNum,i)\n",
    "    print(\"Ground Truth Answer: \", X_train_ans_shuffled[batch_size*batchNum + i])\n",
    "    print(\"Context:\")\n",
    "    printDoc(batchNum,i)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "# q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[6][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(batch_input[7][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
